# Tasks

*Part of the Flow System. See also: [Guidelines](../system/guidelines.md), [Improvements](../planning/improvements.md).*

This file serves as the working memory for all active tasks in the project. It's designed to provide at-a-glance visibility into what's currently being worked on, what's coming up next, and what has recently been completed.

## Active Tasks

- **Testing Strategy:** ðŸŸ¡ â³ Throughout the refactoring process, ensure comprehensive test coverage is maintained and improved. This includes unit tests for individual components, integration tests for interactions between components, and end-to-end (feature) tests for verifying complete workflows (e.g., training, generation). Add specific testing sub-tasks to relevant refactoring items.

- #### Experiment: Train Larger Model with Longer Context (Character-Level)
  - **Name**: `got_char_1M_ctx256_bs64` # Assigned Experiment Name
  - **Goal**: Evaluate training performance (throughput) and model quality (loss, generation) with increased model capacity and context length on consumer hardware.
  - **Configuration**:
    - Model: ~1M parameters (`d_model=128, n_layers=6, n_head=8, d_hid=512`)
    - Context Length: 256 (`block_size=256`)
    - Tokenization: Character-level
    - Training: 5 epochs, `batch_size=64`, `gradient_accumulation_steps=1`
  - **Status**: â¸ï¸ Paused (Ready to resume from latest checkpoint)
  - **Latest Checkpoint**: `outputs/2025-03-29/19-28-11/checkpoints/checkpoint_step_8570.pt`
  - **Context**: Reverted block_size from 1024 to 256 due to performance issues. This run uses batch_size 64 to evaluate performance against previous baselines after code refactoring.
  - **Metrics**: Track Tokens/sec, loss curves (step & epoch), qualitative generation results.

### ðŸ”´ High Priority

*These tasks represent major foundational refactoring.* 

- [âœ…] **Complete Model Architecture Refactoring** (Est. Effort: High) - Standardize model interfaces, config handling (Pydantic), factory, registration.
    - [x] Define model abstraction hierarchy (`Model` -> `GenerativeModel` -> `LanguageModel`).
    - [x] Refactor existing models (`Transformer`, `GPTDecoder`) to fit the new structure.
    - [x] Integrate `generate` method into `GenerativeModel`.
    - [x] Update model loading/saving to use `state_dict` and config.
    - [x] Ensure consistent naming and structure in `src/models/`.
    - [x] Add unit tests for base classes, factory, and `TransformerModel` (including `generate`).
- [ ] **Standardize Data Pipeline** (Est. Effort: High) - Create unified Dataset/DataLoader logic.
    - [x] Define base class `BaseDataset` (`src/data/base.py`).
    - [x] Refactor `CharDataset` to inherit from `BaseDataset` and use config (`src/data/dataset.py`).
    - [x] Refactor factory `create_dataset_from_config` for Hydra/fallback (`src/data/base.py`).
    - [x] Verify default `collate_fn` works for `CharDataset`.
    - [x] Add unit tests for `BaseDataset`, `CharDataset`, and factory.
    - [ ] Define standard tokenizer configuration and loading strategy (Documented in `src/data/base.py`).
    - [ ] Implement unified preprocessing steps (if needed beyond `collate_fn`).
    - [ ] Implement custom `collate_fn` for padding/tokenization (when needed).
- [ ] **Refactor Training Loop** (Est. Effort: Medium) - Decouple components, improve clarity.

### ðŸŸ  Medium Priority

*These tasks improve existing components or address known gaps.*

- #### Improve Configuration Handling
  - **Sub-tasks**:
    - â³ Improve Configuration Validation (Pydantic/JSONSchema) (`src/config/config_manager.py`)
      - *Prerequisite for: Refactor Dataset Loading*
      - ðŸ” Identified during code review 2025-03-28
    - â³ Fix Config Defaults (Paths in `conf/*.yaml`) âœ…

- #### Refactor Core Training Logic
  - **Sub-tasks**:
    - â³ Review Scheduler T_max Calculation (`src/training/base.py::_create_scheduler`)
      - ðŸ” Identified during code review 2025-03-28
    - â³ Integrate Optimizations (`src/training/optimizations.py` -> `Trainer`)
      - *See also: Deprecate Old Training Scripts Task*
      - ðŸ” Identified during code review 2025-03-28
    - â³ Deprecate/Remove Old Training Scripts (`train_runner.py`, `training_loop.py`, etc.)
      - *Depends on: Integrate Optimizations*
      - ðŸ” Identified during code review 2025-03-28
    - â³ Integrate Proven Training Logic (`scripts/train_with_samples.py` -> `Trainer`) âœ…

- #### Refactor Specific Components
  - **Sub-tasks**:
    - â³ Refactor Data Handling Details (`CharDataset`, Vocab, Splitting)
      - *Contributes to: Standardize Data Pipeline Goal*
      - ðŸ” Identified during code review 2025-03-28
    - â³ Remove Unused PositionalEncoding (`src/models/transformer.py`)
      - ðŸ” Identified during code review 2025-03-28
    - â³ Clarify/Consolidate Model Implementations & Generation (`gpt_decoder.py`, `transformer.py`, `generation.py`)
      - *Contributes to: Complete Model Architecture Refactoring Goal*
      - ðŸ” Identified during code review 2025-03-28

- #### Improve Testing & CI
  - **Sub-tasks**:
    - â³ Implement Tests for Remaining Callbacks (`SampleGenerationCallback`, `TensorBoardLogger`)
      - **Status**: â¸ï¸ Paused (See details below)
      - ðŸ” Identified as missing during review 2025-03-29
    - â³ Refactor Test Structure and Runner (`tests/run_all_tests.py`, directory org)
      - ðŸ” Identified during code review 2025-03-28
    - â³ Fix Test Import Handling (Remove `try-except-mock`)
      - ðŸ” Identified during code review 2025-03-28
    - â³ Improve Generation Test Coverage (`GenerationTests`)
      - ðŸ” Identified during code review 2025-03-28
    - â³ Refactor Data Test Configuration (`DataTests`)
      - ðŸ” Identified during code review 2025-03-28
    - â³ Fix Integration Tests (`tests/integration_tests.py`)
      - ðŸ” Identified during code review 2025-03-28
    - â³ Set up CI/CD pipeline (GitHub Actions)
      - ðŸ” Planned

- #### Documentation & Dependencies
  - **Sub-tasks**:
    - â³ Update Architecture Documentation âœ…
    - â³ Investigate Model "Memory Architecture" (`docs/model.md` investigation)
      - ðŸ” Identified during code review 2025-03-28
    - â³ Update Data Pipeline Documentation (`docs/data_pipeline.md`)
      - ðŸ” Identified during code review 2025-03-28
    - â³ Correct Config Import in README âœ…
    - â³ Update Model Documentation (Parameter Count Difference) (`docs/model.md` update)
      - ðŸ” Identified during code review 2025-03-28
    - â³ Verify `transformers` Dependency (Scheduler Usage)
      - ðŸ” Identified during code review 2025-03-28

- ### ðŸŸ¡ Implement Tests for Remaining Callbacks (Details)
  - **Status**: â¸ï¸ Paused
  - **Description**: Ensure comprehensive test coverage for all trainer callbacks (`SampleGenerationCallback`, `TensorBoardLogger`).
  - **Context**: 
    - Initially added `TestTensorBoardLogger`.
    - Encountered persistent `TypeError` issues with `@patch` and method signatures, particularly for `TestSampleGenerationCallback`.
    - Current blockers: 
      - `TestSampleGenerationCallback`: `TypeError: setUp() missing 1 required positional argument: 'mock_logging'` despite code appearing correct.
      - `TestTensorBoardLogger`: AssertionErrors related to mock calls not being detected.
    - Paused due to persistent test execution errors blocking progress.

## Refactoring

- [x] Refactor vocabulary handling:
    - [x] Create `preprocess.py` script to calculate and save vocab/mappings from raw data.
    - [x] Update `CharDataset` to load precomputed vocab instead of calculating it.
    - [x] Update model config (`conf/model/transformer.yaml`) with the static vocab size.
    - [x] Update data config (`conf/data/char.yaml`) and `DataManager` if needed to pass vocab path.
    - [x] Remove dynamic vocab update logic from `train_runner.py` and `generate_text.py`.

## Upcoming Tasks

- ### ðŸŸ  Refactor Data Pipeline for Tokenizer Flexibility
  - **Goal**: Modify the data pipeline to support different tokenizers (character, subword, etc.) configured via Hydra, following the strategy outlined in `src/data/base.py`.
  - **Steps**:
    1. Define/Re-introduce standalone `Tokenizer` classes (e.g., `CharacterTokenizer`) in `src/data/tokenizer.py`.
    2. Define corresponding Hydra configs (e.g., `conf/tokenizer/char.yaml`).
    3. Update `conf/config.yaml` to include `tokenizer` defaults/placeholders.
    4. Modify `Dataset` classes (e.g., `CharDataset`) to return raw text instead of tokenizing internally.
    5. Implement custom `collate_fn` functions (e.g., `character_collate_fn` in `src/data/collation.py`) that use the instantiated tokenizer.
    6. Modify `prepare_dataloaders_from_config` to accept the tokenizer, determine the correct `collate_fn`, and pass it to `DataLoader`.
    7. Modify `train_runner.py` to instantiate the `tokenizer` via Hydra and pass it to `prepare_dataloaders_from_config` and `Trainer`.
    8. Modify `Trainer` to accept and use the `tokenizer` object for sampling, removing `vocab_path`.
  - **Status**: â³ To Do (Postponed from previous attempt)

- ### ðŸŸ¡ Review `benchmarking\\benchmarks` vs `outputs\\benchmarks`

- ### ðŸ”´ Define Goals
  - We should define the precise goals that we wish to attain with this project, and work backwards from there to ensure that the project's implementations are ideal for fulfilling those goals.
  - Develop AI models that work on low-spec devices (performance critical)
  - Experiment with cutting edge AI architectures (research)

- ### ðŸŸ  Review Dependencies
  - Which libraries/frameworks/packages/etc. does Craft depend on? Are they all justified and ideal? What are the pros and cons compared to alternatives? When does it make sense to use custom libraries/frameworks/packages/etc.?

- ### ðŸŸ¡ Improve CLI Generation Tokenization
  - Refactor `src/cli/run.py::generate_text` to load tokenizer based on model checkpoint
  - Remove `TODO` placeholder logic
  - Ensure compatibility with different model types/tokenizers
  - ðŸ” Identified during code review 2025-03-28

## Completed Tasks (Recent)

- ### ðŸŸ¡ Implement Basic Unit Tests âœ… 2025-03-28
- ### ðŸŸ¡ Implement Folder Structure âœ… 2025-03-28
- ### ðŸ”´ Update Architecture Documentation âœ… 2025-03-28
- ### ðŸŸ¡ Fix Config Defaults âœ… 2025-03-28
- ### ðŸ”´ Integrate Proven Training Logic âœ… 2025-03-28
- ### ðŸŸ¡ Complete Config Refactor (`configs` -> `conf`) âœ… 2025-03-28