# Tasks

*Part of the Flow System. See also: [Guidelines](../system/guidelines.md), [Improvements](../planning/improvements.md).*

This file serves as the working memory for all active tasks in the project. It's designed to provide at-a-glance visibility into what's currently being worked on, what's coming up next, and what has recently been completed.

## Active Tasks

- **Test End-to-End Workflow**: ðŸŸ¡ â³ Run minimal training test to verify core loop, logging, config after refactors.
  - *Script*: `python -m src.training.train_runner training=minimal_test`
  - *Status*: Ready to run

- **Implement Consistent TensorBoard Logging**: ðŸŸ¡ â¸ï¸ Modify configuration and potentially training script to ensure TensorBoard logs from resumed runs append to the original experiment log directory, identified by a unique `experiment_id`.
  - *Depends on*: [Fix and Utilize TensorBoard Logging](#fix-and-utilize-tensorboard-logging-tensorboardlogger) (tests passing)
  - *Context*: Needed to correctly visualize loss curves across interrupted training sessions. (Paused pending basic workflow test)

- **Testing Strategy:** ðŸŸ¡ â³ Throughout the refactoring process, ensure comprehensive test coverage is maintained and improved. This includes unit tests for individual components, integration tests for interactions between components, and end-to-end (feature) tests for verifying complete workflows (e.g., training, generation). Add specific testing sub-tasks to relevant refactoring items.

- #### Experiment: Train Larger Model with Longer Context (Character-Level)
  - **Name**: `got_char_1M_ctx256_bs64` # Assigned Experiment Name
  - **Goal**: Evaluate training performance (throughput) and model quality (loss, generation) with increased model capacity and context length on consumer hardware.
  - **Configuration**:
    - Model: ~1M parameters (`d_model=128, n_layers=6, n_head=8, d_hid=512`)
    - Context Length: 256 (`block_size=256`)
    - Tokenization: Character-level
    - Training: 5 epochs, `batch_size=64`, `gradient_accumulation_steps=1`
  - **Status**: ðŸŸ¡ Starting Fresh (Resume path invalid/starting new run)
  - **Latest Checkpoint**: outputs/2025-03-29/19-28-11/checkpoints/checkpoint_step_8570.pt # Old path
  - **Context**: Reverted block_size from 1024 to 256 due to performance issues. This run uses batch_size 64 to evaluate performance against previous baselines after code refactoring.
  - **Metrics**: Track Tokens/sec, loss curves (step & epoch), qualitative generation results.

### ðŸ”´ High Priority

*These tasks represent major foundational refactoring.*

- [ ] **Standardize Data Pipeline** (Est. Effort: High) - Create unified Dataset/DataLoader logic.
    - [x] Define base class `BaseDataset` (`src/data/base.py`).
    - [x] Refactor `CharDataset` to inherit from `BaseDataset` and use config (`src/data/dataset.py`).
    - [x] Refactor factory `create_dataset_from_config` for Hydra/fallback (`src/data/base.py`).
    - [x] Verify default `collate_fn` works for `CharDataset`.
    - [x] Add unit tests for `BaseDataset`, `CharDataset`, and factory.
    - [ ] Define standard tokenizer configuration and loading strategy (Documented in `src/data/base.py`).
    - [ ] Implement unified preprocessing steps (if needed beyond `collate_fn`).
    - [ ] Implement custom `collate_fn` for padding/tokenization (when needed).
- [ ] **Refactor Training Loop** (Est. Effort: Medium) - Decouple components, improve clarity.

### ðŸŸ  Medium Priority

*These tasks improve existing components or address known gaps.*

- #### Improve Configuration Handling
  - **Sub-tasks**:
    - â³ Improve Configuration Validation (Pydantic/JSONSchema) (`src/config/config_manager.py`)
      - *Prerequisite for: Refactor Dataset Loading*
      - ðŸ” Identified during code review 2025-03-28

- #### Refactor Core Training Logic
  - **Sub-tasks**:
    - â³ Review Scheduler T_max Calculation (`src/training/base.py::_create_scheduler`)
      - ðŸ” Identified during code review 2025-03-28
    - â³ Integrate Optimizations (`src/training/optimizations.py` -> `Trainer`)
      - *See also: Deprecate Old Training Scripts Task*
      - ðŸ” Identified during code review 2025-03-28
    - â³ Deprecate/Remove Old Training Scripts (`train_runner.py`, `training_loop.py`, etc.)
      - *Depends on: Integrate Optimizations*
      - ðŸ” Identified during code review 2025-03-28

- #### Refactor Specific Components
  - **Sub-tasks**:
    - â³ Refactor Data Handling Details (`CharDataset`, Vocab, Splitting)
      - *Contributes to: Standardize Data Pipeline Goal*
      - ðŸ” Identified during code review 2025-03-28
    - â³ Remove Unused PositionalEncoding (`src/models/transformer.py`)
      - ðŸ” Identified during code review 2025-03-28
    - â³ Clarify/Consolidate Model Implementations & Generation (`gpt_decoder.py`, `transformer.py`, `generation.py`)
      - *Contributes to: Complete Model Architecture Refactoring Goal*
      - ðŸ” Identified during code review 2025-03-28

- #### Improve Testing & CI
  - **Sub-tasks**:
    - â³ **Fix and Utilize TensorBoard Logging** (`TensorBoardLogger`)
      - **Status**: ðŸŸ¡ Active (Debugging test failures - Paused pending basic workflow test)
      - ðŸ” Previously identified as missing tests during review 2025-03-29
    - â³ Implement Tests for `SampleGenerationCallback`
    - â³ Refactor Test Structure and Runner (`tests/run_all_tests.py`, directory org)
      - ðŸ” Identified during code review 2025-03-28
    - â³ Fix Test Import Handling (Remove `try-except-mock`)
      - ðŸ” Identified during code review 2025-03-28
    - â³ Improve Generation Test Coverage (`GenerationTests`)
      - ðŸ” Identified during code review 2025-03-28
    - â³ Refactor Data Test Configuration (`DataTests`)
      - ðŸ” Identified during code review 2025-03-28
    - â³ Fix Integration Tests (`tests/integration_tests.py`)
      - ðŸ” Identified during code review 2025-03-28
    - â³ Set up CI/CD pipeline (GitHub Actions)
      - ðŸ” Planned

- #### Documentation & Dependencies
  - **Sub-tasks**:
    - â³ Investigate Model "Memory Architecture" (`docs/model.md` investigation)
      - ðŸ” Identified during code review 2025-03-28
    - â³ Update Data Pipeline Documentation (`docs/data_pipeline.md`)
      - ðŸ” Identified during code review 2025-03-28
    - â³ Update Model Documentation (Parameter Count Difference) (`docs/model.md` update)
      - ðŸ” Identified during code review 2025-03-28
    - â³ Verify `transformers` Dependency (Scheduler Usage)
      - ðŸ” Identified during code review 2025-03-28

## Upcoming Tasks

- ### ðŸŸ¡ Review Legacy Code
  - **Goal**: Review `src/` and `scripts/` directories to identify and remove or archive deprecated/legacy code (e.g., potentially unused utilities) after confirming functionality of refactored components (like `src/cli/run.py`).
  - **Status**: â³ To Do

- ### ðŸŸ  Refactor Data Pipeline for Tokenizer Flexibility
  - **Goal**: Modify the data pipeline to support different tokenizers (character, subword, etc.) configured via Hydra, following the strategy outlined in `src/data/base.py`.
  - **Steps**: (See original task for detailed steps)
  - **Status**: â³ To Do (Postponed from previous attempt)

- ### ðŸŸ¡ Review `benchmarking\\benchmarks` vs `outputs\\benchmarks`

- ### ðŸ”´ Define Goals
  - We should define the precise goals that we wish to attain with this project, and work backwards from there to ensure that the project's implementations are ideal for fulfilling those goals.
  - Develop AI models that work on low-spec devices (performance critical)
  - Experiment with cutting edge AI architectures (research)

- ### ðŸŸ  Review Dependencies
  - Which libraries/frameworks/packages/etc. does Craft depend on? Are they all justified and ideal? What are the pros and cons compared to alternatives? When does it make sense to use custom libraries/frameworks/packages/etc.?

- ### ðŸŸ¡ Improve CLI Generation Tokenization
  - Refactor `src/cli/run.py::generate_text` to load tokenizer based on model checkpoint
  - Remove `TODO` placeholder logic
  - Ensure compatibility with different model types/tokenizers
  - ðŸ” Identified during code review 2025-03-28

## Completed Tasks (Recent)

- [âœ…] **Complete Model Architecture Refactoring** (Est. Effort: High) âœ… 2025-03-30 (Assumed completed based on sub-tasks)
- ### ðŸŸ¡ Implement Basic Unit Tests âœ… 2025-03-28
- ### ðŸŸ¡ Implement Folder Structure âœ… 2025-03-28
- ### ðŸ”´ Update Architecture Documentation âœ… 2025-03-28
- ### ðŸŸ¡ Fix Config Defaults âœ… 2025-03-28
- ### ðŸ”´ Integrate Proven Training Logic âœ… 2025-03-28
- ### ðŸŸ¡ Complete Config Refactor (`configs` -> `conf`) âœ… 2025-03-28

### Data Processing Enhancements

-   **Task:** Implement Standard Tokenization in `src/data/processors.py`.
    -   **Status:** âœ… Done (2024-03-31)
    -   **Description:** Modify `prepare_text_data` (or add new logic) to load and use standard tokenizers (e.g., Hugging Face `transformers`/`tokenizers` like 'gpt2') based on configuration, saving token IDs instead of raw text. Save necessary tokenizer info (vocab size, special tokens).
    -   **Priority:** High
-   **Task:** Integrate Data Splitting in `src/data/processors.py`.
    -   **Status:** âœ… Done (2024-03-31)
    -   **Description:** Modify the data preparation logic (e.g., in `prepare_text_data`) to call the existing `split_data` function after loading/tokenizing, saving separate processed files for train, validation, and potentially test splits. Ensure configuration allows specifying split ratios.
    -   **Priority:** High
-   **Task:** Add Unit Tests for Tokenization and Splitting.
    -   **Status:** âœ… Done (2024-03-31)
    -   **Description:** Update `tests/unit/test_data_processors.py` to add tests covering the new standard tokenization logic (handling different tokenizers) and the integrated data splitting functionality.
    -   **Priority:** High