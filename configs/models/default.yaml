# Default Model Configuration

# Model architecture
architecture: gpt
vocab_size: 256  # Character-level so we use 256 as default
d_model: 768     # Embedding dimension
n_layers: 12     # Number of transformer layers
n_head: 12       # Number of attention heads
d_hid: 3072      # Hidden dimension of feedforward layers
dropout: 0.1     # Dropout rate
activation: gelu # Activation function (gelu, relu, swish)
layer_norm_eps: 1.0e-5  # Layer normalization epsilon
bias: true       # Whether to use bias in Linear layers

# Model initialization
init:
  method: normal  # normal, xavier, kaiming
  mean: 0.0       # Mean for normal initialization
  std: 0.02       # Standard deviation for normal initialization
  
# Positional encoding
positional_encoding:
  type: learned    # learned, sinusoidal
  max_length: 2048 # Maximum sequence length for position embeddings 