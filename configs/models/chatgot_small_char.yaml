# Configuration for ChatGoT Small model (~114M parameters)
# Based on GPT-2 Small architecture but with custom transformer implementation

architecture: gpt
vocab_size: 96  # Character-level vocabulary (actual count from the dataset)
d_model: 768       # Embedding dimension
n_layers: 12       # Number of transformer layers
n_head: 12         # Number of attention heads
d_hid: 3072        # Hidden dimension (4x embedding size)
dropout: 0.1       # Dropout rate
activation: gelu   # Activation function
layer_norm_eps: 1.0e-5  # Layer norm epsilon
bias: true         # Use bias in linear layers

# Model initialization
initialization:
  method: normal    # Normal initialization
  mean: 0.0         # Mean for normal distribution
  std: 0.02         # Standard deviation for initialization

# Positional encoding
positional_encoding:
  type: learned     # Learned positional embeddings
  max_length: 1024  # Maximum sequence length

# Additional parameters from the new configuration
n_positions: 1024
embd_pdrop: 0.1
attn_pdrop: 0.1
initializer_range: 0.02
scale_attn_weights: true
use_cache: true

# Memory optimization settings for training
training:
  batch_size: 8     # Reduced batch size for memory optimization
  learning_rate: 0.0005
  weight_decay: 0.01
  mixed_precision: true
  gradient_checkpointing: true
  accumulate_grad_batches: 8  # Accumulate gradients to simulate larger batch
  max_seq_length: 1024  # Use full sequence length 