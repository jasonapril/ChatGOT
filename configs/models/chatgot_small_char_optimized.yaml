# Configuration for ChatGoT Small model (~85M parameters)
model:
  architecture: gpt
  model_type: gpt_decoder  # Specify optimized decoder-only architecture
  vocab_size: 96          # Character-level vocabulary size
  d_model: 768            # Embedding dimension
  n_layers: 12            # Number of layers
  n_head: 12              # Number of attention heads
  d_hid: 3072             # Hidden dimension in feed-forward layers
  dropout: 0.1            # Dropout rate
  attention_dropout: 0.1  # Dropout rate for attention
  layer_norm_eps: 1e-5    # Layer normalization epsilon
  activation: gelu        # Activation function
  bias: true              # Use bias in layers
  
  # Model initialization
  init_method: normal     # Initialization method
  init_range: 0.02        # Initialization range
  
  # Positional encoding
  pos_encoding_type: learned  # Options: learned, sinusoidal
  max_seq_length: 1024     # Maximum sequence length

training:
  batch_size: 8           # Batch size
  learning_rate: 5e-4     # Learning rate
  weight_decay: 0.01      # Weight decay
  epochs: 1               # Number of epochs
  mixed_precision: true   # Use mixed precision training
  gradient_checkpointing: true  # Use gradient checkpointing
  accumulate_grad_batches: 4    # Gradient accumulation steps
  grad_clip: 1.0          # Gradient clipping threshold

data:
  path: data/got/game_of_thrones.txt  # Data path
  test_split: 0.1         # Test split ratio

# Additional parameters from the new configuration
n_positions: 1024
embd_pdrop: 0.1
attn_pdrop: 0.1
initializer_range: 0.02
scale_attn_weights: true
use_cache: true 