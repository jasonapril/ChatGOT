# Configuration for ChatGoT Small (25M parameters) - debug version
model:
  architecture: gpt
  model_type: gpt_decoder  # Specify optimized decoder-only architecture
  vocab_size: 96          # Character-level vocabulary size
  d_model: 384            # Embedding dimension (smaller)
  n_layers: 8             # Number of layers (fewer)
  n_head: 6               # Number of attention heads 
  d_hid: 1536             # Hidden dimension in feed-forward layers
  dropout: 0.1            # Dropout rate
  attention_dropout: 0.1  # Dropout rate for attention
  layer_norm_eps: 1e-5    # Layer normalization epsilon
  activation: gelu        # Activation function
  bias: true              # Use bias in layers
  
  # Model initialization
  init_method: normal     # Initialization method
  init_range: 0.02        # Initialization range
  
  # Positional encoding
  pos_encoding_type: learned  # Options: learned, sinusoidal
  max_seq_length: 512     # Maximum sequence length (reduced)

training:
  batch_size: 16          # Reduced batch size for memory efficiency
  learning_rate: 5e-4     # Higher learning rate
  weight_decay: 0.01      # Weight decay
  epochs: 1               # Number of epochs
  mixed_precision: true   # Use mixed precision training
  gradient_checkpointing: true  # Enable for OOM prevention
  accumulate_grad_batches: 2    # Moderate accumulation for better throughput
  grad_clip: 1.0          # Gradient clipping threshold
  log_interval: 5         # More frequent logs

data:
  path: data/got/game_of_thrones.txt  # Data path
  test_split: 0.1         # Test split ratio

# Additional parameters 
n_positions: 512
embd_pdrop: 0.1
attn_pdrop: 0.1
initializer_range: 0.02
scale_attn_weights: true
use_cache: false  # Set to false when using gradient checkpointing 