# Configuration for ChatGoT Small (14M parameters) - memory optimized
model:
  architecture: gpt
  model_type: gpt_decoder  # Specify optimized decoder-only architecture
  vocab_size: 96          # Character-level vocabulary size
  d_model: 384            # Embedding dimension (smaller)
  n_layers: 8             # Number of layers (fewer)
  n_head: 6               # Number of attention heads 
  d_hid: 1536             # Hidden dimension in feed-forward layers
  dropout: 0.1            # Dropout rate
  attention_dropout: 0.1  # Dropout rate for attention
  layer_norm_eps: 1e-5    # Layer normalization epsilon
  activation: gelu        # Activation function
  bias: true              # Use bias in layers
  
  # Model initialization
  init_method: normal     # Initialization method
  init_range: 0.02        # Initialization range
  
  # Positional encoding
  pos_encoding_type: learned  # Options: learned, sinusoidal
  max_seq_length: 256     # Reduced sequence length for memory savings

training:
  batch_size: 8           # Small batch size for memory efficiency
  learning_rate: 6e-4     # Higher learning rate
  weight_decay: 0.01      # Weight decay
  epochs: 1               # Number of epochs
  mixed_precision: true   # Use mixed precision training
  gradient_checkpointing: false  # Disable gradient checkpointing
  accumulate_grad_batches: 4    # Use accumulation instead of large batch

  grad_clip: 1.0          # Gradient clipping threshold
  log_interval: 10        # Log every 10 batches
  sample_interval: 300    # Generate samples every 5 minutes

data:
  path: data/got/game_of_thrones.txt  # Data path
  test_split: 0.1         # Test split ratio

# Memory optimization settings
memory:
  optimize_memory: true
  empty_cache_freq: 100   # Empty cache every 100 batches
  cpu_offload: false      # Whether to offload optimizer states to CPU

# Additional parameters
n_positions: 256
embd_pdrop: 0.1
attn_pdrop: 0.1
initializer_range: 0.02
scale_attn_weights: true
use_cache: false  # Must be false when using gradient checkpointing 