# Merged configuration for ChatGoT

# Project metadata
project:
  name: ChatGoT
  version: 0.1.0
  description: Character-level GPT for Game of Thrones Text Generation

# System configuration
system:
  seed: 42
  log_level: INFO
  device: auto  # 'auto', 'cpu', 'cuda', 'cuda:0', etc.
  dtype: float32  # 'float32', 'float16', 'bfloat16'
  num_workers: 4  # Number of workers for data loading
  pin_memory: true  # Use pinned memory for faster data loading

# Paths
paths:
  # Data paths
  data_dir: data
  raw_data: ${paths.data_dir}/game_of_thrones_dataset.txt
  data_file: ${paths.raw_data}  # Explicitly set data_file for processor.py
  processed_data_dir: processed_data
  processed_data: ${paths.processed_data_dir}/got_char_data.pkl
  analysis_dir: ${paths.processed_data_dir}/analysis
  
  # Model paths
  models_dir: models
  checkpoint_dir: checkpoints
  
  # Output paths
  output_dir: runs/default
  log_dir: ${paths.output_dir}/logs
  
  # Artifacts
  artifacts_dir: ${paths.output_dir}/artifacts

# Experiment tracking
experiment:
  name: default
  tags: []
  track: true  # Whether to track the experiment
  tracking_uri: mlruns  # MLflow tracking URI

# Model configuration
model:
  name: gpt2_small
  vocab_size: 95  # Character-level vocabulary
  n_positions: 1024
  n_embd: 768
  n_layer: 12
  n_head: 12
  activation_function: gelu_new
  resid_pdrop: 0.1
  embd_pdrop: 0.1
  attn_pdrop: 0.1
  layer_norm_epsilon: 1e-5
  initializer_range: 0.02
  scale_attn_weights: true
  use_cache: true

# Training configuration
training:
  # Training parameters
  epochs: 50
  batch_size: 32
  learning_rate: 5.0e-5
  weight_decay: 0.01
  clip_grad_norm: 1.0
  
  # Optimizer
  optimizer:
    name: adamw
    beta1: 0.9
    beta2: 0.999
    eps: 1.0e-8
  
  # Learning rate scheduler
  scheduler:
    name: cosine
    warmup_ratio: 0.1
    min_lr_ratio: 0.1
  
  # Mixed precision training
  mixed_precision:
    enabled: true
    dtype: float16
  
  # Checkpointing
  checkpoint:
    save_best: true
    metric: val_loss
    mode: min
    patience: 3
  
  # Logging
  logging:
    log_interval: 10  # Log every N batches
    eval_interval: 1  # Evaluate every N epochs
    save_interval: 1  # Save checkpoint every N epochs

# Data configuration
data:
  # Dataset parameters
  train_size: 0.9  # Fraction of data for training
  val_size: 0.1    # Fraction of data for validation
  sequence_length: 1024  # Length of input sequences
  stride: 512  # Stride for sliding window
  batch_size: 32  # Batch size for training
  num_workers: 4  # Number of workers for data loading
  pin_memory: true  # Use pinned memory for faster data loading
  
  # Tokenizer parameters
  tokenizer:
    type: character  # Character-level tokenization
    special_tokens:
      pad_token: " "
      unk_token: "?"
      bos_token: "^"
      eos_token: "$"
    
  # Data augmentation
  augmentation:
    enabled: false  # Whether to use data augmentation
    methods: []  # List of augmentation methods to use

# Pipeline configuration
pipeline:
  # Pipeline stages
  stages:
    - preprocess
    - train
    - evaluate
    - generate
  
  # Stage-specific settings
  preprocess:
    clean_data: true
    tokenize: true
    create_splits: true
  
  train:
    resume: true  # Resume from checkpoint if available
    validate: true  # Run validation during training
    save_checkpoints: true
  
  evaluate:
    metrics:
      - loss
      - perplexity
    generate_samples: true
    num_samples: 5
  
  generate:
    max_length: 100
    temperature: 0.8
    top_k: 40
    top_p: 0.9
    num_return_sequences: 1
    seed_text: "The" 