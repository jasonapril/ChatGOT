# Default Training Configuration

training:
  # Training parameters
  epochs: 50
  batch_size: 32
  learning_rate: 5.0e-5
  weight_decay: 0.01
  clip_grad_norm: 1.0
  
  # Optimizer
  optimizer:
    name: adamw
    beta1: 0.9
    beta2: 0.999
    eps: 1.0e-8
  
  # Learning rate scheduler
  scheduler:
    name: cosine
    warmup_ratio: 0.1
    min_lr_ratio: 0.1
  
  # Mixed precision training
  mixed_precision:
    enabled: true
    dtype: float16
  
  # Checkpointing
  checkpoint:
    save_best: true
    metric: val_loss
    mode: min
    patience: 3
  
  # Logging
  logging:
    log_interval: 10  # Log every N batches
    eval_interval: 1  # Evaluate every N epochs
    save_interval: 1  # Save checkpoint every N epochs

# Training parameters
sequence_length: 1024
save_every: 1  # Save checkpoint every N epochs
eval_every: 0.5  # Evaluate every N epochs (can be fractional)

# Gradient accumulation
gradient_accumulation:
  enabled: false  # Whether to use gradient accumulation
  steps: 1        # Number of steps to accumulate gradients

# Distributed training
distributed:
  enabled: false  # Whether to use distributed training
  backend: nccl   # nccl, gloo
  world_size: -1  # Number of processes (-1 for auto)
  find_unused_parameters: false  # Whether to find unused parameters

# Profiling
profiling:
  enabled: false  # Whether to enable profiling
  start_step: 10  # Step to start profiling
  end_step: 20    # Step to end profiling 