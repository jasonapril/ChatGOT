# conf/model/transformer_1m.yaml

_target_: craft.models.transformer.TransformerModel

# Config based on LanguageModelConfig fields
architecture: transformer
vocab_size: 96 # GoT char-level (will likely be overridden by data config)
max_seq_length: 1024

# Model dimensions (~1M parameters with vocab_size=96)
d_model: 128
n_layers: 4
n_head: 4       # d_model(128) % n_head(4) == 0
d_hid: null     # Let it default to d_model * 4 = 512

# Other standard parameters
dropout: 0.1
bias: True
layer_norm_eps: 1e-5
activation: 'gelu'
norm_first: True 