# conf/model/transformer_model.yaml
_target_: craft.models.transformer.TransformerModel
model_type: language # Corrected: Should be the general type
# architecture field is defined within config below

# Instantiate the config object first
config:
  _target_: src.models.base.LanguageModelConfig
  architecture: transformer # This defines the specific architecture
  model_type: ${model.model_type} # Propagate top-level type into config object
  vocab_size: null      # Add placeholder for vocab_size
  # --- Model Architecture Parameters --- #
  # These parameters now belong under the 'config:' key
  d_model: 768
  n_layers: 10
  n_head: 8
  d_hid: 3072 # Adjusted from ${model.d_model}*4
  dropout: 0.1
  # attention_dropout: 0.1 # Not in LanguageModelConfig
  layer_norm_eps: 1e-5
  activation: 'gelu'
  bias: True
  # init_method: 'normal' # Not in LanguageModelConfig
  # init_range: 0.02      # Not in LanguageModelConfig
  # pos_encoding_type: 'learned' # Not in LanguageModelConfig
  max_seq_length: ${data.block_size} # Inherited via GenerativeModelConfig
  # scale_attn_weights: True # Not in LanguageModelConfig
  # use_cache: True          # Not in LanguageModelConfig
  # vocab_size will be set by the data loader later

# Parameters below might be used directly by TransformerModel __init__ IF it were
# modified, or are currently unused if it only takes `config`.
# Keeping them here commented out for reference.
# attention_dropout: 0.1 
# init_method: 'normal'
# init_range: 0.02
# pos_encoding_type: 'learned'
# scale_attn_weights: True
# use_cache: True

# REMOVED parameters previously moved from root level:
# n_positions: ${data.block_size} 
# embd_pdrop: ${model.dropout}

# prevent_dimension_adjustment: true # Not a standard param in base GPTDecoder? Review if needed. 