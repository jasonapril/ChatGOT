# @package _group_
# Minimal Transformer config for testing

_target_: craft.models.transformer.TransformerModel

architecture: transformer # Discriminator for schemas.py
vocab_size: ??? # REQUIRED override from test script
d_model: 16
n_head: 2
d_hid: 32 # Explicitly set for tiny model (4*d_model would be 64)
n_layers: 1
dropout: 0.0 # No dropout for testing
bias: true
layer_norm_eps: 1e-5
activation: relu # Simple activation for testing
norm_first: true
max_seq_length: 32 # Matches data.block_size in experiment 