# conf/model/transformer_10m.yaml

_target_: craft.models.transformer.TransformerModel
model_type: language # Add model_type for clarity/consistency

# Nested config block matching ModelConfig schema
config:
  _target_: craft.models.configs.LanguageModelConfig
  model_type: ${model.model_type} # Inherit model_type
  architecture: transformer
  # vocab_size: 96 # Remove - will be inferred from tokenizer in train.py
  max_seq_length: 1024

  # Model dimensions (~10M parameters)
  d_model: 448
  n_layers: 4
  n_head: 8       # d_model(448) % n_head(8) == 0
  d_hid: null     # Let it default to d_model * 4 = 1792

  # Other standard parameters
  dropout: 0.1
  bias: True
  layer_norm_eps: 1e-5
  activation: 'gelu'
  norm_first: True 