# conf/model/transformer_10m.yaml

_target_: craft.models.transformer.TransformerModel

# Config based on LanguageModelConfig fields
architecture: transformer
vocab_size: 96 # GoT char-level (will likely be overridden by data config)
max_seq_length: 1024

# Model dimensions (~10M parameters with vocab_size=96)
d_model: 448
n_layers: 4
n_head: 8       # d_model(448) % n_head(8) == 0
d_hid: null     # Let it default to d_model * 4 = 1792

# Other standard parameters
dropout: 0.1
bias: True
layer_norm_eps: 1e-5
activation: 'gelu'
norm_first: True 