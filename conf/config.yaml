# conf/config.yaml - Main Hydra configuration file

# @package _global_

defaults:
  # Specify configuration groups to compose.
  # The order matters; later entries override earlier ones.
  - experiment: null      # Expect override from command line or specific config
  - model: transformer_model
  - training: default
  - data: got_char_level
  - optimizer: adamw
  - scheduler: cosine # Revert to match available file
  - callbacks: default    # Default callbacks configuration
  # - tokenizer: default_hf # REMOVE tokenizer group
  - _self_              # Allows defining variables directly in this file
  - override hydra/job_logging: custom # Use custom logging defined below
  - override hydra/hydra_logging: custom

# Example top-level configuration parameters (optional)
# These could also live in their own group files (e.g., conf/paths/default.yaml)
# paths:
#   # Default output directory pattern
#   output_dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}" # Hydra automatically sets this based on hydra.run.dir
#   # Example: Absolute path (use with caution)
#   # data_root: /path/to/datasets 

# General settings
project_name: "Craft"
experiment_name: "default_experiment" # Add experiment name field

# Add other top-level or default settings as needed 

# Configuration for data handling
data: ??? # Ensure this is resolved by defaults

# Configuration for tokenizer (placeholder)
# tokenizer: ??? # REMOVE tokenizer placeholder

# Configuration for the trainer 

# General Run Settings
seed: 42
# Remove checkpoint_dir - will be constructed in script from hydra.run.dir
log_level: "INFO"
force_cpu: False
resume_from: null # Can be set to "latest" or a specific path/to/checkpoint.pt via command line

# Hydra settings (can be overridden)
hydra:
  output_subdir: .hydra
  run:
    # Use original CWD as the base for relative paths in config
    dir: outputs/hydra/${now:%Y-%m-%d}/${now:%H-%M-%S}
  job:
    chdir: true # Change back to true as it was in the working config
  job_logging:
    custom:
      handlers:
        console:
          class: logging.StreamHandler
          formatter: minimal
          level: INFO
        file:
          class: logging.FileHandler
          formatter: standard
          filename: ${hydra.runtime.output_dir}/${hydra.job.name}.log
          level: DEBUG
      loggers:
        root:
          level: DEBUG
          handlers: [console, file]
  sweep:
    dir: multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  hydra_logging:
    custom:
      handlers:
        console:
          class: logging.StreamHandler
          formatter: minimal
          level: INFO
        file:
          class: logging.FileHandler
          formatter: standard
          filename: ${hydra.runtime.output_dir}/hydra.log
          level: DEBUG
      loggers:
        hydra:
          level: DEBUG
          handlers: [console, file]
          propagate: false

# Single, unified logging configuration
logging:
  version: 1
  disable_existing_loggers: false
  formatters:
    simple:
      format: '%(message)s'
    detailed:
      format: '%(asctime)s [%(levelname)s] %(name)s: %(message)s'
  handlers:
    console:
      class: logging.StreamHandler
      level: INFO
      formatter: simple
      stream: ext://sys.stdout
    file:
      class: logging.FileHandler
      level: INFO
      formatter: detailed
      filename: train_runner.log
      mode: w
  loggers:
    root:
      level: INFO
      handlers: [console, file]
      propagate: true
    src:
      level: INFO
      handlers: [console, file]
      propagate: true
    src.training:
      level: INFO
      handlers: [console, file]
      propagate: true
    src.models:
      level: INFO
      handlers: [console, file]
      propagate: true
    src.data:
      level: INFO
      handlers: [console, file]
      propagate: true

# Add a new group for generation parameters
generation:
  start_prompt: "The meaning of life is\n"
  max_new_tokens: 200
  temperature: 0.8
  top_k: null # null corresponds to Python None, or you can set an integer like 50 

# Seed for reproducibility
device: auto

# Optimizer Configuration
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.001 # Example learning rate
  weight_decay: 0.01
  betas: [0.9, 0.999]

# Scheduler Configuration (Optional)
scheduler: # Set to null or omit if no scheduler is desired
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: 100000 # Example: Total number of steps or epochs
  eta_min: 0.00001

# Callbacks Configuration (List)
callbacks: [] # Placeholder list - add specific callbacks here later
# Example:
# callbacks:
#   - _target_: craft.training.callbacks.LoggingCallback
#     log_interval: 10 # Specific args for LoggingCallback
#   - _target_: craft.training.callbacks.EarlyStoppingCallback
#     monitor: 'val_loss'
#     patience: 5

# Training Process Configuration
training:
  epochs: 10
  max_steps: null # Set to an integer to limit training steps
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0 # Set to null to disable gradient clipping
  use_amp: true # Use Automatic Mixed Precision
  log_interval: 100 # Log metrics every N steps
  eval_interval: 1000 # Evaluate on validation set every N steps

  checkpoints:
    # checkpoint_dir is determined by Hydra output directory
    save_interval: 1000 # Save checkpoint every N steps
    keep_last: 2 # Keep only the last N checkpoints
    resume: null # Path to a specific checkpoint to resume from (e.g., 'outputs/YYYY-MM-DD/HH-MM-SS/checkpoints/last.pt')

# Custom Log Formatters
formatters:
  standard:
    format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  minimal:
    format: '%(message)s' 