# conf/config.yaml - Main Hydra configuration file
# This file defines top-level settings and the Hydra output structure.
# It expects a specific experiment configuration to be provided via the command line
# (e.g., `experiment=my_experiment`).
# The selected experiment configuration (e.g., conf/experiment/my_experiment.yaml)
# is responsible for defining its own complete `defaults` list to compose the
# necessary model, data, training, optimizer, etc., configurations using the
# standard Hydra syntax (e.g., `- /group: name`).

# @package _global_

defaults:
  - _self_
  # Experiment config is now mandatory and must be specified on the command line.
  # Hydra will look for the corresponding file in conf/experiment/
  - experiment: ??? # REQUIRED: Example `experiment=chatgot_95m_subword`
  # NOTE: This config.yaml does NOT load base defaults for model, data, training etc.
  # The chosen experiment file MUST load these itself via its defaults list.
  # - override hydra/launcher: ??? # Optional: Configure job launcher if needed (e.g., submitit for SLURM)

# General settings
project_name: "Craft"
# experiment_name: "default" # Removed: Default handled by ExperimentConfig Pydantic schema

# Add other top-level or default settings as needed

# Configuration for data handling - MUST be defined by the loaded experiment config
data: ???

# Configuration for tokenizer - MUST be defined by the loaded experiment config (usually via data)
# tokenizer: ???

# Configuration for the trainer - MUST be defined by the loaded experiment config
training: ???

# General Run Settings
seed: 42
# Remove checkpoint_dir - will be constructed in script from hydra.run.dir
log_level: "DEBUG"
force_cpu: False
resume_from: null # Can be set to "latest" or a specific path/to/checkpoint.pt via command line

# Hydra settings (can be overridden)
hydra:
  output_subdir: .hydra
  run:
    # Single-run output directory: Group by experiment name under 'experiments'
    dir: outputs/experiments/${experiment.experiment_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}
  job:
    chdir: true # Change working directory to the output directory
  sweep:
    # Multi-run (sweep) base directory: Group by experiment name under 'experiments'
    dir: outputs/experiments/${experiment.experiment_name}/multirun/${now:%Y-%m-%d_%H-%M-%S}
    subdir: ${hydra.job.num} # Subdirectory per job within the sweep directory (job number)

# Device selection
device: auto

# Custom Log Formatters
formatters:
  standard:
    format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  minimal:
    format: '%(message)s'

# --- Root Configuration Keys --- #
# These correspond to the top-level structure expected by the application
# and Pydantic schemas where applicable.

# Note: Default values for sub-configs (model, data, etc.) are typically set
#       within the chosen experiment file's defaults list.

model: ???         # Placeholder: Actual model config chosen by experiment
optimizer: ???     # Placeholder: Actual optimizer config chosen by experiment
scheduler: none    # Default: No scheduler unless specified by experiment
callbacks: default # Default: Use callbacks defined in conf/callbacks/default.yaml
checkpointing: default # Default: Use settings from conf/checkpointing/default.yaml
evaluation: default  # Default: Use settings from conf/evaluation/default.yaml

# --- Optional Top-Level Settings --- #

# Global flag for torch.compile (can be overridden by Trainer)
torch_compile: false

# Compile options (if torch_compile is true)
# torch_compile_options:
#   mode: reduce-overhead
#   fullgraph: false 