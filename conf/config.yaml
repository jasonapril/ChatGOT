# conf/config.yaml - Main Hydra configuration file

# @package _global_

defaults:
  # Specify configuration groups to compose.
  # The order matters; later entries override earlier ones.
  - experiment: null      # Expect override from command line or specific config
  - model: transformer_model
  - training: default
  - data: got_char_level
  - optimizer: adamw
  - scheduler: cosine # Revert to match available file
  - callbacks: default    # Default callbacks configuration
  # - tokenizer: default_hf # REMOVE tokenizer group
  - _self_              # Allows defining variables directly in this file
  - override hydra/job_logging: custom # Use custom logging defined below
  - override hydra/hydra_logging: custom

# Example top-level configuration parameters (optional)
# These could also live in their own group files (e.g., conf/paths/default.yaml)
# paths:
#   # Default output directory pattern
#   output_dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}" # Hydra automatically sets this based on hydra.run.dir
#   # Example: Absolute path (use with caution)
#   # data_root: /path/to/datasets 

# General settings
project_name: "Craft"
experiment_name: "default_experiment" # Add experiment name field

# Add other top-level or default settings as needed 

# Configuration for data handling
data: ??? # Ensure this is resolved by defaults

# Configuration for tokenizer (placeholder)
# tokenizer: ??? # REMOVE tokenizer placeholder

# Configuration for the trainer 

# General Run Settings
seed: 42
# Remove checkpoint_dir - will be constructed in script from hydra.run.dir
log_level: "INFO"
force_cpu: False
resume_from: null # Can be set to "latest" or a specific path/to/checkpoint.pt via command line

# Hydra settings (can be overridden)
hydra:
  output_subdir: .hydra
  run:
    # Use original CWD as the base for relative paths in config
    dir: outputs/hydra/${now:%Y-%m-%d}/${now:%H-%M-%S}
  job:
    chdir: true # Change back to true as it was in the working config
  sweep:
    dir: multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}

# Single, unified logging configuration
logging:
  version: 1
  disable_existing_loggers: false
  formatters:
    simple:
      format: '%(message)s'
    detailed:
      format: '%(asctime)s [%(levelname)s] %(name)s: %(message)s'
  handlers:
    console:
      class: logging.StreamHandler
      level: INFO
      formatter: simple
      stream: ext://sys.stdout
    file:
      class: logging.FileHandler
      level: INFO
      formatter: detailed
      filename: train_runner.log
      mode: w
  loggers:
    root:
      level: INFO
      handlers: [console, file]
      propagate: true
    src:
      level: INFO
      handlers: [console, file]
      propagate: true
    src.training:
      level: INFO
      handlers: [console, file]
      propagate: true
    src.models:
      level: INFO
      handlers: [console, file]
      propagate: true
    src.data:
      level: INFO
      handlers: [console, file]
      propagate: true

# Add a new group for generation parameters
generation:
  start_prompt: "The meaning of life is\n"
  max_new_tokens: 200
  temperature: 0.8
  top_k: null # null corresponds to Python None, or you can set an integer like 50 

# Seed for reproducibility
device: auto

# Optimizer Configuration
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.001 # Example learning rate
  weight_decay: 0.01
  betas: [0.9, 0.999]

# Scheduler Configuration (Optional)
scheduler: # Set to null or omit if no scheduler is desired
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: 100000 # Example: Total number of steps or epochs
  eta_min: 0.00001

# Callbacks Configuration - Let the defaults/overrides handle this
# callbacks: []

# Custom Log Formatters
formatters:
  standard:
    format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  minimal:
    format: '%(message)s' 