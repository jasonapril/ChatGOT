# conf/optimizer/adamw.yaml
_target_: torch.optim.AdamW
lr: 1e-4 # Default learning rate, can be overridden
# type: adamw # REMOVED - Redundant with _target_ in main config
# learning_rate: 1e-4 # REMOVED - Use 'lr' defined in main config or overrides
weight_decay: 0.01
# Other AdamW params like betas, eps can be added here if needed
# betas: [0.9, 0.999]
# eps: 1e-8 