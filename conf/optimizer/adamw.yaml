# conf/optimizer/adamw.yaml
_target_: torch.optim.AdamW
lr: 1e-4 # Default learning rate (override in experiment)
# type: adamw # REMOVED - Redundant with _target_ in main config
# learning_rate: 1e-4 # REMOVED - Use 'lr' defined in main config or overrides
weight_decay: 0.01 # Default weight decay (override in experiment)
# Other AdamW params like betas, eps can be added here if needed
betas: [0.9, 0.999]
eps: 1e-8 