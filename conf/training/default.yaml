# conf/training/default.yaml

epochs: 1 # Default to 1 epoch, override in experiment if needed
use_amp: true
gradient_accumulation_steps: 1 # Default, override as needed
max_grad_norm: 1.0
log_interval: 10 # Log every 10 steps
save_steps_interval: 5000 # Save every 5000 steps
time_save_interval_seconds: 300 # Save every 5 minutes
compile_model: false # Torch compile (experimental)
activation_checkpointing: true # Enable gradient checkpointing
torch_compile: false # Separate flag if needed

# Sample generation defaults (can be overridden in experiment)
sample_max_new_tokens: 100
sample_temperature: 0.8
sample_start_text: "\n" # Default start text

# These rely on experiment config values, so use interpolations
batch_size: ${experiment.data.batch_size}
max_steps: null # Default to None, use epochs or override in experiment
# Optimizer/Scheduler related params
warmup_steps: 100
weight_decay: 0.01 # Default weight decay

# --- Generation settings specific to training loop evaluation/sampling ---
generation:
  _target_: craft.core.generation.GeneratorConfig # Ensure this target is correct
  prompt: ${experiment.training.generation.start_prompt} # Interpolate directly from experiment config
  max_new_tokens: ${..sample_max_new_tokens} # Use training config sample settings
  temperature: ${..sample_temperature}
  top_k: 50 # Default Top-K for sampling during training
  top_p: 0.9 # Default Top-P for sampling
  repetition_penalty: 1.1 # Default repetition penalty 