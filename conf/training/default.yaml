# conf/training/default.yaml

num_epochs: 1 # Default to 1 epoch, override in experiment if needed
use_amp: true
gradient_accumulation_steps: 1 # Number of steps to accumulate gradients before optimizer step
max_grad_norm: 1.0
log_interval: 10 # Log every 10 steps
save_interval: 5000 # Renamed from save_steps_interval
time_save_interval_seconds: 300 # Save every 5 minutes
compile_model: false # Torch compile (experimental)
activation_checkpointing: true # Enable gradient checkpointing
torch_compile: false # Separate flag if needed

# These rely on experiment config values, so use interpolations
batch_size: ${experiment.data.batch_size}
max_steps: null # Default to None, use epochs or override in experiment
# Optimizer/Scheduler related params
warmup_steps: 100

# --- Generation settings specific to training loop evaluation/sampling ---
generation:
  _target_: craft.config.schemas.GenerationConfig # Corrected path
  top_k: 50 # Default Top-K for sampling during training
  top_p: 0.9 # Default Top-P for sampling
  # repetition_penalty: 1.1 # REMOVED Default repetition penalty 