# conf/training/default.yaml
epochs: 5 # Set to 5 epochs
use_amp: True
gradient_accumulation_steps: 2
max_grad_norm: 1.0
log_interval: 1 # Log every batch
save_steps_interval: 1000 # Save checkpoint every N steps (0 or less to disable step-based saving)
time_save_interval_seconds: 300 # 5 minutes (Handled by callback)
compile_model: False
activation_checkpointing: False
torch_compile: False # Add default for torch.compile flag

# Sampling configuration (used during timed saves)
sample_max_new_tokens: 100
sample_temperature: 0.8
sample_start_text: "The meaning of life is"

# REMOVE Redundant generation block (already defined in config.yaml)
# Generation config used by Trainer._generate_sample_and_log
generation:
  prompt: "The night is dark and full of"
  max_new_tokens: 200
  temperature: 0.8
  top_k: 50
  top_p: 0.9
  repetition_penalty: 1.1

# Default training configuration - REMOVED nested 'training:' key
# Training settings - these are now top-level within this file
batch_size: 32
learning_rate: 3e-4
max_steps: 1000
warmup_steps: 100
weight_decay: 0.1 