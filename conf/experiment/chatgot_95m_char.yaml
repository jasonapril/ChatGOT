# Configuration for ChatGoT ~100M parameters - Refactored for Hydra/Trainer

# Hydra settings (optional, provides defaults)
defaults:
  - _self_
  # - model: gpt_decoder # REMOVED - Inherit from main config
  # The rest of the defaults should come from the main config.yaml
  # We only override specific sections below.

# Configuration for the experiment run
experiment_name: "chatgot_95m_char" # Reflects model size and level
experiment_id: ${experiment_name}_${now:%Y%m%d_%H%M%S} # Unique ID for the run
resume_from: latest # Attempt to resume from latest checkpoint matching filters

# System settings (Overriding defaults if needed)
system:
  seed: 42
  device: auto # Trainer will attempt auto-detection

# Model configuration is now primarily defined in conf/model/gpt_decoder.yaml
# We could override specific parameters here if needed, e.g.:
# model:
#   dropout: 0.15 

# Data configuration (Overriding defaults)
# data:
#   _target_: src.data.text.build_text_dataloaders # Example target data builder
#   file_path: data/raw/got/game_of_thrones.txt # Renamed from 'path'
#   # tokenizer_name: gpt2 # REMOVED - Using CharDataset defined in data config
#   vocab_size: null # Let the data loader determine this from tokenizer
#   block_size: 256
#   batch_size: 32
#   num_workers: 4 # Explicitly set

# Training configuration (Overrides general training params)
training:
  num_epochs: 10
  learning_rate: 5e-4 # Define the base LR for this experiment HERE
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0
  use_amp: true
  gradient_checkpointing: false
  log_interval: 10
  eval_interval: 500
  # activation_checkpointing: false # Inherit from default
  # torch_compile: false # Inherit from default
  # checkpoint_subdir: 'checkpoints' # Inherit from default

# Optimizer configuration (Overrides optimizer group)
optimizer:
  # type: adamw # Inherited from config.yaml default
  learning_rate: 5e-4 # Set directly, remove interpolation
  weight_decay: 0.01 # Specify override if different from default adamw config

# Scheduler configuration (Overrides scheduler group)
scheduler:
  # type: cosine # Inherited from config.yaml default
  num_warmup_steps: 1000

# Callbacks configuration (Overrides callbacks group)
callbacks:
  callbacks_list: # Re-enable callbacks
    - _target_: src.training.callbacks.TensorBoardLogger
      log_dir: outputs/tensorboard/${experiment_name}_${now:%Y%m%d_%H%M%S}
    - _target_: src.training.callbacks.SampleGenerationCallback
      log_interval: 1000
      max_length: 400
      temperature: 0.8
      num_samples: 2
      prompt: 'The night is dark and full of terrors'

# Configuration for the tokenizer path
# tokenizer_path: gpt2 # REMOVED - Not used by CharDataset

# MLflow configuration (optional)
mlflow:
  experiment_name: ${experiment_name} # Use top-level name
  log_params: true
  log_metrics: true 