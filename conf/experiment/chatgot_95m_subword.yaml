# conf/experiment/chatgot_95m_subword.yaml
defaults:
  - /model: transformer_95m
  - /training: default # Use correct filename
  - /data: got_subword # Switched to full dataset subword config
  - /optimizer: adamw
  - /scheduler: cosine # Corrected filename from cosine_annealing
  - /callbacks: default # Use correct filename
  - /generation: default # Use correct filename

# Experiment Specific Overrides
experiment_name: chatgot_95m_subword

# Defaults from conf/config.yaml are merged unless overridden here

# --- Overrides for Data Config --- #
data:
  batch_size: 16
  num_workers: 0
  block_size: 1024
  # Override default tokenizer config - assume this tokenizer is already trained
  # The actual tokenizer logic should handle loading this based on model_path
  tokenizer:
    _target_: craft.data.tokenizers.subword.SubwordTokenizer
    # REMOVED nested config, provide path directly if tokenizer load supports it
    # If load expects a config object, keep this structure but ensure it matches load method
    config:
      model_path: outputs/tokenizers/got_subword_8k
      vocab_size: 8000 # This might be ignored if loading pre-trained

  # Override datasets config
  datasets:
    train:
      dataset:
        _target_: craft.data.dataset.TextDataset
        file_paths:
          - data/raw/got/game_of_thrones.txt
        block_size: ${experiment.data.block_size}
        tokenizer: ${experiment.data.tokenizer}
      dataloader: {}
    val:
      dataset:
        _target_: craft.data.dataset.TextDataset
        file_paths:
          - data/raw/got/game_of_thrones.txt
        block_size: ${experiment.data.block_size}
        tokenizer: ${experiment.data.tokenizer}
      dataloader: {}

# --- Overrides for Model Config --- #
model:
  # Override config params inherited from default model config
  config:
    d_model: 768
    n_layers: 10
    n_head: 8
    d_hid: 3072
    max_seq_length: ${experiment.data.block_size}

# --- Overrides for Training Config --- #
training:
  log_interval: 1
  save_steps_interval: 1000
  batch_size: ${experiment.data.batch_size}
  max_steps: 1000
  warmup_steps: 100
  weight_decay: 0.1
  sample_start_text: The night is dark and full of
  # Specify any other training overrides here

# --- Overrides for Optimizer Config --- #
optimizer:
  # Override optimizer params inherited from default optimizer config
  lr: 0.0001
  weight_decay: ${experiment.training.weight_decay}

# --- Overrides for Scheduler Config --- #
scheduler:
  # Override scheduler params inherited from default scheduler config
  T_max: ${experiment.training.max_steps}
  eta_min: 1.0e-06

# --- Overrides for Callbacks --- #
callbacks:
  tensorboard:
    _target_: craft.training.callbacks.TensorBoardLogger
    # log_dir: will be derived from output dir
  sample_generation:
    _target_: craft.training.callbacks.SampleGenerationCallback
    start_prompt: ${experiment.generation.start_prompt}
    # Add any other generation specific overrides here

# --- Optional Generation Overrides (if needed) --- #
# generation:
#   temperature: 0.7
#   top_k: 50

# generation settings seem misplaced here, should be top level or specific tool config
# generation:
#   prompt: The night is dark and full of
#   max_new_tokens: 200
#   temperature: 0.8
#   top_k: 50
#   top_p: 0.9
#   repetition_penalty: 1.1 