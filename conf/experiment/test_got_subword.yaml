# conf/experiment/test_got_subword.yaml
# Defines an experiment for quick end-to-end testing using the full subword tokenizer.

# @package _global_
defaults:
  - /data: got_subword      # Include base got_subword config
  - /model: transformer_debug  # Fix: Use 'transformer_debug'
  - /optimizer: adamw        # Fix: Use 'adamw', not 'adamw_common'
  - /training: default      # Fix: Use 'default', not 'default_train'
  - _self_                  # Include local settings from this file last
  # Inherit scheduler (often None by default)
  # Inherit model, training, optimizer, scheduler, generation defaults from main config

# Configuration for the experiment run
experiment_name: test_got_subword
resume_from: null

# Data configuration (Override block_size if needed, inherits tokenizer from got_subword)
data:
  block_size: 256 # Define top-level block size

# Training configuration (Override for minimal run, enable checkpointing/sampling)
training:
  batch_size: 32          # Ensure batch_size is defined here or inherited
  num_epochs: 1           # Run for only 1 epoch
  max_steps: 10           # Limit to only ~10 training steps for speed
  log_interval: 1         # Log every step
  save_steps_interval: 5  # Save checkpoint every 5 steps
  time_save_interval_seconds: 0 # Disable time-based saving for this short test
  compile_model: False
  torch_compile: False
  activation_checkpointing: false # Keep false for speed unless testing this feature specifically

  # Use minimal generation settings for sample callback test
  generation:
    max_new_tokens: 20 # Generate very short samples
    start_prompt: "The " # Add default start prompt

# Callbacks configuration (Enable minimal necessary callbacks)
callbacks:
  # Keep only essential callbacks for testing save/generate
  tensorboard: # Minimal TensorBoardLogger
    _target_: craft.training.callbacks.TensorBoardLogger
  sample_generation: # Minimal SampleGenerationCallback
    _target_: craft.training.callbacks.SampleGenerationCallback
    start_prompt: ${experiment.training.generation.start_prompt} # Correct interpolation
  # progress_bar: # Optionally add progress bar if desired for visibility
  #   _target_: craft.training.callbacks.ProgressBarCallback

# Disable MLflow logging for tests
mlflow:
  log_params: false
  log_metrics: false

# Explicitly override model block_size if needed for memory, otherwise inherit
model:
  _target_: craft.models.transformer.TransformerModel
  config:
    _target_: craft.config.schemas.LanguageModelConfig # Use the Pydantic schema for validation
    architecture: transformer # Required discriminator
    vocab_size: null # Will be inferred from data
    d_model: 128
    n_layers: 2
 