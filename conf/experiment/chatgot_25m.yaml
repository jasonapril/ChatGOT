# Configuration for ChatGoT Small (~25M parameters) - Refactored for Hydra/Trainer

# Hydra settings (optional, provides defaults)
defaults:
  - _self_
  # - override hydra/job_logging: custom # Example for custom logging setup
  # - override hydra/hydra_logging: custom

# Base experiment name (can be used by MLflow or other loggers)
experiment_name: chatgot_small_25m_hydra

# System settings
system:
  seed: 42
  device: auto # Trainer will attempt auto-detection

# Model configuration
model:
  _target_: src.models.gpt_decoder.GPTDecoder # Specify the target class
  # vocab_size: 96 # Let data loader set this based on tokenizer
  d_model: 512
  n_layers: 8
  n_head: 8
  d_hid: 2048 # Typically calculated as 4 * d_model, but keeping original value
  dropout: 0.1
  attention_dropout: 0.1 # Same as 'attn_pdrop' from original
  layer_norm_eps: 1e-5
  activation: gelu
  bias: true
  init_method: normal
  init_range: 0.02 # Using 'initializer_range' from original
  pos_encoding_type: learned
  max_seq_length: ${data.block_size} # Use block_size from data config
  # prevent_dimension_adjustment: true # Not a standard param in base GPTDecoder? Review if needed.
  # Parameters moved from root level:
  n_positions: ${data.block_size} # Match block_size
  embd_pdrop: ${model.dropout} # Use model dropout
  scale_attn_weights: true
  use_cache: true # Relevant for inference, might not affect training directly

# Data configuration
data:
  _target_: src.data.text.build_text_dataloaders # Example target data builder
  file_path: data/raw/got/game_of_thrones.txt # Renamed from 'path'
  # format: text # Implicit in build_text_dataloaders, or add as param if needed
  tokenizer_name: "gpt2" # Placeholder - Update with actual tokenizer name/path used!
  vocab_size: null # Let the data loader determine this from tokenizer
  block_size: 256
  batch_size: 32
  num_workers: 4 # Moved from 'training' section

# Training configuration
training:
  _target_: src.training.base.LanguageModelTrainer # Specify Trainer class (optional if using create_trainer_from_config)
  # model: instantiated via run script
  # train_dataloader: instantiated via run script
  # eval_dataloader: instantiated via run script
  # optimizer and scheduler are configured below
  num_epochs: 10
  learning_rate: 5e-4 # Base LR for optimizer/scheduler setup
  optimizer:
    _target_: torch.optim.AdamW
    lr: ${training.learning_rate}
    weight_decay: 0.01
    # Add other AdamW params like betas if needed
  scheduler:
    _target_: src.training.schedulers.get_cosine_schedule_with_warmup # Example target scheduler function
    # Requires num_training_steps, passed by Trainer. Can add parameters here if needed.
    num_warmup_steps: 1000 # Renamed from warm_up_steps
    # num_training_steps: automatically calculated by Trainer based on epochs/dataset size
  gradient_accumulation_steps: 1 # Renamed from accumulate_grad_batches
  max_grad_norm: 1.0 # Renamed from clip_grad_norm
  use_amp: true # Renamed from mixed_precision
  # compile_model: false # Trainer does not handle this directly yet
  gradient_checkpointing: true # Moved from 'training' section
  log_interval: 50 # Default value, adjust as needed
  eval_interval: 500 # Default value, adjust as needed
  # save_strategy: 'best_val_loss' # Example save strategy
  # save_interval: ${training.eval_interval} # Example: save checkpoints during evaluation
  # max_checkpoints_to_keep: 3 # Trainer needs logic update or callback for this

# Callbacks configuration
callbacks:
  - _target_: src.training.callbacks.TensorBoardLogger
    log_dir: ${hydra:run.dir}/tensorboard # Log relative to Hydra output dir

  - _target_: src.training.callbacks.SampleGenerationCallback
    log_interval: 1000 # Original 'sample_every'
    max_length: 400 # Original 'sample_length'
    temperature: 0.8 # Original 'sample_temperature'
    num_samples: 2 # Default value, add if needed
    prompt: "The night is dark and full of terrors" # Example prompt
    # tokenizer: will be injected by runner script

  # Example memory optimization callback (if implemented)
  # - _target_: src.training.callbacks.MemoryProfilerCallback
  #   log_interval: 100 # Original 'empty_cache_freq' could map here

# Tokenizer path (needed by runner.py for SampleGenerationCallback injection)
# Should match data.tokenizer_name or be a direct path if loading from file
tokenizer_path: "gpt2" # Placeholder - Update with actual path or name!

# MLflow configuration (optional, if runner uses it or if MLflowLogger callback is added)
mlflow:
  experiment_name: ${experiment_name} # Use top-level name
  log_params: true
  log_metrics: true
  # tracking_uri: <your_mlflow_server_uri_if_not_local>
  # run_name: ${hydra:job.name}-${now:%Y%m%d_%H%M%S} # Example run name 