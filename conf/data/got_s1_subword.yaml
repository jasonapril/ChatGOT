# conf/data/got_s1_subword.yaml
# Configuration for loading GoT Season 1 text data and using a Subword BPE tokenizer.

# Define settings directly
batch_size: 16 # Added back: Required by DataConfig schema
num_workers: 0 # Example num workers
block_size: 1024 # Or inherit via ${data.block_size} if defined in default_data_settings

# --- Tokenizer Configuration ---
tokenizer:
  _target_: craft.data.tokenizers.subword.SubwordTokenizer
  # Configuration used during tokenizer TRAINING (and needed for loading info)
  config:
    # Path where the TRAINED tokenizer model is saved/loaded from (Passed via config)
    # --- Pointing this base config to the FULL dataset tokenizer --- #
    model_path: outputs/tokenizers/got_full_subword_8k
    vocab_size: 8000
    # Default special tokens will be used unless overridden here or during training script run
    # pad_token: "<pad>"
    # unk_token: "<unk>"
    # bos_token: "<s>"
    # eos_token: "</s>"

# --- Dataset Split Configurations (Nested) --- #
datasets:
  train:
    dataset:
      _target_: craft.data.dataset.TextDataset # Assumes this class exists
      # --- Use the FULL dataset path here too --- #
      file_paths:
        - data/raw/got/game_of_thrones.txt
      block_size: ${data.block_size}
      tokenizer: ${data.tokenizer} # Use the tokenizer defined above
    dataloader: {} # Add empty dataloader section

  val:
    dataset:
      _target_: craft.data.dataset.TextDataset # Assumes this class exists
      # --- Use the FULL dataset path here too --- #
      file_paths:
        - data/raw/got/game_of_thrones.txt
      block_size: ${data.block_size}
      tokenizer: ${data.tokenizer} # Use the tokenizer defined above
    dataloader: {} # Add empty dataloader section

# test: null # No test set defined for now 