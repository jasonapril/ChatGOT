# Data configuration for Game of Thrones with subword tokenization
data:
  # Data type
  type: text

  # Data paths
  train_path: data/raw/got/game_of_thrones.txt
  val_path: data/raw/got/game_of_thrones.txt
  processed_dir: data/processed/got

  # Processing settings
  block_size: 256
  train_split: 0.9
  val_split: 0.1
  num_workers: 4
  batch_size: 32

  # Tokenizer settings
  tokenizer:
    type: subword
    vocab_size: 32000
    pad_token: "<pad>"
    unk_token: "<unk>"
    bos_token: "<s>"
    eos_token: "</s>"

  # Dataset settings
  dataset:
    type: pickled
    train_file: game_of_thrones_train.pkl
    val_file: game_of_thrones_val.pkl
    block_size: 256
    num_workers: 4
    batch_size: 32 