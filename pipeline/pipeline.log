2025-03-23 13:47:59 - Logging to file: C:\Users\nimbu\Desktop\Code\AI\ChatGoT\pipeline\pipeline.log
2025-03-23 13:47:59 - === LOGGING STARTED AT 2025-03-23 13:47:59 ===
2025-03-23 13:47:59 - Logger initialized with console and file output
2025-03-23 13:48:00 - Random seed set to 42 for reproducibility
2025-03-23 13:48:00 - Using GPU: NVIDIA GeForce GTX 1650 Ti
2025-03-23 13:48:00 - Number of GPUs: 1
2025-03-23 13:48:00 - CUDA Version: 11.8
2025-03-23 13:48:00 - GPU Memory: 4.00 GB (Free: 3.23 GB)
2025-03-23 13:48:00 - Starting pipeline from stage: process
2025-03-23 13:48:00 - ================================================================================
2025-03-23 13:48:00 -                              RUNNING STAGE: PROCESS                             
2025-03-23 13:48:00 - ================================================================================
2025-03-23 13:48:00 - Skipping data processing stage as requested.
2025-03-23 13:48:00 - Completed stage: process
2025-03-23 13:48:00 - ================================================================================
2025-03-23 13:48:00 -                             RUNNING STAGE: OPTIMIZE                             
2025-03-23 13:48:00 - ================================================================================
2025-03-23 13:48:00 - Finding optimal training settings...
2025-03-23 13:48:00 - [CUDA] Enabled TF32 precision for matrix operations
2025-03-23 13:48:00 - [CUDA] Optimized cuDNN settings for maximum performance
2025-03-23 13:48:00 - [CUDA] Using optimized scaled_dot_product_attention implementation
2025-03-23 13:48:00 - [CUDA] Optimized CUDA memory allocation settings
2025-03-23 13:48:00 - [CUDA] Optimizations applied for: NVIDIA GeForce GTX 1650 Ti (CUDA 11.8)
2025-03-23 13:48:00 - [CUDA] Applied GTX 1650 Ti specific optimizations
2025-03-23 13:48:00 - CUDA optimization results: {'cuda_available': True, 'tf32_enabled': True, 'cudnn_optimized': True, 'memory_optimized': True, 'gpu_specific_optimizations': True}
2025-03-23 13:48:00 - Using GPU: NVIDIA GeForce GTX 1650 Ti
2025-03-23 13:48:00 - Number of GPUs: 1
2025-03-23 13:48:00 - CUDA Version: 11.8
2025-03-23 13:48:00 - GPU Memory: 4.00 GB (Free: 3.23 GB)
2025-03-23 13:48:00 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:48:00 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:48:00 - Vocabulary size: 89 characters
2025-03-23 13:48:01 - Created data loaders: 19268 training batches, 536 validation batches
2025-03-23 13:48:01 - Model initialized with 768 dimensions, 85,192,793 parameters
2025-03-23 13:48:01 - Trainable parameters: 85,192,793
2025-03-23 13:48:01 - Estimated model size: 327.98MB
2025-03-23 13:48:01 - Created transformer model with 85,192,793 parameters
2025-03-23 13:48:01 - Model config: d_model=768, n_head=12, d_hid=3072, n_layers=12
2025-03-23 13:48:01 - Using standard GPT-2 Small architecture
2025-03-23 13:48:01 - Using memory-efficient attention implementation
2025-03-23 13:48:01 - Starting batch size optimization search: 1 to 256
2025-03-23 13:48:01 - Testing with sequence length: 1024
2025-03-23 13:48:01 - Testing batch size: 128
2025-03-23 13:48:04 - Error in stage optimize: load_data() got an unexpected keyword argument 'sequence_length'
Traceback (most recent call last):
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\pipeline.py", line 252, in run
    stage_method()
    ~~~~~~~~~~~~^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\pipeline.py", line 350, in run_optimize
    optimal_batch_size = find_optimal_batch_size(bsf_args)
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\src\batch_size_finder.py", line 208, in find_optimal_batch_size
    throughput, success = measure_throughput(
                          ~~~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<4 lines>...
        test_batches=args.test_batches
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\src\batch_size_finder.py", line 81, in measure_throughput
    train_loader, _, _, _ = load_data(
                            ~~~~~~~~~^
        data_path=data_path,
        ^^^^^^^^^^^^^^^^^^^^
    ...<3 lines>...
        device_type=device.type
        ^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
TypeError: load_data() got an unexpected keyword argument 'sequence_length'
2025-03-23 13:48:38 - Logging to file: C:\Users\nimbu\Desktop\Code\AI\ChatGoT\pipeline\pipeline.log
2025-03-23 13:48:38 - === LOGGING STARTED AT 2025-03-23 13:48:38 ===
2025-03-23 13:48:38 - Logger initialized with console and file output
2025-03-23 13:48:39 - Random seed set to 42 for reproducibility
2025-03-23 13:48:39 - Using GPU: NVIDIA GeForce GTX 1650 Ti
2025-03-23 13:48:39 - Number of GPUs: 1
2025-03-23 13:48:39 - CUDA Version: 11.8
2025-03-23 13:48:39 - GPU Memory: 4.00 GB (Free: 3.23 GB)
2025-03-23 13:48:39 - Starting pipeline from stage: process
2025-03-23 13:48:39 - ================================================================================
2025-03-23 13:48:39 -                              RUNNING STAGE: PROCESS                             
2025-03-23 13:48:39 - ================================================================================
2025-03-23 13:48:39 - Skipping data processing stage as requested.
2025-03-23 13:48:39 - Completed stage: process
2025-03-23 13:48:39 - ================================================================================
2025-03-23 13:48:39 -                             RUNNING STAGE: OPTIMIZE                             
2025-03-23 13:48:39 - ================================================================================
2025-03-23 13:48:39 - Finding optimal training settings...
2025-03-23 13:48:39 - [CUDA] Enabled TF32 precision for matrix operations
2025-03-23 13:48:39 - [CUDA] Optimized cuDNN settings for maximum performance
2025-03-23 13:48:39 - [CUDA] Using optimized scaled_dot_product_attention implementation
2025-03-23 13:48:39 - [CUDA] Optimized CUDA memory allocation settings
2025-03-23 13:48:39 - [CUDA] Optimizations applied for: NVIDIA GeForce GTX 1650 Ti (CUDA 11.8)
2025-03-23 13:48:39 - [CUDA] Applied GTX 1650 Ti specific optimizations
2025-03-23 13:48:39 - CUDA optimization results: {'cuda_available': True, 'tf32_enabled': True, 'cudnn_optimized': True, 'memory_optimized': True, 'gpu_specific_optimizations': True}
2025-03-23 13:48:39 - Using GPU: NVIDIA GeForce GTX 1650 Ti
2025-03-23 13:48:39 - Number of GPUs: 1
2025-03-23 13:48:39 - CUDA Version: 11.8
2025-03-23 13:48:39 - GPU Memory: 4.00 GB (Free: 3.23 GB)
2025-03-23 13:48:39 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:48:39 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:48:39 - Vocabulary size: 89 characters
2025-03-23 13:48:39 - Created data loaders: 19268 training batches, 536 validation batches
2025-03-23 13:48:40 - Model initialized with 768 dimensions, 85,192,793 parameters
2025-03-23 13:48:40 - Trainable parameters: 85,192,793
2025-03-23 13:48:40 - Estimated model size: 327.98MB
2025-03-23 13:48:40 - Created transformer model with 85,192,793 parameters
2025-03-23 13:48:40 - Model config: d_model=768, n_head=12, d_hid=3072, n_layers=12
2025-03-23 13:48:40 - Using standard GPT-2 Small architecture
2025-03-23 13:48:40 - Using memory-efficient attention implementation
2025-03-23 13:48:40 - Starting batch size optimization search: 1 to 256
2025-03-23 13:48:40 - Testing with sequence length: 1024
2025-03-23 13:48:40 - Testing batch size: 128
2025-03-23 13:48:42 - Error in stage optimize: load_data() got an unexpected keyword argument 'max_seq_length'
Traceback (most recent call last):
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\pipeline.py", line 252, in run
    stage_method()
    ~~~~~~~~~~~~^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\pipeline.py", line 350, in run_optimize
    optimal_batch_size = find_optimal_batch_size(bsf_args)
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\src\batch_size_finder.py", line 208, in find_optimal_batch_size
    throughput, success = measure_throughput(
                          ~~~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<4 lines>...
        test_batches=args.test_batches
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\src\batch_size_finder.py", line 81, in measure_throughput
    train_loader, _, _, _ = load_data(
                            ~~~~~~~~~^
        data_path=data_path,
        ^^^^^^^^^^^^^^^^^^^^
    ...<3 lines>...
        device_type=device.type
        ^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
TypeError: load_data() got an unexpected keyword argument 'max_seq_length'
2025-03-23 13:49:34 - Logging to file: C:\Users\nimbu\Desktop\Code\AI\ChatGoT\pipeline\pipeline.log
2025-03-23 13:49:34 - === LOGGING STARTED AT 2025-03-23 13:49:34 ===
2025-03-23 13:49:34 - Logger initialized with console and file output
2025-03-23 13:49:35 - Random seed set to 42 for reproducibility
2025-03-23 13:49:35 - Using GPU: NVIDIA GeForce GTX 1650 Ti
2025-03-23 13:49:35 - Number of GPUs: 1
2025-03-23 13:49:35 - CUDA Version: 11.8
2025-03-23 13:49:35 - GPU Memory: 4.00 GB (Free: 3.23 GB)
2025-03-23 13:49:35 - Starting pipeline from stage: process
2025-03-23 13:49:35 - ================================================================================
2025-03-23 13:49:35 -                              RUNNING STAGE: PROCESS                             
2025-03-23 13:49:35 - ================================================================================
2025-03-23 13:49:35 - Skipping data processing stage as requested.
2025-03-23 13:49:35 - Completed stage: process
2025-03-23 13:49:35 - ================================================================================
2025-03-23 13:49:35 -                             RUNNING STAGE: OPTIMIZE                             
2025-03-23 13:49:35 - ================================================================================
2025-03-23 13:49:35 - Finding optimal training settings...
2025-03-23 13:49:35 - [CUDA] Enabled TF32 precision for matrix operations
2025-03-23 13:49:35 - [CUDA] Optimized cuDNN settings for maximum performance
2025-03-23 13:49:35 - [CUDA] Using optimized scaled_dot_product_attention implementation
2025-03-23 13:49:35 - [CUDA] Optimized CUDA memory allocation settings
2025-03-23 13:49:35 - [CUDA] Optimizations applied for: NVIDIA GeForce GTX 1650 Ti (CUDA 11.8)
2025-03-23 13:49:35 - [CUDA] Applied GTX 1650 Ti specific optimizations
2025-03-23 13:49:35 - CUDA optimization results: {'cuda_available': True, 'tf32_enabled': True, 'cudnn_optimized': True, 'memory_optimized': True, 'gpu_specific_optimizations': True}
2025-03-23 13:49:35 - Using GPU: NVIDIA GeForce GTX 1650 Ti
2025-03-23 13:49:35 - Number of GPUs: 1
2025-03-23 13:49:35 - CUDA Version: 11.8
2025-03-23 13:49:35 - GPU Memory: 4.00 GB (Free: 3.23 GB)
2025-03-23 13:49:35 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:49:35 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:49:35 - Vocabulary size: 89 characters
2025-03-23 13:49:35 - Created data loaders: 19268 training batches, 536 validation batches
2025-03-23 13:49:36 - Model initialized with 768 dimensions, 85,192,793 parameters
2025-03-23 13:49:36 - Trainable parameters: 85,192,793
2025-03-23 13:49:36 - Estimated model size: 327.98MB
2025-03-23 13:49:36 - Created transformer model with 85,192,793 parameters
2025-03-23 13:49:36 - Model config: d_model=768, n_head=12, d_hid=3072, n_layers=12
2025-03-23 13:49:36 - Using standard GPT-2 Small architecture
2025-03-23 13:49:36 - Using memory-efficient attention implementation
2025-03-23 13:49:36 - Starting batch size optimization search: 1 to 256
2025-03-23 13:49:36 - Testing with sequence length: 1024
2025-03-23 13:49:36 - Testing batch size: 128
2025-03-23 13:49:38 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:49:38 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:49:38 - Vocabulary size: 89 characters
2025-03-23 13:49:38 - Created data loaders: 150 training batches, 17 validation batches
2025-03-23 13:49:42 - Batch size 128 caused OOM: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. 3.80 GiB allowed; Of the allocated memory 3.73 GiB is allocated by PyTorch, and 60.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-23 13:49:42 - Testing batch size: 64
2025-03-23 13:49:42 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:49:42 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:49:42 - Vocabulary size: 89 characters
2025-03-23 13:49:42 - Created data loaders: 301 training batches, 17 validation batches
2025-03-23 13:49:46 - Batch size 64 caused OOM: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. 3.80 GiB allowed; Of the allocated memory 3.72 GiB is allocated by PyTorch, and 82.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-23 13:49:46 - Testing batch size: 32
2025-03-23 13:49:46 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:49:46 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:49:46 - Vocabulary size: 89 characters
2025-03-23 13:49:46 - Created data loaders: 602 training batches, 17 validation batches
2025-03-23 13:50:00 - Batch size 32 caused OOM: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. 3.80 GiB allowed; Of the allocated memory 3.69 GiB is allocated by PyTorch, and 109.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-23 13:50:00 - Testing batch size: 16
2025-03-23 13:50:00 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:50:00 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:50:00 - Vocabulary size: 89 characters
2025-03-23 13:50:00 - Created data loaders: 1204 training batches, 34 validation batches
2025-03-23 13:50:28 - Batch size 16 successful: 756.29 tokens/sec
2025-03-23 13:50:28 - Testing batch size: 24
2025-03-23 13:50:28 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:50:28 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:50:28 - Vocabulary size: 89 characters
2025-03-23 13:50:28 - Created data loaders: 802 training batches, 23 validation batches
2025-03-23 13:51:09 - Batch size 24 successful: 750.27 tokens/sec
2025-03-23 13:51:09 - Testing batch size: 28
2025-03-23 13:51:09 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:51:09 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:51:09 - Vocabulary size: 89 characters
2025-03-23 13:51:09 - Created data loaders: 688 training batches, 20 validation batches
2025-03-23 13:51:57 - Batch size 28 successful: 743.39 tokens/sec
2025-03-23 13:51:57 - Testing batch size: 30
2025-03-23 13:51:57 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:51:57 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:51:57 - Vocabulary size: 89 characters
2025-03-23 13:51:57 - Created data loaders: 642 training batches, 18 validation batches
2025-03-23 13:52:07 - Batch size 30 caused OOM: CUDA out of memory. Tried to allocate 46.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. 3.80 GiB allowed; Of the allocated memory 3.66 GiB is allocated by PyTorch, and 123.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-23 13:52:07 - Testing batch size: 29
2025-03-23 13:52:07 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:52:07 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:52:07 - Vocabulary size: 89 characters
2025-03-23 13:52:07 - Created data loaders: 664 training batches, 19 validation batches
2025-03-23 13:52:14 - Batch size 29 caused OOM: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. 3.80 GiB allowed; Of the allocated memory 3.59 GiB is allocated by PyTorch, and 197.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-23 13:52:14 - Testing candidate batch size: 15
2025-03-23 13:52:14 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:52:14 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:52:14 - Vocabulary size: 89 characters
2025-03-23 13:52:14 - Created data loaders: 1284 training batches, 36 validation batches
2025-03-23 13:52:40 - Testing candidate batch size: 14
2025-03-23 13:52:40 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:52:40 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:52:40 - Vocabulary size: 89 characters
2025-03-23 13:52:40 - Created data loaders: 1376 training batches, 39 validation batches
2025-03-23 13:53:05 - Testing candidate batch size: 13
2025-03-23 13:53:05 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:53:05 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:53:05 - Vocabulary size: 89 characters
2025-03-23 13:53:05 - Created data loaders: 1482 training batches, 42 validation batches
2025-03-23 13:53:27 - Testing candidate batch size: 8
2025-03-23 13:53:27 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:53:27 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:53:27 - Vocabulary size: 89 characters
2025-03-23 13:53:27 - Created data loaders: 2408 training batches, 67 validation batches
2025-03-23 13:53:41 - Testing candidate batch size: 16
2025-03-23 13:53:41 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:53:41 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:53:41 - Vocabulary size: 89 characters
2025-03-23 13:53:41 - Created data loaders: 1204 training batches, 34 validation batches
2025-03-23 13:54:09 - Optimal batch size: 16
2025-03-23 13:54:09 - Maximum throughput: 756.29 tokens/sec
2025-03-23 13:54:09 - Found optimal batch size: 16
2025-03-23 13:54:09 - Error in stage optimize: name 'torch' is not defined
Traceback (most recent call last):
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\pipeline.py", line 252, in run
    stage_method()
    ~~~~~~~~~~~~^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\pipeline.py", line 359, in run_optimize
    gpu_name = torch.cuda.get_device_name(0)
               ^^^^^
NameError: name 'torch' is not defined
2025-03-23 13:54:45 - Logging to file: C:\Users\nimbu\Desktop\Code\AI\ChatGoT\pipeline\pipeline.log
2025-03-23 13:54:45 - === LOGGING STARTED AT 2025-03-23 13:54:45 ===
2025-03-23 13:54:45 - Logger initialized with console and file output
2025-03-23 13:54:46 - Random seed set to 42 for reproducibility
2025-03-23 13:54:46 - Using GPU: NVIDIA GeForce GTX 1650 Ti
2025-03-23 13:54:46 - Number of GPUs: 1
2025-03-23 13:54:46 - CUDA Version: 11.8
2025-03-23 13:54:46 - GPU Memory: 4.00 GB (Free: 3.23 GB)
2025-03-23 13:54:46 - Starting pipeline from stage: process
2025-03-23 13:54:46 - ================================================================================
2025-03-23 13:54:46 -                              RUNNING STAGE: PROCESS                             
2025-03-23 13:54:46 - ================================================================================
2025-03-23 13:54:46 - Skipping data processing stage as requested.
2025-03-23 13:54:46 - Completed stage: process
2025-03-23 13:54:46 - ================================================================================
2025-03-23 13:54:46 -                             RUNNING STAGE: OPTIMIZE                             
2025-03-23 13:54:46 - ================================================================================
2025-03-23 13:54:46 - Finding optimal training settings...
2025-03-23 13:54:46 - [CUDA] Enabled TF32 precision for matrix operations
2025-03-23 13:54:46 - [CUDA] Optimized cuDNN settings for maximum performance
2025-03-23 13:54:46 - [CUDA] Using optimized scaled_dot_product_attention implementation
2025-03-23 13:54:47 - [CUDA] Optimized CUDA memory allocation settings
2025-03-23 13:54:47 - [CUDA] Optimizations applied for: NVIDIA GeForce GTX 1650 Ti (CUDA 11.8)
2025-03-23 13:54:47 - [CUDA] Applied GTX 1650 Ti specific optimizations
2025-03-23 13:54:47 - CUDA optimization results: {'cuda_available': True, 'tf32_enabled': True, 'cudnn_optimized': True, 'memory_optimized': True, 'gpu_specific_optimizations': True}
2025-03-23 13:54:47 - Using GPU: NVIDIA GeForce GTX 1650 Ti
2025-03-23 13:54:47 - Number of GPUs: 1
2025-03-23 13:54:47 - CUDA Version: 11.8
2025-03-23 13:54:47 - GPU Memory: 4.00 GB (Free: 3.23 GB)
2025-03-23 13:54:47 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:54:47 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:54:47 - Vocabulary size: 89 characters
2025-03-23 13:54:47 - Created data loaders: 19268 training batches, 536 validation batches
2025-03-23 13:54:47 - Model initialized with 768 dimensions, 85,192,793 parameters
2025-03-23 13:54:47 - Trainable parameters: 85,192,793
2025-03-23 13:54:47 - Estimated model size: 327.98MB
2025-03-23 13:54:47 - Created transformer model with 85,192,793 parameters
2025-03-23 13:54:47 - Model config: d_model=768, n_head=12, d_hid=3072, n_layers=12
2025-03-23 13:54:47 - Using standard GPT-2 Small architecture
2025-03-23 13:54:47 - Using memory-efficient attention implementation
2025-03-23 13:54:48 - Starting batch size optimization search: 1 to 256
2025-03-23 13:54:48 - Testing with sequence length: 1024
2025-03-23 13:54:48 - Testing batch size: 128
2025-03-23 13:54:50 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:54:50 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:54:50 - Vocabulary size: 89 characters
2025-03-23 13:54:50 - Created data loaders: 150 training batches, 17 validation batches
2025-03-23 13:54:54 - Batch size 128 caused OOM: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. 3.80 GiB allowed; Of the allocated memory 3.73 GiB is allocated by PyTorch, and 60.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-23 13:54:54 - Testing batch size: 64
2025-03-23 13:54:54 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:54:54 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:54:54 - Vocabulary size: 89 characters
2025-03-23 13:54:54 - Created data loaders: 301 training batches, 17 validation batches
2025-03-23 13:54:57 - Batch size 64 caused OOM: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. 3.80 GiB allowed; Of the allocated memory 3.72 GiB is allocated by PyTorch, and 82.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-23 13:54:57 - Testing batch size: 32
2025-03-23 13:54:57 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:54:57 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:54:57 - Vocabulary size: 89 characters
2025-03-23 13:54:57 - Created data loaders: 602 training batches, 17 validation batches
2025-03-23 13:55:12 - Batch size 32 caused OOM: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. 3.80 GiB allowed; Of the allocated memory 3.69 GiB is allocated by PyTorch, and 109.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-23 13:55:12 - Testing batch size: 16
2025-03-23 13:55:12 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:55:12 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:55:12 - Vocabulary size: 89 characters
2025-03-23 13:55:12 - Created data loaders: 1204 training batches, 34 validation batches
2025-03-23 13:55:39 - Batch size 16 successful: 750.34 tokens/sec
2025-03-23 13:55:39 - Testing batch size: 24
2025-03-23 13:55:39 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:55:39 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:55:39 - Vocabulary size: 89 characters
2025-03-23 13:55:39 - Created data loaders: 802 training batches, 23 validation batches
2025-03-23 13:56:20 - Batch size 24 successful: 750.27 tokens/sec
2025-03-23 13:56:20 - Testing batch size: 28
2025-03-23 13:56:20 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:56:21 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:56:21 - Vocabulary size: 89 characters
2025-03-23 13:56:21 - Created data loaders: 688 training batches, 20 validation batches
2025-03-23 13:57:09 - Batch size 28 successful: 743.25 tokens/sec
2025-03-23 13:57:09 - Testing batch size: 30
2025-03-23 13:57:09 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:57:09 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:57:09 - Vocabulary size: 89 characters
2025-03-23 13:57:09 - Created data loaders: 642 training batches, 18 validation batches
2025-03-23 13:57:19 - Batch size 30 caused OOM: CUDA out of memory. Tried to allocate 46.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. 3.80 GiB allowed; Of the allocated memory 3.66 GiB is allocated by PyTorch, and 123.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-23 13:57:19 - Testing batch size: 29
2025-03-23 13:57:19 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:57:19 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:57:19 - Vocabulary size: 89 characters
2025-03-23 13:57:19 - Created data loaders: 664 training batches, 19 validation batches
2025-03-23 13:57:26 - Batch size 29 caused OOM: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. 3.80 GiB allowed; Of the allocated memory 3.59 GiB is allocated by PyTorch, and 197.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-23 13:57:26 - Testing candidate batch size: 15
2025-03-23 13:57:26 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:57:26 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:57:26 - Vocabulary size: 89 characters
2025-03-23 13:57:26 - Created data loaders: 1284 training batches, 36 validation batches
2025-03-23 13:57:52 - Testing candidate batch size: 14
2025-03-23 13:57:52 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:57:52 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:57:52 - Vocabulary size: 89 characters
2025-03-23 13:57:52 - Created data loaders: 1376 training batches, 39 validation batches
2025-03-23 13:58:17 - Testing candidate batch size: 13
2025-03-23 13:58:17 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:58:17 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:58:17 - Vocabulary size: 89 characters
2025-03-23 13:58:17 - Created data loaders: 1482 training batches, 42 validation batches
2025-03-23 13:58:39 - Testing candidate batch size: 8
2025-03-23 13:58:39 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:58:39 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:58:39 - Vocabulary size: 89 characters
2025-03-23 13:58:39 - Created data loaders: 2408 training batches, 67 validation batches
2025-03-23 13:58:53 - Testing candidate batch size: 16
2025-03-23 13:58:53 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:58:53 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:58:53 - Vocabulary size: 89 characters
2025-03-23 13:58:53 - Created data loaders: 1204 training batches, 34 validation batches
2025-03-23 13:59:21 - Optimal batch size: 16
2025-03-23 13:59:21 - Maximum throughput: 750.34 tokens/sec
2025-03-23 13:59:21 - Found optimal batch size: 16
2025-03-23 13:59:21 - Detected 4096MB total VRAM - optimizing for maximum utilization
2025-03-23 13:59:21 - Applied ULTRA-AGGRESSIVE GTX 1650 Ti settings for MAXIMUM throughput and VRAM utilization
2025-03-23 13:59:21 - Optimization completed with settings: {'batch_size': 16, 'gradient_accumulation_steps': 1, 'use_amp': False, 'cuda_optimized': True}
2025-03-23 13:59:21 - Completed stage: optimize
2025-03-23 13:59:21 - ================================================================================
2025-03-23 13:59:21 -                               RUNNING STAGE: TRAIN                              
2025-03-23 13:59:21 - ================================================================================
2025-03-23 13:59:21 - Running training with command: python -m src.train_optimized --data_path processed_data/got_char_data.pkl --sequence_length 1024 --epochs 5 --batch_size 16 --gradient_accumulation_steps 1 --lr 0.0005 --d_model 768 --n_head 12 --d_hid 3072 --n_layers 12 --save_every 1 --checkpoint_dir pipeline\train\checkpoints --output_dir pipeline\train\output --seed 42
2025-03-23 14:27:19 - Logging to file: C:\Users\nimbu\Desktop\Code\AI\ChatGoT\pipeline\pipeline.log
2025-03-23 14:27:19 - === LOGGING STARTED AT 2025-03-23 14:27:19 ===
2025-03-23 14:27:19 - Logger initialized with console and file output
2025-03-23 14:27:19 - Random seed set to 42 for reproducibility
2025-03-23 14:27:19 - Using GPU: NVIDIA GeForce GTX 1650 Ti
2025-03-23 14:27:19 - Number of GPUs: 1
2025-03-23 14:27:19 - CUDA Version: 11.8
2025-03-23 14:27:20 - GPU Memory: 4.00 GB (Free: 3.23 GB)
2025-03-23 14:27:20 - Starting pipeline from stage: process
2025-03-23 14:27:20 - ================================================================================
2025-03-23 14:27:20 -                              RUNNING STAGE: PROCESS                             
2025-03-23 14:27:20 - ================================================================================
2025-03-23 14:27:20 - Skipping data processing stage as requested.
2025-03-23 14:27:20 - Completed stage: process
2025-03-23 14:27:20 - ================================================================================
2025-03-23 14:27:20 -                             RUNNING STAGE: OPTIMIZE                             
2025-03-23 14:27:20 - ================================================================================
2025-03-23 14:27:20 - Finding optimal training settings...
2025-03-23 14:27:20 - [CUDA] Enabled TF32 precision for matrix operations
2025-03-23 14:27:20 - [CUDA] Optimized cuDNN settings for maximum performance
2025-03-23 14:27:20 - [CUDA] Configured memory allocation for small GPU (<5GB)
2025-03-23 14:27:20 - [CUDA] Using optimized scaled_dot_product_attention implementation
2025-03-23 14:27:20 - [CUDA] Disabled debug APIs for faster execution
2025-03-23 14:27:20 - [CUDA] Detected GPU: NVIDIA GeForce GTX 1650 Ti (Compute Capability 7.5)
2025-03-23 14:27:20 - [CUDA] Applied optimizations for Ampere/RTX architecture
2025-03-23 14:27:20 - [CUDA] Optimizations applied for: NVIDIA GeForce GTX 1650 Ti (CUDA 11.8, 4.00 GB)
2025-03-23 14:27:20 - CUDA optimization results: {'cuda_available': True, 'tf32_enabled': True, 'cudnn_optimized': True, 'memory_optimized': True, 'debug_apis_disabled': True, 'gpu_specific_optimizations': True, 'throughput_optimized': False}
2025-03-23 14:27:20 - Using GPU: NVIDIA GeForce GTX 1650 Ti
2025-03-23 14:27:20 - Number of GPUs: 1
2025-03-23 14:27:20 - CUDA Version: 11.8
2025-03-23 14:27:20 - GPU Memory: 4.00 GB (Free: 3.23 GB)
2025-03-23 14:27:20 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:27:20 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:27:20 - Vocabulary size: 89 characters
2025-03-23 14:27:20 - Created data loaders: 19268 training batches, 536 validation batches
2025-03-23 14:27:20 - GPU: NVIDIA GeForce GTX 1650 Ti with 4.00 GB memory
2025-03-23 14:27:20 - Model initialized with 768 dimensions, 85,192,793 parameters
2025-03-23 14:27:20 - Trainable parameters: 85,192,793
2025-03-23 14:27:20 - Estimated model size: 327.98MB
2025-03-23 14:27:20 - Created transformer model with 85,192,793 parameters
2025-03-23 14:27:20 - Model config: d_model=768, n_head=12, d_hid=3072, n_layers=12
2025-03-23 14:27:20 - Using standard GPT-2 Small architecture
2025-03-23 14:27:20 - Using memory-efficient attention implementation
2025-03-23 14:27:21 - Low GPU memory detected, testing conservative batch sizes
2025-03-23 14:27:21 - Testing batch sizes: [4, 8, 12, 16, 20, 24, 28, 32]
2025-03-23 14:27:21 - Testing batch size: 4
2025-03-23 14:27:22 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:27:22 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:27:22 - Vocabulary size: 89 characters
2025-03-23 14:27:22 - Created data loaders: 4817 training batches, 134 validation batches
2025-03-23 14:27:31 - Batch size 4 successful: 743.89 tokens/sec
2025-03-23 14:27:31 - Testing batch size: 8
2025-03-23 14:27:31 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:27:31 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:27:31 - Vocabulary size: 89 characters
2025-03-23 14:27:31 - Created data loaders: 2408 training batches, 67 validation batches
2025-03-23 14:27:47 - Batch size 8 successful: 748.17 tokens/sec
2025-03-23 14:27:47 - Testing batch size: 12
2025-03-23 14:27:47 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:27:47 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:27:47 - Vocabulary size: 89 characters
2025-03-23 14:27:47 - Created data loaders: 1605 training batches, 45 validation batches
2025-03-23 14:28:12 - Batch size 12 successful: 749.88 tokens/sec
2025-03-23 14:28:12 - Testing batch size: 16
2025-03-23 14:28:12 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:28:12 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:28:12 - Vocabulary size: 89 characters
2025-03-23 14:28:12 - Created data loaders: 1204 training batches, 34 validation batches
2025-03-23 14:28:45 - Batch size 16 successful: 750.64 tokens/sec
2025-03-23 14:28:45 - Testing batch size: 20
2025-03-23 14:28:45 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:28:45 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:28:45 - Vocabulary size: 89 characters
2025-03-23 14:28:45 - Created data loaders: 963 training batches, 27 validation batches
2025-03-23 14:29:26 - Batch size 20 successful: 750.17 tokens/sec
2025-03-23 14:29:26 - Testing batch size: 24
2025-03-23 14:29:26 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:29:26 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:29:26 - Vocabulary size: 89 characters
2025-03-23 14:29:26 - Created data loaders: 802 training batches, 23 validation batches
2025-03-23 14:30:15 - Batch size 24 successful: 749.46 tokens/sec
2025-03-23 14:30:15 - Testing batch size: 28
2025-03-23 14:30:15 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:30:15 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:30:15 - Vocabulary size: 89 characters
2025-03-23 14:30:15 - Created data loaders: 688 training batches, 20 validation batches
2025-03-23 14:31:13 - Batch size 28 successful: 743.92 tokens/sec
2025-03-23 14:31:13 - Testing batch size: 32
2025-03-23 14:31:13 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:31:13 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:31:13 - Vocabulary size: 89 characters
2025-03-23 14:31:14 - Created data loaders: 602 training batches, 17 validation batches
2025-03-23 14:32:21 - Batch size 32 successful: 736.27 tokens/sec
2025-03-23 14:32:21 - Testing additional batch sizes around best: [10, 14, 18, 22]
2025-03-23 14:32:21 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:32:21 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:32:21 - Vocabulary size: 89 characters
2025-03-23 14:32:21 - Created data loaders: 1926 training batches, 54 validation batches
2025-03-23 14:32:42 - Batch size 10 successful: 731.54 tokens/sec
2025-03-23 14:32:42 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:32:42 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:32:42 - Vocabulary size: 89 characters
2025-03-23 14:32:42 - Created data loaders: 1376 training batches, 39 validation batches
2025-03-23 14:33:11 - Batch size 14 successful: 732.18 tokens/sec
2025-03-23 14:33:11 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:33:11 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:33:11 - Vocabulary size: 89 characters
2025-03-23 14:33:11 - Created data loaders: 1070 training batches, 30 validation batches
2025-03-23 14:33:49 - Batch size 18 successful: 735.49 tokens/sec
2025-03-23 14:33:49 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:33:49 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:33:49 - Vocabulary size: 89 characters
2025-03-23 14:33:49 - Created data loaders: 875 training batches, 25 validation batches
2025-03-23 14:34:35 - Batch size 22 successful: 735.47 tokens/sec
2025-03-23 14:34:35 - 
Batch size throughput results:
2025-03-23 14:34:35 -   Batch size 16: 750.64 tokens/sec
2025-03-23 14:34:35 -   Batch size 20: 750.17 tokens/sec
2025-03-23 14:34:35 -   Batch size 12: 749.88 tokens/sec
2025-03-23 14:34:35 -   Batch size 24: 749.46 tokens/sec
2025-03-23 14:34:35 -   Batch size 8: 748.17 tokens/sec
2025-03-23 14:34:35 -   Batch size 28: 743.92 tokens/sec
2025-03-23 14:34:35 -   Batch size 4: 743.89 tokens/sec
2025-03-23 14:34:35 -   Batch size 32: 736.27 tokens/sec
2025-03-23 14:34:35 -   Batch size 18: 735.49 tokens/sec
2025-03-23 14:34:35 -   Batch size 22: 735.47 tokens/sec
2025-03-23 14:34:35 -   Batch size 14: 732.18 tokens/sec
2025-03-23 14:34:35 -   Batch size 10: 731.54 tokens/sec
2025-03-23 14:34:35 - 
Optimal batch size: 16
2025-03-23 14:34:35 - Maximum throughput: 750.64 tokens/sec
2025-03-23 14:34:35 - 
Additional performance tips:
2025-03-23 14:34:35 -   - Try using --use_amp for faster training with minimal precision loss
2025-03-23 14:34:35 -   - Consider reducing model size with --d_model and --n_layers
2025-03-23 14:34:35 - Found optimal batch size: 16
2025-03-23 14:34:35 - Detected 4096MB total VRAM - optimizing for maximum utilization
2025-03-23 14:34:35 - Applied ULTRA-AGGRESSIVE GTX 1650 Ti settings for MAXIMUM throughput and VRAM utilization
2025-03-23 14:34:35 - Optimization completed with settings: {'batch_size': 16, 'gradient_accumulation_steps': 1, 'use_amp': False, 'cuda_optimized': True}
2025-03-23 14:34:35 - Completed stage: optimize
2025-03-23 14:34:35 - ================================================================================
2025-03-23 14:34:35 -                               RUNNING STAGE: TRAIN                              
2025-03-23 14:34:35 - ================================================================================
2025-03-23 14:34:35 - Running training with command: python -m src.train_optimized --data_path processed_data/got_char_data.pkl --sequence_length 1024 --epochs 5 --batch_size 16 --gradient_accumulation_steps 1 --lr 0.0005 --d_model 768 --n_head 12 --d_hid 3072 --n_layers 12 --save_every 1 --checkpoint_dir pipeline\train\checkpoints --output_dir pipeline\train\output --seed 42
2025-03-23 14:51:46 - Logging to file: C:\Users\nimbu\Desktop\Code\AI\ChatGoT\pipeline\pipeline.log
2025-03-23 14:51:46 - === LOGGING STARTED AT 2025-03-23 14:51:46 ===
2025-03-23 14:51:46 - Logger initialized with console and file output
2025-03-23 14:51:47 - Random seed set to 42 for reproducibility
2025-03-23 14:51:47 - Using CUDA with 1 GPU(s)
2025-03-23 14:51:47 - PyTorch version: 2.6.0+cu118
2025-03-23 14:51:47 - GPU Memory Info:
GPU 0: NVIDIA GeForce GTX 1650 Ti, 4.00 GB total, 3.23 GB free
2025-03-23 14:51:47 - Enabled TF32 precision for faster training
2025-03-23 14:51:47 - Enabled cuDNN benchmark mode for faster training
2025-03-23 14:52:26 - Logging to file: C:\Users\nimbu\Desktop\Code\AI\ChatGoT\pipeline\pipeline.log
2025-03-23 14:52:26 - === LOGGING STARTED AT 2025-03-23 14:52:26 ===
2025-03-23 14:52:26 - Logger initialized with console and file output
2025-03-23 14:52:28 - Random seed set to 42 for reproducibility
2025-03-23 14:52:28 - Using CUDA with 1 GPU(s)
2025-03-23 14:52:28 - PyTorch version: 2.6.0+cu118
2025-03-23 14:52:28 - GPU Memory Info:
GPU 0: NVIDIA GeForce GTX 1650 Ti, 4.00 GB total, 3.23 GB free
2025-03-23 14:52:28 - Enabled TF32 precision for faster training
2025-03-23 14:52:28 - Enabled cuDNN benchmark mode for faster training
2025-03-23 14:52:28 - Starting pipeline from stage: process
2025-03-23 14:52:28 - ================================================================================
2025-03-23 14:52:28 -                              RUNNING STAGE: PROCESS                             
2025-03-23 14:52:28 - ================================================================================
2025-03-23 14:52:28 - Skipping data processing stage as requested.
2025-03-23 14:52:28 - Completed stage: process
2025-03-23 14:52:28 - ================================================================================
2025-03-23 14:52:28 -                             RUNNING STAGE: OPTIMIZE                             
2025-03-23 14:52:28 - ================================================================================
2025-03-23 14:52:28 - Finding optimal training settings...
2025-03-23 14:52:28 - [CUDA] Enabled TF32 precision for matrix operations
2025-03-23 14:52:28 - [CUDA] Optimized cuDNN settings for maximum performance
2025-03-23 14:52:28 - [CUDA] Configured memory allocation for small GPU (<5GB)
2025-03-23 14:52:28 - [CUDA] Using optimized scaled_dot_product_attention implementation
2025-03-23 14:52:28 - [CUDA] Disabled debug APIs for faster execution
2025-03-23 14:52:28 - [CUDA] Detected GPU: NVIDIA GeForce GTX 1650 Ti (Compute Capability 7.5)
2025-03-23 14:52:28 - [CUDA] Applied optimizations for Ampere/RTX architecture
2025-03-23 14:52:28 - [CUDA] Optimizations applied for: NVIDIA GeForce GTX 1650 Ti (CUDA 11.8, 4.00 GB)
2025-03-23 14:52:28 - CUDA optimization results: {'cuda_available': True, 'tf32_enabled': True, 'cudnn_optimized': True, 'memory_optimized': True, 'debug_apis_disabled': True, 'gpu_specific_optimizations': True, 'throughput_optimized': False}
2025-03-23 14:52:28 - Using CUDA with 1 GPU(s)
2025-03-23 14:52:28 - PyTorch version: 2.6.0+cu118
2025-03-23 14:52:28 - GPU Memory Info:
GPU 0: NVIDIA GeForce GTX 1650 Ti, 4.00 GB total, 3.23 GB free
2025-03-23 14:52:28 - Enabled TF32 precision for faster training
2025-03-23 14:52:28 - Enabled cuDNN benchmark mode for faster training
2025-03-23 14:52:28 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:52:28 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:52:28 - Vocabulary size: 89 characters
2025-03-23 14:52:28 - Created data loaders: 19268 training batches, 536 validation batches
2025-03-23 14:52:28 - GPU: NVIDIA GeForce GTX 1650 Ti with 4.00 GB memory
2025-03-23 14:52:28 - Model initialized with 768 dimensions, 85,192,793 parameters
2025-03-23 14:52:28 - Trainable parameters: 85,192,793
2025-03-23 14:52:28 - Estimated model size: 327.98MB
2025-03-23 14:52:29 - Created transformer model with 85,192,793 parameters
2025-03-23 14:52:29 - Model config: d_model=768, n_head=12, d_hid=3072, n_layers=12
2025-03-23 14:52:29 - Using standard GPT-2 Small architecture
2025-03-23 14:52:29 - Using memory-efficient attention implementation
2025-03-23 14:52:29 - Low GPU memory detected, testing conservative batch sizes
2025-03-23 14:52:29 - Testing batch sizes: [4, 8, 12, 16, 20, 24, 28, 32]
2025-03-23 14:52:29 - Testing batch size: 4
2025-03-23 14:52:30 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:52:31 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:52:31 - Vocabulary size: 89 characters
2025-03-23 14:52:31 - Created data loaders: 4817 training batches, 134 validation batches
2025-03-23 14:52:39 - Batch size 4 successful: 751.57 tokens/sec
2025-03-23 14:52:39 - Testing batch size: 8
2025-03-23 14:52:39 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:52:39 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:52:39 - Vocabulary size: 89 characters
2025-03-23 14:52:39 - Created data loaders: 2408 training batches, 67 validation batches
2025-03-23 14:52:55 - Batch size 8 successful: 754.47 tokens/sec
2025-03-23 14:52:55 - Testing batch size: 12
2025-03-23 14:52:55 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:52:56 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:52:56 - Vocabulary size: 89 characters
2025-03-23 14:52:56 - Created data loaders: 1605 training batches, 45 validation batches
2025-03-23 14:53:20 - Batch size 12 successful: 753.96 tokens/sec
2025-03-23 14:53:20 - Testing batch size: 16
2025-03-23 14:53:20 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:53:20 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:53:20 - Vocabulary size: 89 characters
2025-03-23 14:53:20 - Created data loaders: 1204 training batches, 34 validation batches
2025-03-23 14:53:53 - Batch size 16 successful: 750.62 tokens/sec
2025-03-23 14:53:53 - Testing batch size: 20
2025-03-23 14:53:53 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:53:53 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:53:53 - Vocabulary size: 89 characters
2025-03-23 14:53:53 - Created data loaders: 963 training batches, 27 validation batches
2025-03-23 14:54:34 - Batch size 20 successful: 750.16 tokens/sec
2025-03-23 14:54:34 - Testing batch size: 24
2025-03-23 14:54:34 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:54:34 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:54:34 - Vocabulary size: 89 characters
2025-03-23 14:54:34 - Created data loaders: 802 training batches, 23 validation batches
2025-03-23 14:55:23 - Batch size 24 successful: 750.42 tokens/sec
2025-03-23 14:55:23 - Testing batch size: 28
2025-03-23 14:55:24 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:55:24 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:55:24 - Vocabulary size: 89 characters
2025-03-23 14:55:24 - Created data loaders: 688 training batches, 20 validation batches
2025-03-23 14:56:21 - Batch size 28 successful: 749.81 tokens/sec
2025-03-23 14:56:21 - Testing batch size: 32
2025-03-23 14:56:21 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:56:21 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:56:21 - Vocabulary size: 89 characters
2025-03-23 14:56:21 - Created data loaders: 602 training batches, 17 validation batches
2025-03-23 14:57:28 - Batch size 32 successful: 741.77 tokens/sec
2025-03-23 14:57:28 - Testing additional batch sizes around best: [2, 6, 10, 14]
2025-03-23 14:57:28 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:57:28 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:57:28 - Vocabulary size: 89 characters
2025-03-23 14:57:28 - Created data loaders: 9634 training batches, 268 validation batches
2025-03-23 14:57:32 - Batch size 2 successful: 666.20 tokens/sec
2025-03-23 14:57:32 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:57:32 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:57:32 - Vocabulary size: 89 characters
2025-03-23 14:57:32 - Created data loaders: 3211 training batches, 90 validation batches
2025-03-23 14:57:45 - Batch size 6 successful: 727.24 tokens/sec
2025-03-23 14:57:45 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:57:45 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:57:45 - Vocabulary size: 89 characters
2025-03-23 14:57:45 - Created data loaders: 1926 training batches, 54 validation batches
2025-03-23 14:58:06 - Batch size 10 successful: 737.28 tokens/sec
2025-03-23 14:58:06 - Loading data from processed_data/got_char_data.pkl
2025-03-23 14:58:06 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 14:58:06 - Vocabulary size: 89 characters
2025-03-23 14:58:06 - Created data loaders: 1376 training batches, 39 validation batches
2025-03-23 14:58:35 - Batch size 14 successful: 737.89 tokens/sec
2025-03-23 14:58:35 - 
Batch size throughput results:
2025-03-23 14:58:35 -   Batch size 8: 754.47 tokens/sec
2025-03-23 14:58:35 -   Batch size 12: 753.96 tokens/sec
2025-03-23 14:58:35 -   Batch size 4: 751.57 tokens/sec
2025-03-23 14:58:35 -   Batch size 16: 750.62 tokens/sec
2025-03-23 14:58:35 -   Batch size 24: 750.42 tokens/sec
2025-03-23 14:58:35 -   Batch size 20: 750.16 tokens/sec
2025-03-23 14:58:35 -   Batch size 28: 749.81 tokens/sec
2025-03-23 14:58:35 -   Batch size 32: 741.77 tokens/sec
2025-03-23 14:58:35 -   Batch size 14: 737.89 tokens/sec
2025-03-23 14:58:35 -   Batch size 10: 737.28 tokens/sec
2025-03-23 14:58:35 -   Batch size 6: 727.24 tokens/sec
2025-03-23 14:58:35 -   Batch size 2: 666.20 tokens/sec
2025-03-23 14:58:35 - 
Optimal batch size: 8
2025-03-23 14:58:35 - Maximum throughput: 754.47 tokens/sec
2025-03-23 14:58:35 - 
Additional performance tips:
2025-03-23 14:58:35 -   - Try using --use_amp for faster training with minimal precision loss
2025-03-23 14:58:35 -   - Consider reducing model size with --d_model and --n_layers
2025-03-23 14:58:35 - Found optimal batch size: 8
2025-03-23 14:58:35 - Detected 4096MB total VRAM - optimizing for maximum utilization
2025-03-23 14:58:35 - Applied ULTRA-AGGRESSIVE GTX 1650 Ti settings for MAXIMUM throughput and VRAM utilization
2025-03-23 14:58:35 - Optimization completed with settings: {'batch_size': 8, 'gradient_accumulation_steps': 1, 'use_amp': False, 'cuda_optimized': True}
2025-03-23 14:58:36 - Completed stage: optimize
2025-03-23 14:58:36 - ================================================================================
2025-03-23 14:58:36 -                               RUNNING STAGE: TRAIN                              
2025-03-23 14:58:36 - ================================================================================
2025-03-23 14:58:36 - Running training with command: python -m src.train_optimized --data_path processed_data/got_char_data.pkl --sequence_length 1024 --epochs 5 --batch_size 8 --gradient_accumulation_steps 1 --lr 0.0005 --d_model 768 --n_head 12 --d_hid 3072 --n_layers 12 --save_every 1 --checkpoint_dir pipeline\train\checkpoints --output_dir pipeline\train\output --seed 42
2025-03-23 15:04:46 - Logging to file: C:\Users\nimbu\Desktop\Code\AI\ChatGoT\pipeline\pipeline.log
2025-03-23 15:04:46 - === LOGGING STARTED AT 2025-03-23 15:04:46 ===
2025-03-23 15:04:46 - Logger initialized with console and file output
2025-03-23 15:04:47 - Random seed set to 42 for reproducibility
2025-03-23 15:04:47 - Using CUDA with 1 GPU(s)
2025-03-23 15:04:47 - PyTorch version: 2.6.0+cu118
2025-03-23 15:04:47 - GPU Memory Info:
GPU 0: NVIDIA GeForce GTX 1650 Ti, 4.00 GB total, 3.23 GB free
2025-03-23 15:04:47 - Enabled TF32 precision for faster training
2025-03-23 15:04:47 - Enabled cuDNN benchmark mode for faster training
2025-03-23 15:04:47 - Starting pipeline from stage: process
2025-03-23 15:04:47 - ================================================================================
2025-03-23 15:04:47 -                              RUNNING STAGE: PROCESS                             
2025-03-23 15:04:47 - ================================================================================
2025-03-23 15:04:47 - Skipping data processing stage as requested.
2025-03-23 15:04:47 - Completed stage: process
2025-03-23 15:04:47 - ================================================================================
2025-03-23 15:04:47 -                             RUNNING STAGE: OPTIMIZE                             
2025-03-23 15:04:47 - ================================================================================
2025-03-23 15:04:47 - Finding optimal training settings...
2025-03-23 15:04:47 - [CUDA] Enabled TF32 precision for matrix operations
2025-03-23 15:04:47 - [CUDA] Optimized cuDNN settings for maximum performance
2025-03-23 15:04:47 - [CUDA] Configured memory allocation for small GPU (<5GB)
2025-03-23 15:04:48 - [CUDA] Using optimized scaled_dot_product_attention implementation
2025-03-23 15:04:48 - [CUDA] Disabled debug APIs for faster execution
2025-03-23 15:04:48 - [CUDA] Detected GPU: NVIDIA GeForce GTX 1650 Ti (Compute Capability 7.5)
2025-03-23 15:04:48 - [CUDA] Applied optimizations for Ampere/RTX architecture
2025-03-23 15:04:48 - [CUDA] Optimizations applied for: NVIDIA GeForce GTX 1650 Ti (CUDA 11.8, 4.00 GB)
2025-03-23 15:04:48 - CUDA optimization results: {'cuda_available': True, 'tf32_enabled': True, 'cudnn_optimized': True, 'memory_optimized': True, 'debug_apis_disabled': True, 'gpu_specific_optimizations': True, 'throughput_optimized': False}
2025-03-23 15:04:48 - Using CUDA with 1 GPU(s)
2025-03-23 15:04:48 - PyTorch version: 2.6.0+cu118
2025-03-23 15:04:48 - GPU Memory Info:
GPU 0: NVIDIA GeForce GTX 1650 Ti, 4.00 GB total, 3.23 GB free
2025-03-23 15:04:48 - Enabled TF32 precision for faster training
2025-03-23 15:04:48 - Enabled cuDNN benchmark mode for faster training
2025-03-23 15:04:48 - Loading data from processed_data/got_char_data.pkl
2025-03-23 15:04:48 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 15:04:48 - Vocabulary size: 89 characters
2025-03-23 15:04:48 - Created data loaders: 19268 training batches, 536 validation batches
2025-03-23 15:04:48 - GPU: NVIDIA GeForce GTX 1650 Ti with 4.00 GB memory
2025-03-23 15:04:48 - Model initialized with 768 dimensions, 85,192,793 parameters
2025-03-23 15:04:48 - Trainable parameters: 85,192,793
2025-03-23 15:04:48 - Estimated model size: 327.98MB
2025-03-23 15:04:48 - Created transformer model with 85,192,793 parameters
2025-03-23 15:04:48 - Model config: d_model=768, n_head=12, d_hid=3072, n_layers=12
2025-03-23 15:04:48 - Using standard GPT-2 Small architecture
2025-03-23 15:04:48 - Using memory-efficient attention implementation
2025-03-23 15:04:49 - Low GPU memory detected, testing conservative batch sizes
2025-03-23 15:04:49 - Testing batch sizes: [4, 8, 12, 16, 20, 24, 28, 32]
2025-03-23 15:04:49 - Testing batch size: 4
2025-03-23 15:04:51 - Loading data from processed_data/got_char_data.pkl
2025-03-23 15:04:51 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 15:04:51 - Vocabulary size: 89 characters
2025-03-23 15:04:51 - Created data loaders: 4817 training batches, 134 validation batches
2025-03-23 15:04:53 - Error during benchmark: element 0 of tensors does not require grad and does not have a grad_fn
2025-03-23 15:04:53 - Batch size 4 failed (OOM)
2025-03-23 15:04:53 - No successful batch sizes found, using minimum batch size
2025-03-23 15:04:53 - 
Optimal batch size: 1
2025-03-23 15:04:53 - Maximum throughput: 0.00 tokens/sec
2025-03-23 15:04:53 - 
Additional performance tips:
2025-03-23 15:04:53 -   - Try using --use_amp for faster training with minimal precision loss
2025-03-23 15:04:53 -   - Consider reducing model size with --d_model and --n_layers
2025-03-23 15:04:53 - Found optimal batch size: 1
2025-03-23 15:04:53 - Detected 4096MB total VRAM - optimizing for maximum utilization
2025-03-23 15:04:53 - Applied ULTRA-AGGRESSIVE GTX 1650 Ti settings for MAXIMUM throughput and VRAM utilization
2025-03-23 15:04:53 - Optimization completed with settings: {'batch_size': 1, 'gradient_accumulation_steps': 1, 'use_amp': False, 'cuda_optimized': True}
2025-03-23 15:04:53 - Completed stage: optimize
2025-03-23 15:04:53 - ================================================================================
2025-03-23 15:04:53 -                               RUNNING STAGE: TRAIN                              
2025-03-23 15:04:53 - ================================================================================
2025-03-23 15:04:53 - Running training with command: python -m src.train_optimized --data_path processed_data/got_char_data.pkl --sequence_length 1024 --epochs 5 --batch_size 1 --gradient_accumulation_steps 1 --lr 0.0005 --d_model 768 --n_head 12 --d_hid 3072 --n_layers 12 --save_every 1 --checkpoint_dir pipeline\train\checkpoints --output_dir pipeline\train\output --seed 42
2025-03-23 15:42:51 - Logging to file: C:\Users\nimbu\Desktop\Code\AI\ChatGoT\pipeline\pipeline.log
2025-03-23 15:42:51 - === LOGGING STARTED AT 2025-03-23 15:42:51 ===
2025-03-23 15:42:51 - Logger initialized with console and file output
2025-03-23 15:42:52 - Random seed set to 42 for reproducibility
2025-03-23 15:42:52 - Using CUDA with 1 GPU(s)
2025-03-23 15:42:52 - PyTorch version: 2.6.0+cu118
2025-03-23 15:42:52 - GPU Memory Info:
GPU 0: NVIDIA GeForce GTX 1650 Ti, 4.00 GB total, 3.23 GB free
2025-03-23 15:42:52 - Enabled TF32 precision for faster training
2025-03-23 15:42:52 - Enabled cuDNN benchmark mode for faster training
2025-03-23 15:42:52 - Loaded pipeline state: {'last_completed_stage': 'optimize', 'start_time': 1742756687.944722, 'stages': {'process': {'completed': True, 'timestamp': 1742756687.9625528, 'output_path': 'processed_data/got_char_data.pkl'}, 'optimize': {'completed': True, 'timestamp': 1742756693.5404828, 'settings': {'batch_size': 1, 'gradient_accumulation_steps': 1, 'use_amp': False, 'cuda_optimized': True}}, 'train': {'completed': False, 'timestamp': None, 'checkpoint_path': None, 'best_validation_loss': inf}, 'generate': {'completed': False, 'timestamp': None, 'output_file': None}}}
2025-03-23 15:42:52 - Starting pipeline from stage: train
2025-03-23 15:42:52 - ================================================================================
2025-03-23 15:42:52 -                               RUNNING STAGE: TRAIN                              
2025-03-23 15:42:52 - ================================================================================
2025-03-23 15:42:52 - Running training with command: --data_path processed_data/got_char_data.pkl --batch_size 1 --epochs 5 --learning_rate 5e-05 --gradient_accumulation_steps 1 --save_every 1 --seed 42 --log_level INFO --use_torch_compile --compile_mode reduce-overhead
2025-03-23 15:42:52 - Error in stage train: Command '--data_path processed_data/got_char_data.pkl --batch_size 1 --epochs 5 --learning_rate 5e-05 --gradient_accumulation_steps 1 --save_every 1 --seed 42 --log_level INFO --use_torch_compile --compile_mode reduce-overhead' returned non-zero exit status 1.
Traceback (most recent call last):
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\pipeline.py", line 258, in run
    stage_method()
    ~~~~~~~~~~~~^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\pipeline.py", line 437, in run_train
    process = subprocess.run(train_cmd, shell=True, check=True)
  File "C:\Users\nimbu\AppData\Local\Programs\Python\Python313\Lib\subprocess.py", line 579, in run
    raise CalledProcessError(retcode, process.args,
                             output=stdout, stderr=stderr)
subprocess.CalledProcessError: Command '--data_path processed_data/got_char_data.pkl --batch_size 1 --epochs 5 --learning_rate 5e-05 --gradient_accumulation_steps 1 --save_every 1 --seed 42 --log_level INFO --use_torch_compile --compile_mode reduce-overhead' returned non-zero exit status 1.
