2025-03-23 13:33:16 - Logging to file: C:\Users\nimbu\Desktop\Code\AI\ChatGoT\optimization_results.log
2025-03-23 13:33:16 - === LOGGING STARTED AT 2025-03-23 13:33:16 ===
2025-03-23 13:33:16 - Logger initialized with console and file output
2025-03-23 13:33:16 - 
================================================================================
2025-03-23 13:33:16 - STARTING OPTIMIZATION FOR MAXIMUM TRAINING THROUGHPUT
2025-03-23 13:33:16 - ================================================================================

2025-03-23 13:33:17 - Random seed set to 42 for reproducibility
2025-03-23 13:33:17 - Applying CUDA optimizations...
2025-03-23 13:33:17 - [CUDA] Enabled TF32 precision for matrix operations
2025-03-23 13:33:17 - [CUDA] Optimized cuDNN settings for maximum performance
2025-03-23 13:33:17 - [CUDA] Using optimized scaled_dot_product_attention implementation
2025-03-23 13:33:17 - [CUDA] Optimized CUDA memory allocation settings
2025-03-23 13:33:17 - [CUDA] Optimizations applied for: NVIDIA GeForce GTX 1650 Ti (CUDA 11.8)
2025-03-23 13:33:18 - [CUDA] Applied GTX 1650 Ti specific optimizations
2025-03-23 13:33:18 - Using GPU: NVIDIA GeForce GTX 1650 Ti
2025-03-23 13:33:18 - Number of GPUs: 1
2025-03-23 13:33:18 - CUDA Version: 11.8
2025-03-23 13:33:18 - GPU Memory: 4.00 GB (Free: 3.23 GB)
2025-03-23 13:33:18 - Using GPU: NVIDIA GeForce GTX 1650 Ti
2025-03-23 13:33:18 - Total VRAM: 4.00 GB
2025-03-23 13:33:18 - 
================================================================================
2025-03-23 13:33:18 - FINDING OPTIMAL BATCH SIZE
2025-03-23 13:33:18 - ================================================================================

2025-03-23 13:33:18 - Using GPU: NVIDIA GeForce GTX 1650 Ti
2025-03-23 13:33:18 - Number of GPUs: 1
2025-03-23 13:33:18 - CUDA Version: 11.8
2025-03-23 13:33:18 - GPU Memory: 4.00 GB (Free: 3.23 GB)
2025-03-23 13:33:18 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:33:18 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:33:18 - Vocabulary size: 89 characters
2025-03-23 13:33:18 - Created data loaders: 19268 training batches, 536 validation batches
2025-03-23 13:33:18 - Model initialized with 768 dimensions, 85,192,793 parameters
2025-03-23 13:33:18 - Trainable parameters: 85,192,793
2025-03-23 13:33:18 - Estimated model size: 327.98MB
2025-03-23 13:33:18 - Created transformer model with 85,192,793 parameters
2025-03-23 13:33:18 - Model config: d_model=768, n_head=12, d_hid=3072, n_layers=12
2025-03-23 13:33:18 - Using standard GPT-2 Small architecture
2025-03-23 13:33:18 - Using memory-efficient attention implementation
2025-03-23 13:33:19 - Starting batch size optimization search: 1 to 256
2025-03-23 13:33:19 - Testing with sequence length: 1024
2025-03-23 13:33:19 - Testing batch size: 128
2025-03-23 13:33:20 - Error during optimization: load_data() got an unexpected keyword argument 'max_seq_length'
Traceback (most recent call last):
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\optimize_training.py", line 161, in <module>
    main()
    ~~~~^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\optimize_training.py", line 102, in main
    optimal_batch_size = find_optimal_batch_size(args)
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\src\batch_size_finder.py", line 208, in find_optimal_batch_size
    throughput, success = measure_throughput(
                          ~~~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<4 lines>...
        test_batches=args.test_batches
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\src\batch_size_finder.py", line 81, in measure_throughput
    train_loader, _, _, _ = load_data(
                            ~~~~~~~~~^
        data_path=data_path,
        ^^^^^^^^^^^^^^^^^^^^
    ...<3 lines>...
        device_type=device.type
        ^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
TypeError: load_data() got an unexpected keyword argument 'max_seq_length'
