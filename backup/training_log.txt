2025-03-23 01:48:16 - Logging output to training_log.txt
2025-03-23 01:48:16 - You can monitor progress with: type training_log.txt
2025-03-23 01:48:16 - ================================================================================
2025-03-23 01:48:16 - Starting training run at 2025-03-23 01:48:16
2025-03-23 01:48:16 - ================================================================================
2025-03-23 01:48:16 - 
Training configuration:
2025-03-23 01:48:16 -   data_path: processed_data/got_char_data.pkl
2025-03-23 01:48:16 -   batch_size: 11
2025-03-23 01:48:16 -   eval_batch_size: 128
2025-03-23 01:48:16 -   accumulation_steps: 1
2025-03-23 01:48:16 -   epochs: 1
2025-03-23 01:48:16 -   lr: 0.001
2025-03-23 01:48:16 -   weight_decay: 0.01
2025-03-23 01:48:16 -   min_lr: 1e-05
2025-03-23 01:48:16 -   warmup_steps: 1000
2025-03-23 01:48:16 -   seed: 42
2025-03-23 01:48:16 -   device: cuda
2025-03-23 01:48:16 -   checkpoint_dir: checkpoints
2025-03-23 01:48:16 -   resume: None
2025-03-23 01:48:16 -   disable_amp: False
2025-03-23 01:48:16 -   auto_tune: False
2025-03-23 01:48:16 -   save_loss_plot: False
2025-03-23 01:48:16 -   verbose: False
2025-03-23 01:48:16 -   progress_every: 10
2025-03-23 01:48:16 -   sample_length: 300
2025-03-23 01:48:16 -   temperature: 1.0
2025-03-23 01:48:16 -   early_stopping: 5
2025-03-23 01:48:16 -   max_memory_usage: 0.85
2025-03-23 01:48:16 -   memory_profile: None
2025-03-23 01:48:16 -   force_batch_size: False
2025-03-23 01:48:16 - 
Using device: cuda
2025-03-23 01:48:16 - 
System information:
2025-03-23 01:48:16 -   CUDA device: NVIDIA GeForce GTX 1650 Ti
2025-03-23 01:48:16 -   CUDA capability: (7, 5)
2025-03-23 01:48:16 -   ⚠️ GTX 1650 Ti detected - optimizing for maximum speed
2025-03-23 01:48:16 -   ⚠️ Using fast settings (targeting maximum speed)
2025-03-23 01:48:16 -   CUDA total memory: 4.29 GB
2025-03-23 01:48:16 -   CUDA usable memory: 3.49 GB
2025-03-23 01:48:16 -   CUDA memory allocation target: 2.97 GB (based on 85% usage limit)
2025-03-23 01:48:16 -   PyTorch version: 2.6.0+cu118
2025-03-23 01:48:16 - 
Disabling automatic mixed precision to ensure compatibility
2025-03-23 01:48:16 - 
Loading data from processed_data/got_char_data.pkl
2025-03-23 01:48:16 - Loading data from processed_data/got_char_data.pkl
2025-03-23 01:48:16 - Vocabulary size: 89
2025-03-23 01:48:16 - Train dataset: 19268 sequences
2025-03-23 01:48:16 - Validation dataset: 2141 sequences
2025-03-23 01:48:16 - Applied additional 10% safety margin for small GPU
2025-03-23 01:48:16 - Adjusted device memory for model creation: 3.29 GB
2025-03-23 01:48:17 - 
Model created with 19,223,040 parameters (19,223,040 trainable)
2025-03-23 01:48:17 - Model size in memory: 73.33 MB
2025-03-23 01:48:17 - Using model's optimized optimizer configuration
2025-03-23 01:48:20 - 
================================================================================
2025-03-23 01:48:20 - Starting training from epoch 1 for 1 epochs
2025-03-23 01:48:20 - ================================================================================
2025-03-23 01:48:20 - 
============================================================
2025-03-23 01:48:20 - Epoch 1/1
2025-03-23 01:48:20 - ============================================================
2025-03-23 01:48:20 - 
[2025-03-23 01:48:20] Starting training epoch
2025-03-23 01:48:20 - Total batches: 1752, Batch size: 11
2025-03-23 01:48:20 - Initial CUDA memory allocated: 80.3 MB
2025-03-23 01:49:02 - Progress: 0.5% | Batch 11/2409 | Loss: 5.4587 | Time: 910.6s | ETA: 198519.9s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 01:49:12 - Progress: 0.6% | Batch 7/1205 | Loss: 5.8489 | Time: 1098.8s | ETA: 188050.1s | Memory: 2389.6MB (Peak: 6822.5MB)
2025-03-23 01:49:13 - Progress plot updated
2025-03-23 01:50:55 - Logging output to training_log.txt
2025-03-23 01:50:55 - You can monitor progress with: type training_log.txt
2025-03-23 01:50:55 - ================================================================================
2025-03-23 01:50:55 - Starting training run at 2025-03-23 01:50:55
2025-03-23 01:50:55 - ================================================================================
2025-03-23 01:50:55 - 
Training configuration:
2025-03-23 01:50:55 -   data_path: processed_data/got_char_data.pkl
2025-03-23 01:50:55 -   batch_size: 11
2025-03-23 01:50:55 -   eval_batch_size: 128
2025-03-23 01:50:55 -   epochs: 1
2025-03-23 01:50:55 -   lr: 0.001
2025-03-23 01:50:55 -   weight_decay: 0.01
2025-03-23 01:50:55 -   min_lr: 1e-05
2025-03-23 01:50:55 -   warmup_steps: 1000
2025-03-23 01:50:55 -   seed: None
2025-03-23 01:50:55 -   device: None
2025-03-23 01:50:55 -   early_stopping: 5
2025-03-23 01:50:55 -   max_memory_usage: 0.85
2025-03-23 01:50:55 -   save_loss_plot: True
2025-03-23 01:50:55 -   force_batch_size: False
2025-03-23 01:50:55 -   accumulation_steps: 1
2025-03-23 01:51:42 - Logging output to training_log.txt
2025-03-23 01:51:42 - You can monitor progress with: type training_log.txt
2025-03-23 01:51:42 - ================================================================================
2025-03-23 01:51:42 - Starting training run at 2025-03-23 01:51:42
2025-03-23 01:51:42 - ================================================================================
2025-03-23 01:51:42 - 
Training configuration:
2025-03-23 01:51:42 -   data_path: processed_data/got_char_data.pkl
2025-03-23 01:51:42 -   batch_size: 11
2025-03-23 01:51:42 -   eval_batch_size: 128
2025-03-23 01:51:42 -   accumulation_steps: 1
2025-03-23 01:51:42 -   epochs: 1
2025-03-23 01:51:42 -   lr: 0.001
2025-03-23 01:51:42 -   weight_decay: 0.01
2025-03-23 01:51:42 -   min_lr: 1e-05
2025-03-23 01:51:42 -   warmup_steps: 1000
2025-03-23 01:51:42 -   seed: 42
2025-03-23 01:51:42 -   device: None
2025-03-23 01:51:42 -   checkpoint_dir: checkpoints
2025-03-23 01:51:42 -   resume: None
2025-03-23 01:51:42 -   disable_amp: False
2025-03-23 01:51:42 -   auto_tune: False
2025-03-23 01:51:42 -   save_loss_plot: True
2025-03-23 01:51:42 -   verbose: False
2025-03-23 01:51:42 -   progress_every: 10
2025-03-23 01:51:42 -   sample_length: 300
2025-03-23 01:51:42 -   temperature: 1.0
2025-03-23 01:51:42 -   early_stopping: 5
2025-03-23 01:51:42 -   max_memory_usage: 0.85
2025-03-23 01:51:42 -   memory_profile: None
2025-03-23 01:51:42 -   force_batch_size: False
2025-03-23 01:51:42 - 
Using device: cuda
2025-03-23 01:51:42 - 
System information:
2025-03-23 01:51:42 -   CUDA device: NVIDIA GeForce GTX 1650 Ti
2025-03-23 01:51:42 -   CUDA capability: (7, 5)
2025-03-23 01:51:42 -   ⚠️ GTX 1650 Ti detected - applying speed optimizations
2025-03-23 01:51:42 -   ⚠️ Using optimized memory settings (75% of usable VRAM)
2025-03-23 01:51:42 -   ⚠️ Using gradient accumulation with 2 steps for stability
2025-03-23 01:51:42 -   CUDA total memory: 4.00 GB
2025-03-23 01:51:42 -   CUDA usable memory: 4.00 GB
2025-03-23 01:51:42 -   CUDA memory allocation target: 3.00 GB (based on 75% usage limit)
2025-03-23 01:51:42 -   PyTorch version: 2.6.0+cu118
2025-03-23 01:51:42 - 
Disabling automatic mixed precision to ensure compatibility
2025-03-23 01:51:42 - 
Loading data from processed_data/got_char_data.pkl
2025-03-23 01:51:42 - Loading data from processed_data/got_char_data.pkl
2025-03-23 01:51:42 - Vocabulary size: 89
2025-03-23 01:51:42 - Train dataset: 19268 sequences
2025-03-23 01:51:42 - Validation dataset: 2141 sequences
2025-03-23 01:51:42 - Applied additional 10% safety margin for small GPU
2025-03-23 01:51:42 - Adjusted device memory for model creation: 2.90 GB
2025-03-23 01:51:46 - 
Model created with 85,517,568 parameters (85,517,568 trainable)
2025-03-23 01:51:46 - Model size in memory: 326.22 MB
2025-03-23 01:51:46 - Using model's optimized optimizer configuration
2025-03-23 01:51:49 - 
================================================================================
2025-03-23 01:51:49 - Starting training from epoch 1 for 1 epochs
2025-03-23 01:51:49 - ================================================================================
2025-03-23 01:51:49 - 
============================================================
2025-03-23 01:51:49 - Epoch 1/1
2025-03-23 01:51:49 - ============================================================
2025-03-23 01:51:49 - Initial CUDA memory allocated: 338.7 MB
2025-03-23 01:52:22 - Progress: 0.5% | Batch 12/2409 | Loss: 5.0627 | Time: 1110.3s | ETA: 221782.0s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 01:53:49 - Progress: 0.5% | Batch 13/2409 | Loss: 4.6755 | Time: 1197.7s | ETA: 220739.6s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 01:57:01 - Logging output to training_log.txt
2025-03-23 01:57:01 - You can monitor progress with: type training_log.txt
2025-03-23 01:57:01 - ================================================================================
2025-03-23 01:57:01 - Starting training run at 2025-03-23 01:57:01
2025-03-23 01:57:01 - ================================================================================
2025-03-23 01:57:01 - 
Training configuration:
2025-03-23 01:57:01 -   data_path: processed_data/got_char_data.pkl
2025-03-23 01:57:01 -   batch_size: 11
2025-03-23 01:57:01 -   eval_batch_size: 128
2025-03-23 01:57:01 -   accumulation_steps: 1
2025-03-23 01:57:01 -   epochs: 1
2025-03-23 01:57:01 -   lr: 0.001
2025-03-23 01:57:01 -   weight_decay: 0.01
2025-03-23 01:57:01 -   min_lr: 1e-05
2025-03-23 01:57:01 -   warmup_steps: 1000
2025-03-23 01:57:01 -   seed: 42
2025-03-23 01:57:01 -   device: None
2025-03-23 01:57:01 -   checkpoint_dir: checkpoints
2025-03-23 01:57:01 -   resume: None
2025-03-23 01:57:01 -   disable_amp: False
2025-03-23 01:57:01 -   auto_tune: False
2025-03-23 01:57:01 -   save_loss_plot: False
2025-03-23 01:57:01 -   verbose: False
2025-03-23 01:57:01 -   progress_every: 10
2025-03-23 01:57:01 -   sample_length: 300
2025-03-23 01:57:01 -   temperature: 1.0
2025-03-23 01:57:01 -   early_stopping: 5
2025-03-23 01:57:01 -   max_memory_usage: 0.85
2025-03-23 01:57:01 -   memory_profile: None
2025-03-23 01:57:01 -   force_batch_size: False
2025-03-23 01:57:01 - 
Using device: cuda
2025-03-23 01:57:01 - 
System information:
2025-03-23 01:57:01 -   CUDA device: NVIDIA GeForce GTX 1650 Ti
2025-03-23 01:57:01 -   CUDA capability: (7, 5)
2025-03-23 01:57:01 -   ⚠️ GTX 1650 Ti detected - optimizing for maximum speed
2025-03-23 01:57:01 -   CUDA total memory: 4.00 GB
2025-03-23 01:57:01 -   CUDA usable memory: 4.00 GB
2025-03-23 01:57:01 -   CUDA memory allocation target: 3.40 GB (based on 85% usage limit)
2025-03-23 01:57:01 -   PyTorch version: 2.6.0+cu118
2025-03-23 01:57:01 - 
Disabling automatic mixed precision to ensure compatibility
2025-03-23 01:57:01 - 
Loading data from processed_data/got_char_data.pkl
2025-03-23 01:57:01 - Loading data from processed_data/got_char_data.pkl
2025-03-23 01:57:02 - Vocabulary size: 89
2025-03-23 01:57:02 - Train dataset: 19268 sequences
2025-03-23 01:57:02 - Validation dataset: 2141 sequences
2025-03-23 01:57:02 - Applied additional 10% safety margin for small GPU
2025-03-23 01:57:02 - Adjusted device memory for model creation: 3.29 GB
2025-03-23 01:57:04 - Progress: 0.6% | Batch 14/2409 | Loss: 4.4102 | Time: 1393.1s | ETA: 238326.8s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 01:57:05 - 
Model created with 85,517,568 parameters (85,517,568 trainable)
2025-03-23 01:57:05 - Model size in memory: 326.22 MB
2025-03-23 01:57:05 - Using model's optimized optimizer configuration
2025-03-23 01:57:08 - 
================================================================================
2025-03-23 01:57:08 - Starting training from epoch 1 for 1 epochs
2025-03-23 01:57:08 - ================================================================================
2025-03-23 01:57:08 - 
============================================================
2025-03-23 01:57:08 - Epoch 1/1
2025-03-23 01:57:08 - ============================================================
2025-03-23 01:57:08 - Initial CUDA memory allocated: 338.7 MB
2025-03-23 01:58:24 - Progress: 0.6% | Batch 15/2409 | Loss: 4.2414 | Time: 1473.0s | ETA: 235091.6s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 01:58:38 - Progress: 0.7% | Batch 8/1205 | Loss: 5.6599 | Time: 1664.7s | ETA: 249085.5s | Memory: 2389.6MB (Peak: 6822.5MB)
2025-03-23 01:58:39 - Progress plot updated
2025-03-23 01:59:37 - Progress: 0.7% | Batch 16/2409 | Loss: 4.0691 | Time: 1545.9s | ETA: 231201.6s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 02:02:07 - ================================================================================
2025-03-23 02:02:07 - Starting training run at 2025-03-23 02:02:07
2025-03-23 02:02:07 - ================================================================================
2025-03-23 02:02:07 - 
Training configuration:
2025-03-23 02:02:07 -   data_path: processed_data/got_char_data.pkl
2025-03-23 02:02:07 -   batch_size: 11
2025-03-23 02:02:07 -   eval_batch_size: 128
2025-03-23 02:02:07 -   epochs: 1
2025-03-23 02:02:07 -   lr: 0.001
2025-03-23 02:02:07 -   weight_decay: 0.01
2025-03-23 02:02:07 -   min_lr: 1e-05
2025-03-23 02:02:07 -   warmup_steps: 1000
2025-03-23 02:02:07 -   seed: 42
2025-03-23 02:02:07 -   device: None
2025-03-23 02:02:07 -   progress_every: 20
2025-03-23 02:02:07 -   checkpoint_dir: checkpoints
2025-03-23 02:02:07 -   resume: None
2025-03-23 02:02:07 -   disable_amp: False
2025-03-23 02:02:07 -   max_memory_usage: 0.85
2025-03-23 02:02:07 -   force_batch_size: False
2025-03-23 02:02:07 -   save_loss_plot: False
2025-03-23 02:02:07 -   verbose: False
2025-03-23 02:02:07 -   sample_length: 300
2025-03-23 02:02:07 -   temperature: 1.0
2025-03-23 02:02:07 -   early_stopping: 5
2025-03-23 02:02:31 - ================================================================================
2025-03-23 02:02:31 - Starting training run at 2025-03-23 02:02:31
2025-03-23 02:02:31 - ================================================================================
2025-03-23 02:02:31 - 
Training configuration:
2025-03-23 02:02:31 -   data_path: processed_data/got_char_data.pkl
2025-03-23 02:02:31 -   batch_size: 11
2025-03-23 02:02:31 -   eval_batch_size: 128
2025-03-23 02:02:31 -   epochs: 1
2025-03-23 02:02:31 -   lr: 0.001
2025-03-23 02:02:31 -   weight_decay: 0.01
2025-03-23 02:02:31 -   min_lr: 1e-05
2025-03-23 02:02:31 -   warmup_steps: 1000
2025-03-23 02:02:31 -   seed: 42
2025-03-23 02:02:31 -   device: None
2025-03-23 02:02:31 -   progress_every: 20
2025-03-23 02:02:31 -   checkpoint_dir: checkpoints
2025-03-23 02:02:31 -   resume: None
2025-03-23 02:02:31 -   disable_amp: False
2025-03-23 02:02:31 -   max_memory_usage: 0.85
2025-03-23 02:02:31 -   force_batch_size: False
2025-03-23 02:02:31 -   save_loss_plot: False
2025-03-23 02:02:31 -   verbose: False
2025-03-23 02:02:31 -   sample_length: 300
2025-03-23 02:02:31 -   temperature: 1.0
2025-03-23 02:02:31 -   early_stopping: 5
2025-03-23 02:02:31 - 
Using device: cuda
2025-03-23 02:02:31 - 
System information:
2025-03-23 02:02:31 -   CUDA device: NVIDIA GeForce GTX 1650 Ti
2025-03-23 02:02:31 -   CUDA capability: (7, 5)
2025-03-23 02:02:31 -   ⚠️ GTX 1650 Ti detected - optimizing for speed
2025-03-23 02:02:31 -   ⚠️ Using optimal batch size of 11 for training
2025-03-23 02:02:31 -   ⚠️ Using 85% memory usage limit for maximum speed
2025-03-23 02:02:31 -   CUDA total memory: 4.00 GB
2025-03-23 02:02:31 -   CUDA usable memory: 3.80 GB
2025-03-23 02:02:31 -   CUDA memory allocation target: 3.23 GB (based on 85.0% usage limit)
2025-03-23 02:02:31 -   PyTorch version: 2.6.0+cu118
2025-03-23 02:02:31 - 
Disabling automatic mixed precision to ensure compatibility
2025-03-23 02:02:31 - 
Loading data from processed_data/got_char_data.pkl
2025-03-23 02:02:31 - Loading data from processed_data/got_char_data.pkl
2025-03-23 02:02:31 - Loading data from processed_data/got_char_data.pkl
2025-03-23 02:02:31 - Adjusted device memory for model creation: 3.55 GB
2025-03-23 02:02:34 - 
Model created with 85,517,568 parameters (85,517,568 trainable)
2025-03-23 02:02:34 - Model size in memory: 326.22 MB
2025-03-23 02:02:34 - Using model's optimized optimizer configuration
2025-03-23 02:03:04 - ================================================================================
2025-03-23 02:03:04 - Starting training run at 2025-03-23 02:03:04
2025-03-23 02:03:04 - ================================================================================
2025-03-23 02:03:04 - 
Training configuration:
2025-03-23 02:03:04 -   data_path: processed_data/got_char_data.pkl
2025-03-23 02:03:04 -   batch_size: 11
2025-03-23 02:03:04 -   eval_batch_size: 128
2025-03-23 02:03:04 -   epochs: 1
2025-03-23 02:03:04 -   lr: 0.001
2025-03-23 02:03:04 -   weight_decay: 0.01
2025-03-23 02:03:04 -   min_lr: 1e-05
2025-03-23 02:03:04 -   warmup_steps: 1000
2025-03-23 02:03:04 -   seed: 42
2025-03-23 02:03:04 -   device: None
2025-03-23 02:03:04 -   progress_every: 20
2025-03-23 02:03:04 -   checkpoint_dir: checkpoints
2025-03-23 02:03:04 -   resume: None
2025-03-23 02:03:04 -   disable_amp: False
2025-03-23 02:03:04 -   max_memory_usage: 0.85
2025-03-23 02:03:04 -   force_batch_size: False
2025-03-23 02:03:04 -   save_loss_plot: False
2025-03-23 02:03:04 -   verbose: False
2025-03-23 02:03:04 -   sample_length: 300
2025-03-23 02:03:04 -   temperature: 1.0
2025-03-23 02:03:04 -   early_stopping: 5
2025-03-23 02:03:04 - 
Using device: cuda
2025-03-23 02:03:04 - 
System information:
2025-03-23 02:03:04 -   CUDA device: NVIDIA GeForce GTX 1650 Ti
2025-03-23 02:03:04 -   CUDA capability: (7, 5)
2025-03-23 02:03:04 -   ⚠️ GTX 1650 Ti detected - optimizing for speed
2025-03-23 02:03:04 -   ⚠️ Using optimal batch size of 11 for training
2025-03-23 02:03:04 -   ⚠️ Using 85% memory usage limit for maximum speed
2025-03-23 02:03:04 -   CUDA total memory: 4.00 GB
2025-03-23 02:03:04 -   CUDA usable memory: 3.80 GB
2025-03-23 02:03:04 -   CUDA memory allocation target: 3.23 GB (based on 85.0% usage limit)
2025-03-23 02:03:04 -   PyTorch version: 2.6.0+cu118
2025-03-23 02:03:04 - 
Disabling automatic mixed precision to ensure compatibility
2025-03-23 02:03:04 - 
Loading data from processed_data/got_char_data.pkl
2025-03-23 02:03:04 - Loading data from processed_data/got_char_data.pkl
2025-03-23 02:03:04 - Loading data from processed_data/got_char_data.pkl
2025-03-23 02:03:04 - Adjusted device memory for model creation: 3.55 GB
2025-03-23 02:03:07 - 
Model created with 85,517,568 parameters (85,517,568 trainable)
2025-03-23 02:03:07 - Model size in memory: 326.22 MB
2025-03-23 02:03:07 - Using model's optimized optimizer configuration
2025-03-23 02:03:09 - 
================================================================================
2025-03-23 02:03:09 - Starting training from epoch 1 for 1 epochs
2025-03-23 02:03:09 - ================================================================================
2025-03-23 02:03:09 - 
============================================================
2025-03-23 02:03:09 - Epoch 1/1
2025-03-23 02:03:09 - ============================================================
2025-03-23 02:03:09 - Initial CUDA memory allocated: 338.7 MB
2025-03-23 02:03:53 - Progress: 0.7% | Batch 17/2409 | Loss: 3.9103 | Time: 1802.0s | ETA: 253556.9s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 02:04:34 - Progress: 0.2% | Batch 3/1752 | Loss: 0.0000 | Time: 85.1s | ETA: 49593.4s | Memory: 1345.6MB (Peak: 3965.4MB)
2025-03-23 02:06:12 - ================================================================================
2025-03-23 02:06:12 - Starting training run at 2025-03-23 02:06:12
2025-03-23 02:06:12 - ================================================================================
2025-03-23 02:06:12 - 
Training configuration:
2025-03-23 02:06:12 -   data_path: processed_data/got_char_data.pkl
2025-03-23 02:06:12 -   batch_size: 11
2025-03-23 02:06:12 -   eval_batch_size: 128
2025-03-23 02:06:12 -   epochs: 1
2025-03-23 02:06:12 -   lr: 0.001
2025-03-23 02:06:12 -   weight_decay: 0.01
2025-03-23 02:06:12 -   min_lr: 1e-05
2025-03-23 02:06:12 -   warmup_steps: 1000
2025-03-23 02:06:12 -   seed: 42
2025-03-23 02:06:12 -   device: None
2025-03-23 02:06:12 -   progress_every: 20
2025-03-23 02:06:12 -   checkpoint_dir: checkpoints
2025-03-23 02:06:12 -   resume: None
2025-03-23 02:06:12 -   disable_amp: False
2025-03-23 02:06:12 -   max_memory_usage: 0.85
2025-03-23 02:06:12 -   force_batch_size: False
2025-03-23 02:06:12 -   save_loss_plot: False
2025-03-23 02:06:12 -   verbose: False
2025-03-23 02:06:12 -   sample_length: 300
2025-03-23 02:06:12 -   temperature: 1.0
2025-03-23 02:06:12 -   early_stopping: 5
2025-03-23 02:06:12 - 
Using device: cuda
2025-03-23 02:06:12 - 
System information:
2025-03-23 02:06:12 -   CUDA device: NVIDIA GeForce GTX 1650 Ti
2025-03-23 02:06:12 -   CUDA capability: (7, 5)
2025-03-23 02:06:12 -   ⚠️ GTX 1650 Ti detected - optimizing for speed
2025-03-23 02:06:12 -   ⚠️ Using optimal batch size of 11 for training
2025-03-23 02:06:12 -   ⚠️ Using 85% memory usage limit for maximum speed
2025-03-23 02:06:12 -   CUDA total memory: 4.00 GB
2025-03-23 02:06:12 -   CUDA usable memory: 3.80 GB
2025-03-23 02:06:12 -   CUDA memory allocation target: 3.23 GB (based on 85.0% usage limit)
2025-03-23 02:06:12 -   PyTorch version: 2.6.0+cu118
2025-03-23 02:06:12 - 
Disabling automatic mixed precision to ensure compatibility
2025-03-23 02:06:12 - 
Loading data from processed_data/got_char_data.pkl
2025-03-23 02:06:12 - Loading data from processed_data/got_char_data.pkl
2025-03-23 02:06:12 - Loading data from processed_data/got_char_data.pkl
2025-03-23 02:06:12 - Adjusted device memory for model creation: 3.55 GB
2025-03-23 02:06:16 - 
Model created with 85,517,568 parameters (85,517,568 trainable)
2025-03-23 02:06:16 - Model size in memory: 326.22 MB
2025-03-23 02:06:16 - Using model's optimized optimizer configuration
2025-03-23 02:06:17 - Progress: 0.7% | Batch 18/2409 | Loss: 3.7651 | Time: 1945.5s | ETA: 258423.1s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 02:06:19 - 
================================================================================
2025-03-23 02:06:19 - Starting training from epoch 1 for 1 epochs
2025-03-23 02:06:19 - ================================================================================
2025-03-23 02:06:19 - 
============================================================
2025-03-23 02:06:19 - Epoch 1/1
2025-03-23 02:06:19 - ============================================================
2025-03-23 02:06:19 - Initial CUDA memory allocated: 338.7 MB
2025-03-23 02:07:41 - Progress: 0.8% | Batch 19/2409 | Loss: 3.7003 | Time: 2029.8s | ETA: 255328.2s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 02:08:05 - Progress: 0.7% | Batch 9/1205 | Loss: 5.4649 | Time: 2231.2s | ETA: 296496.2s | Memory: 2389.6MB (Peak: 6822.5MB)
2025-03-23 02:08:06 - Progress plot updated
2025-03-23 02:15:16 - ================================================================================
2025-03-23 02:15:16 - Starting training run at 2025-03-23 02:15:16
2025-03-23 02:15:16 - ================================================================================
2025-03-23 02:15:16 - 
Training configuration:
2025-03-23 02:15:16 -   data_path: processed_data/got_char_data.pkl
2025-03-23 02:15:16 -   batch_size: 11
2025-03-23 02:15:16 -   eval_batch_size: 128
2025-03-23 02:15:16 -   epochs: 1
2025-03-23 02:15:16 -   lr: 0.001
2025-03-23 02:15:16 -   weight_decay: 0.01
2025-03-23 02:15:16 -   min_lr: 1e-05
2025-03-23 02:15:16 -   warmup_steps: 1000
2025-03-23 02:15:16 -   seed: 42
2025-03-23 02:15:16 -   device: None
2025-03-23 02:15:16 -   progress_every: 20
2025-03-23 02:15:16 -   checkpoint_dir: checkpoints
2025-03-23 02:15:16 -   resume: None
2025-03-23 02:15:16 -   disable_amp: False
2025-03-23 02:15:16 -   max_memory_usage: 0.85
2025-03-23 02:15:16 -   force_batch_size: False
2025-03-23 02:15:16 -   save_loss_plot: False
2025-03-23 02:15:16 -   verbose: False
2025-03-23 02:15:16 -   sample_length: 300
2025-03-23 02:15:16 -   temperature: 1.0
2025-03-23 02:15:16 -   early_stopping: 5
2025-03-23 02:15:16 - 
Using device: cuda
2025-03-23 02:15:16 - 
System information:
2025-03-23 02:15:16 -   CUDA device: NVIDIA GeForce GTX 1650 Ti
2025-03-23 02:15:16 -   CUDA capability: (7, 5)
2025-03-23 02:15:16 -   ⚠️ GTX 1650 Ti detected - optimizing for MAXIMUM speed
2025-03-23 02:15:16 -   ⚠️ Using optimal batch size of 11 for faster training
2025-03-23 02:15:16 -   ⚠️ Using 85% memory usage limit for maximum speed
2025-03-23 02:15:16 -   CUDA total memory: 4.00 GB
2025-03-23 02:15:16 -   CUDA usable memory: 3.80 GB
2025-03-23 02:15:16 -   CUDA memory allocation target: 3.23 GB (based on 85.0% usage limit)
2025-03-23 02:15:16 -   PyTorch version: 2.6.0+cu118
2025-03-23 02:15:16 - 
Disabling automatic mixed precision to ensure compatibility and maximum speed
2025-03-23 02:15:16 - 
Loading data from processed_data/got_char_data.pkl
2025-03-23 02:15:16 - Loading data from processed_data/got_char_data.pkl
2025-03-23 02:15:16 - Loading data from processed_data/got_char_data.pkl
2025-03-23 02:15:17 - Using the proven 85M parameter model that was previously fast
2025-03-23 02:15:21 - 
Model created with 85,517,568 parameters (85,517,568 trainable)
2025-03-23 02:15:21 - Model size in memory: 326.22 MB
2025-03-23 02:15:21 - Using model's optimized optimizer configuration
2025-03-23 02:15:24 - 
================================================================================
2025-03-23 02:15:24 - Starting training from epoch 1 for 1 epochs
2025-03-23 02:15:24 - ================================================================================
2025-03-23 02:15:24 - 
============================================================
2025-03-23 02:15:24 - Epoch 1/1
2025-03-23 02:15:24 - ============================================================
2025-03-23 02:15:24 - Initial CUDA memory allocated: 338.7 MB
2025-03-23 02:16:49 - Progress: 0.1% | Batch 2/1752 | Loss: 5.3611 | Speed: 0.02 batches/s (0.3 samples/s) | Time: 85.7s | ETA: 74987.9s | Memory: 1346.8MB
2025-03-23 02:16:54 - Progress: 0.9% | Batch 21/2409 | Loss: 3.6222 | Time: 2582.7s | ETA: 293694.1s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 02:17:55 - Progress: 0.9% | Batch 22/2409 | Loss: 3.6070 | Time: 2643.5s | ETA: 286821.4s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 02:18:29 - Progress: 0.2% | Batch 4/1752 | Loss: 4.7784 | Speed: 0.02 batches/s (0.2 samples/s) | Time: 185.0s | ETA: 80847.7s | Memory: 1347.8MB
2025-03-23 02:19:09 - Progress: 1.0% | Batch 23/2409 | Loss: 3.5869 | Time: 2718.2s | ETA: 281980.7s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 02:19:23 - Progress: 0.8% | Batch 10/1205 | Loss: 5.2673 | Time: 2910.0s | ETA: 347743.9s | Memory: 2389.6MB (Peak: 6822.5MB)
2025-03-23 02:19:24 - Progress plot updated
2025-03-23 02:21:46 - ================================================================================
2025-03-23 02:21:46 - Starting training run at 2025-03-23 02:21:46
2025-03-23 02:21:46 - ================================================================================
2025-03-23 02:21:46 - 
Training configuration:
2025-03-23 02:21:46 -   data_path: processed_data/got_char_data.pkl
2025-03-23 02:21:46 -   batch_size: 16
2025-03-23 02:21:46 -   eval_batch_size: 128
2025-03-23 02:21:46 -   epochs: 1
2025-03-23 02:21:46 -   lr: 0.001
2025-03-23 02:21:46 -   weight_decay: 0.01
2025-03-23 02:21:46 -   min_lr: 1e-05
2025-03-23 02:21:46 -   warmup_steps: 1000
2025-03-23 02:21:46 -   seed: 42
2025-03-23 02:21:46 -   disable_amp: False
2025-03-23 02:21:46 -   progress_every: 20
2025-03-23 02:21:46 -   checkpoint_dir: checkpoints
2025-03-23 02:21:46 -   resume: None
2025-03-23 02:21:46 -   save_loss_plot: False
2025-03-23 02:21:46 -   sample_length: 300
2025-03-23 02:21:46 -   temperature: 1.0
2025-03-23 02:21:46 -   early_stopping: 5
2025-03-23 02:21:46 - 
Using device: cuda
2025-03-23 02:21:46 - 
System information:
2025-03-23 02:21:46 -   CUDA device: NVIDIA GeForce GTX 1650 Ti
2025-03-23 02:21:46 -   PyTorch version: 2.6.0+cu118
2025-03-23 02:21:46 -   CUDA total memory: 4.00 GB
2025-03-23 02:21:46 - Loading data from processed_data/got_char_data.pkl
2025-03-23 02:22:24 - ================================================================================
2025-03-23 02:22:24 - Starting training run at 2025-03-23 02:22:24
2025-03-23 02:22:24 - ================================================================================
2025-03-23 02:22:24 - 
Training configuration:
2025-03-23 02:22:24 -   data_path: processed_data/got_char_data.pkl
2025-03-23 02:22:24 -   batch_size: 16
2025-03-23 02:22:24 -   eval_batch_size: 128
2025-03-23 02:22:24 -   epochs: 1
2025-03-23 02:22:24 -   lr: 0.001
2025-03-23 02:22:24 -   weight_decay: 0.01
2025-03-23 02:22:24 -   min_lr: 1e-05
2025-03-23 02:22:24 -   warmup_steps: 1000
2025-03-23 02:22:24 -   seed: 42
2025-03-23 02:22:24 -   disable_amp: False
2025-03-23 02:22:24 -   progress_every: 20
2025-03-23 02:22:24 -   checkpoint_dir: checkpoints
2025-03-23 02:22:24 -   resume: None
2025-03-23 02:22:24 -   save_loss_plot: False
2025-03-23 02:22:24 -   sample_length: 300
2025-03-23 02:22:24 -   temperature: 1.0
2025-03-23 02:22:24 -   early_stopping: 5
2025-03-23 02:22:24 - 
Using device: cuda
2025-03-23 02:22:24 - 
System information:
2025-03-23 02:22:24 -   CUDA device: NVIDIA GeForce GTX 1650 Ti
2025-03-23 02:22:24 -   PyTorch version: 2.6.0+cu118
2025-03-23 02:22:24 -   CUDA total memory: 4.00 GB
2025-03-23 02:22:24 - Loading data from processed_data/got_char_data.pkl
2025-03-23 02:22:28 - 
Model created with 85,320,960 parameters (85,320,960 trainable)
2025-03-23 02:22:30 - 
================================================================================
2025-03-23 02:22:30 - Starting training from epoch 1 for 1 epochs
2025-03-23 02:22:30 - ================================================================================
2025-03-23 02:22:30 - 
============================================================
2025-03-23 02:22:30 - Epoch 1/1
2025-03-23 02:22:30 - ============================================================
2025-03-23 02:22:30 - Starting epoch with 1205 batches of size 16
2025-03-23 02:23:18 - Progress: 0.1% | Batch 1/1205 | Loss: 4.5714 | Speed: 0.02 it/s | ETA: 0.0s
2025-03-23 02:23:21 - Progress: 1.0% | Batch 24/2409 | Loss: 3.5400 | Time: 2970.2s | ETA: 295158.7s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 02:25:02 - Progress: 1.0% | Batch 25/2409 | Loss: 3.5054 | Time: 3070.5s | ETA: 292800.7s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 02:25:12 - Progress: 0.2% | Batch 3/1205 | Loss: 4.7220 | Speed: 0.02 it/s | ETA: 64822.6s
2025-03-23 02:33:56 - 
========== TRAINING START ==========
2025-03-23 02:33:56 - Start time: 2025-03-23 02:33:56
2025-03-23 02:33:56 - Batch size: None, Epochs: 1, Learning rate: 0.0003
2025-03-23 02:33:56 - Using device: cuda
2025-03-23 02:33:56 - GPU: NVIDIA GeForce GTX 1650 Ti
2025-03-23 02:33:56 - Using optimal batch size for GTX 1650 Ti: 11
2025-03-23 02:33:56 - Loading data from processed_data/got_char_data.pkl
2025-03-23 02:33:56 - Vocab size: 89
2025-03-23 02:33:56 - Training sequences: 19268
2025-03-23 02:33:56 - Validation sequences: 2141
2025-03-23 02:33:59 - Model: 85,910,784 parameters
2025-03-23 02:34:02 - Automatic mixed precision disabled for GTX GPU
2025-03-23 02:34:02 - 
========== STARTING TRAINING ==========
2025-03-23 02:34:02 - 
Epoch 1/1
2025-03-23 02:34:17 - Progress: 1.1% | Batch 26/2409 | Loss: 3.4828 | Time: 3625.6s | ETA: 332295.8s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 02:34:45 - Epoch progress: 0.1% | Loss: 4.6372 | Speed: 66 tokens/sec | 42657.1 ms/batch
2025-03-23 02:34:49 - Progress: 0.9% | Batch 11/1205 | Loss: 5.1677 | Time: 3835.9s | ETA: 416373.8s | Memory: 2389.6MB (Peak: 6822.5MB)
2025-03-23 02:34:51 - Progress plot updated
2025-03-23 02:35:40 - Progress: 1.1% | Batch 27/2409 | Loss: 3.4861 | Time: 3709.2s | ETA: 327234.8s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 02:41:45 - 
========== TRAINING START ==========
2025-03-23 02:41:45 - Start time: 2025-03-23 02:41:45
2025-03-23 02:41:45 - Applied GTX 1650 Ti specific optimizations
2025-03-23 02:41:45 - Optimized batch size for GTX 1650 Ti: 12
2025-03-23 02:41:45 - Batch size: 12, Epochs: 1, Learning rate: 0.0003
2025-03-23 02:41:45 - Using device: cuda
2025-03-23 02:41:45 - GPU: NVIDIA GeForce GTX 1650 Ti
2025-03-23 02:41:45 - CUDA optimizations: cuDNN benchmark enabled
2025-03-23 02:41:45 - Loading data from processed_data/got_char_data.pkl
2025-03-23 02:41:45 - Data loaded: Train sequences: 19268, Val sequences: 2141
2025-03-23 02:41:45 - Created data loaders: 1605 training batches, 45 validation batches
2025-03-23 02:41:46 - Vocab size: 89
2025-03-23 02:41:46 - Training sequences: 19268
2025-03-23 02:41:46 - Validation sequences: 2141
2025-03-23 02:41:49 - Using channels_last memory format for improved performance
2025-03-23 02:41:49 - Model: 85,910,784 parameters (327.72 MB)
2025-03-23 02:41:51 - Progress: 1.2% | Batch 28/2409 | Loss: 3.4918 | Time: 4079.3s | ETA: 346890.3s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 02:41:51 - Automatic mixed precision disabled for maximum compatibility
2025-03-23 02:41:51 - 
========== STARTING TRAINING ==========
2025-03-23 02:41:51 - 
Epoch 1/1
2025-03-23 02:42:30 - Epoch progress: 0.1% | Loss: 4.6280 | Speed: 80 tokens/sec | 38355.7 ms/batch | ETA: 0.0s | Mem: 1390MB/3328MB
2025-03-23 02:43:43 - Progress: 1.2% | Batch 29/2409 | Loss: 3.4680 | Time: 4191.5s | ETA: 343989.5s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 02:45:04 - Progress: 1.0% | Batch 12/1205 | Loss: 4.7993 | Time: 4450.6s | ETA: 442467.0s | Memory: 2389.6MB (Peak: 6822.5MB)
2025-03-23 02:45:05 - Progress plot updated
2025-03-23 02:51:59 - Progress: 1.2% | Batch 30/2409 | Loss: 3.4598 | Time: 4688.1s | ETA: 371767.3s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 02:53:40 - Progress: 1.3% | Batch 31/2409 | Loss: 3.4456 | Time: 4789.2s | ETA: 367375.8s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 02:55:01 - Progress: 1.3% | Batch 32/2409 | Loss: 3.4424 | Time: 4869.4s | ETA: 361705.1s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 03:05:47 - Progress: 1.4% | Batch 33/2409 | Loss: 3.4383 | Time: 5516.0s | ETA: 397152.3s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 03:06:45 - Progress: 1.1% | Batch 13/1205 | Loss: 4.3709 | Time: 5751.7s | ETA: 527383.9s | Memory: 2389.6MB (Peak: 6822.5MB)
2025-03-23 03:06:47 - Progress plot updated
2025-03-23 03:06:51 - Progress: 1.4% | Batch 34/2409 | Loss: 3.4454 | Time: 5579.4s | ETA: 389737.7s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 03:08:13 - Progress: 1.5% | Batch 35/2409 | Loss: 3.4502 | Time: 5662.2s | ETA: 384060.7s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 03:09:56 - Progress: 1.5% | Batch 36/2409 | Loss: 3.4511 | Time: 5764.5s | ETA: 379975.2s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 03:10:59 - Progress: 1.5% | Batch 37/2409 | Loss: 3.4483 | Time: 5827.4s | ETA: 373580.6s | Memory: 1603.3MB (Peak: 2917.7MB)
2025-03-23 03:26:22 - Starting training with arguments: {'data_path': 'processed_data/got_char_data.pkl', 'batch_size': 8, 'force_batch_size': False, 'epochs': 1, 'lr': 0.0003, 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'disable_amp': True, 'use_bfloat16': False, 'max_memory_usage': 0.85, 'gradient_accumulation_steps': 4, 'use_compile': False, 'num_workers': 2, 'use_onecycle': True, 'warmup_pct': 0.1, 'seed': 42, 'device': 'cuda', 'checkpoint_dir': 'checkpoints', 'sample_every_epoch': False}
2025-03-23 03:26:22 - Using device: cuda (NVIDIA GeForce GTX 1650 Ti), CUDA Version: 11.8, PyTorch: 2.6.0+cu118
2025-03-23 03:26:22 - Detected GTX 1650 Ti - Using enhanced optimizations for maximum speed
2025-03-23 03:26:22 - Setting optimal batch size to 12 for GTX 1650 Ti
2025-03-23 03:26:22 - Setting max memory usage to 90.0% for faster training
2025-03-23 03:26:22 - Disabling Automatic Mixed Precision (AMP) for better speed on GTX 1650 Ti
2025-03-23 03:26:22 - Total CUDA memory: 4095.7MB
2025-03-23 03:26:22 - Usable CUDA memory (after context): 3295.7MB
2025-03-23 03:26:22 - Memory allocation target: 2966.1MB (90.0%)
2025-03-23 03:26:22 - Not using AMP - training with standard precision
2025-03-23 03:26:22 - Loading data from processed_data/got_char_data.pkl
2025-03-23 03:26:22 - Loading data from processed_data/got_char_data.pkl
2025-03-23 03:26:22 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 03:26:22 - Created data loaders: 1605 training batches, 45 validation batches
2025-03-23 03:26:22 - Using 2 workers for data loading with prefetch_factor=2
2025-03-23 03:26:23 - Model created with 85,910,784 parameters
2025-03-23 03:26:23 - Model size in memory: 327.72MB
2025-03-23 03:26:26 - Using OneCycleLR scheduler with warmup_pct=0.1
2025-03-23 03:26:26 - OneCycleLR: Initial LR=0.000012, Peak LR=0.000900, Final LR=0.00000000
2025-03-23 03:26:26 - Starting epoch 1/1
2025-03-23 03:26:26 - Starting training epoch 1 with gradient accumulation steps: 4
2025-03-23 03:26:26 - Total batches: 1605, Batch size: 12
2025-03-23 03:26:26 - Initial CUDA memory allocated: 379.0 MB
2025-03-23 03:26:31 - [03:26:31] Progress: 0.1% | Batch 1/1605 | Loss: 4.6015 | Rate: 10043.3 tokens/sec | ETA: N/A, CUDA Memory: 731.5MB/4095.7MB
2025-03-23 03:26:54 - [03:26:54] Progress: 0.6% | Batch 10/1605 | Loss: 3.7841 | Rate: 5036.4 tokens/sec | ETA: N/A, CUDA Memory: 1390.6MB/4095.7MB
2025-03-23 03:27:58 - [03:27:58] Progress: 1.7% | Batch 27/1605 | Loss: 3.4484 | Rate: 3751.7 tokens/sec | ETA: 1.4h, CUDA Memory: 1390.1MB/4095.7MB
2025-03-23 03:29:02 - [03:29:02] Progress: 2.7% | Batch 44/1605 | Loss: 3.3452 | Rate: 3549.9 tokens/sec | ETA: 1.5h, CUDA Memory: 1390.6MB/4095.7MB
2025-03-23 03:29:54 - Starting training with arguments: {'data_path': 'processed_data/got_char_data.pkl', 'batch_size': None, 'force_batch_size': False, 'epochs': 1, 'lr': 0.0003, 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'disable_amp': True, 'use_bfloat16': False, 'max_memory_usage': 0.85, 'gradient_accumulation_steps': 4, 'use_compile': False, 'num_workers': 2, 'use_onecycle': True, 'warmup_pct': 0.1, 'seed': 42, 'device': 'cuda', 'checkpoint_dir': 'checkpoints', 'sample_every_epoch': False}
2025-03-23 03:29:54 - Using device: cuda (NVIDIA GeForce GTX 1650 Ti), CUDA Version: 11.8, PyTorch: 2.6.0+cu118
2025-03-23 03:29:54 - Detected GTX 1650 Ti - Using enhanced optimizations for maximum speed
2025-03-23 03:29:54 - Setting optimal batch size to 16 for GTX 1650 Ti
2025-03-23 03:29:54 - Setting max memory usage to 95.0% for faster training
2025-03-23 03:29:54 - Disabling Automatic Mixed Precision (AMP) for better speed on GTX 1650 Ti
2025-03-23 03:29:54 - Total CUDA memory: 4095.7MB
2025-03-23 03:29:54 - Usable CUDA memory (after context): 3295.7MB
2025-03-23 03:29:54 - Memory allocation target: 3130.9MB (95.0%)
2025-03-23 03:29:54 - Not using AMP - training with standard precision
2025-03-23 03:29:54 - Loading data from processed_data/got_char_data.pkl
2025-03-23 03:29:54 - Loading data from processed_data/got_char_data.pkl
2025-03-23 03:29:55 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 03:29:55 - Created data loaders: 1204 training batches, 34 validation batches
2025-03-23 03:29:55 - Using 2 workers for data loading with prefetch_factor=2
2025-03-23 03:29:56 - Model created with 85,910,784 parameters
2025-03-23 03:29:56 - Model size in memory: 327.72MB
2025-03-23 03:29:58 - Using OneCycleLR scheduler with warmup_pct=0.1
2025-03-23 03:29:58 - OneCycleLR: Initial LR=0.000012, Peak LR=0.000900, Final LR=0.00000000
2025-03-23 03:29:58 - Starting epoch 1/1
2025-03-23 03:29:58 - Starting training epoch 1 with gradient accumulation steps: 4
2025-03-23 03:29:58 - Total batches: 1204, Batch size: 16
2025-03-23 03:29:58 - Initial CUDA memory allocated: 379.0 MB
2025-03-23 03:30:06 - [03:30:06] Progress: 0.1% | Batch 1/1204 | Loss: 4.6084 | Rate: 4574.6 tokens/sec | ETA: N/A, CUDA Memory: 730.8MB/4095.7MB
2025-03-23 03:31:06 - [03:31:06] Progress: 0.7% | Batch 8/1204 | Loss: 3.9699 | Rate: 2053.4 tokens/sec | ETA: N/A, CUDA Memory: 1397.3MB/4095.7MB
2025-03-23 03:31:27 - [03:31:27] Progress: 0.8% | Batch 10/1204 | Loss: 3.7968 | Rate: 1947.0 tokens/sec | ETA: N/A, CUDA Memory: 1398.0MB/4095.7MB
2025-03-23 03:33:38 - Starting training with arguments: {'data_path': 'processed_data/got_char_data.pkl', 'batch_size': None, 'force_batch_size': False, 'epochs': 1, 'lr': 0.0003, 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'disable_amp': True, 'use_bfloat16': False, 'max_memory_usage': 0.85, 'gradient_accumulation_steps': 4, 'use_compile': False, 'num_workers': 2, 'use_onecycle': True, 'warmup_pct': 0.1, 'seed': 42, 'device': 'cuda', 'checkpoint_dir': 'checkpoints', 'sample_every_epoch': False}
2025-03-23 03:33:38 - Using device: cuda (NVIDIA GeForce GTX 1650 Ti), CUDA Version: 11.8, PyTorch: 2.6.0+cu118
2025-03-23 03:33:38 - Detected GTX 1650 Ti - Using enhanced optimizations for maximum speed
2025-03-23 03:33:38 - Setting optimal batch size to 32 for GTX 1650 Ti
2025-03-23 03:33:38 - Setting max memory usage to 98.0% for faster training
2025-03-23 03:33:38 - Disabling Automatic Mixed Precision (AMP) for better speed on GTX 1650 Ti
2025-03-23 03:33:38 - Total CUDA memory: 4095.7MB
2025-03-23 03:33:38 - Usable CUDA memory (after context): 3295.7MB
2025-03-23 03:33:38 - Memory allocation target: 3229.8MB (98.0%)
2025-03-23 03:33:38 - Not using AMP - training with standard precision
2025-03-23 03:33:38 - Loading data from processed_data/got_char_data.pkl
2025-03-23 03:33:38 - Loading data from processed_data/got_char_data.pkl
2025-03-23 03:33:38 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 03:33:38 - Created data loaders: 602 training batches, 17 validation batches
2025-03-23 03:33:38 - Using 2 workers for data loading with prefetch_factor=2
2025-03-23 03:33:40 - Model created with 85,910,784 parameters
2025-03-23 03:33:40 - Model size in memory: 327.72MB
2025-03-23 03:33:41 - Using OneCycleLR scheduler with warmup_pct=0.1
2025-03-23 03:33:41 - OneCycleLR: Initial LR=0.000012, Peak LR=0.000900, Final LR=0.00000000
2025-03-23 03:33:41 - Starting epoch 1/1
2025-03-23 03:33:41 - Starting training epoch 1 with gradient accumulation steps: 4
2025-03-23 03:33:41 - Total batches: 602, Batch size: 32
2025-03-23 03:33:41 - Initial CUDA memory allocated: 379.0 MB
2025-03-23 03:34:28 - Progress: 0.2% | Batch 1/602 | Loss: 4.6192 | Rate: 779.4 tokens/sec | ETA: N/A, CUDA Memory: 729.1MB/4095.7MB
2025-03-23 03:35:42 - Progress: 0.5% | Batch 3/602 | Loss: 4.6108 | Rate: 844.9 tokens/sec | ETA: N/A, CUDA Memory: 729.1MB/4095.7MB
2025-03-23 03:38:06 - Starting training with arguments: {'data_path': 'processed_data/got_char_data.pkl', 'batch_size': None, 'force_batch_size': False, 'epochs': 1, 'lr': 0.0003, 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'disable_amp': True, 'use_bfloat16': False, 'max_memory_usage': 0.85, 'gradient_accumulation_steps': 1, 'use_compile': False, 'num_workers': 2, 'use_onecycle': True, 'warmup_pct': 0.1, 'seed': 42, 'device': 'cuda', 'checkpoint_dir': 'checkpoints', 'sample_every_epoch': False}
2025-03-23 03:38:06 - Using device: cuda (NVIDIA GeForce GTX 1650 Ti), CUDA Version: 11.8, PyTorch: 2.6.0+cu118
2025-03-23 03:38:06 - Detected GTX 1650 Ti - Using enhanced optimizations for maximum speed
2025-03-23 03:38:06 - Setting max memory usage to 99.0% for maximum performance
2025-03-23 03:38:06 - Disabling Automatic Mixed Precision (AMP) for better speed on GTX 1650 Ti
2025-03-23 03:38:06 - Setting gradient_accumulation_steps to 2 for better throughput
2025-03-23 03:38:06 - Total CUDA memory: 4095.7MB
2025-03-23 03:38:06 - Usable CUDA memory (after context): 3295.7MB
2025-03-23 03:38:06 - Memory allocation target: 3262.7MB (99.0%)
2025-03-23 03:38:06 - Pre-allocating CUDA memory to force high utilization...
2025-03-23 03:38:07 - Successfully pre-allocated 2610.0MB of CUDA memory
2025-03-23 03:38:07 - Released pre-allocated memory, now available for training
2025-03-23 03:38:07 - Not using AMP - training with standard precision
2025-03-23 03:38:07 - Loading data from processed_data/got_char_data.pkl
2025-03-23 03:38:07 - Loading data from processed_data/got_char_data.pkl
2025-03-23 03:38:07 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 03:38:50 - Starting training with arguments: {'data_path': 'processed_data/got_char_data.pkl', 'batch_size': None, 'force_batch_size': False, 'epochs': 1, 'lr': 0.0003, 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'disable_amp': True, 'use_bfloat16': False, 'max_memory_usage': 0.85, 'gradient_accumulation_steps': 1, 'use_compile': False, 'num_workers': 2, 'use_onecycle': True, 'warmup_pct': 0.1, 'seed': 42, 'device': 'cuda', 'checkpoint_dir': 'checkpoints', 'sample_every_epoch': False}
2025-03-23 03:38:50 - Using device: cuda (NVIDIA GeForce GTX 1650 Ti), CUDA Version: 11.8, PyTorch: 2.6.0+cu118
2025-03-23 03:38:50 - Detected GTX 1650 Ti - Using enhanced optimizations for maximum speed
2025-03-23 03:38:50 - Setting high initial batch size: 48
2025-03-23 03:38:50 - Setting max memory usage to 99.0% for maximum performance
2025-03-23 03:38:50 - Disabling Automatic Mixed Precision (AMP) for better speed on GTX 1650 Ti
2025-03-23 03:38:50 - Setting gradient_accumulation_steps to 2 for better throughput
2025-03-23 03:38:50 - Total CUDA memory: 4095.7MB
2025-03-23 03:38:51 - Usable CUDA memory (after context): 3295.7MB
2025-03-23 03:38:51 - Memory allocation target: 3262.7MB (99.0%)
2025-03-23 03:38:51 - Pre-allocating CUDA memory to force high utilization...
2025-03-23 03:38:51 - Successfully pre-allocated 2610.0MB of CUDA memory
2025-03-23 03:38:51 - Released pre-allocated memory, now available for training
2025-03-23 03:38:51 - Not using AMP - training with standard precision
2025-03-23 03:38:51 - Loading data from processed_data/got_char_data.pkl
2025-03-23 03:38:51 - Loading data from processed_data/got_char_data.pkl
2025-03-23 03:38:51 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 03:38:51 - Created data loaders: 401 training batches, 17 validation batches
2025-03-23 03:38:51 - Using 2 workers for data loading with prefetch_factor=2
2025-03-23 03:38:52 - Model created with 85,910,784 parameters
2025-03-23 03:38:52 - Model size in memory: 327.72MB
2025-03-23 03:38:52 - Determining maximum possible batch size for your GPU...
2025-03-23 03:38:56 - Finding maximum possible batch size for available CUDA memory...
2025-03-23 03:38:56 - Unexpected error during batch size search: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-03-23 03:38:56 - Could not determine maximum batch size. Using fallback size of 8
2025-03-23 03:38:56 - Using maximum detected batch size: 8
2025-03-23 03:38:56 - Loading data from processed_data/got_char_data.pkl
2025-03-23 03:38:57 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 03:38:57 - Created data loaders: 2408 training batches, 67 validation batches
2025-03-23 03:38:57 - Using 2 workers for data loading with prefetch_factor=2
2025-03-23 03:38:59 - Using OneCycleLR scheduler with warmup_pct=0.1
2025-03-23 03:38:59 - OneCycleLR: Initial LR=0.000012, Peak LR=0.000900, Final LR=0.00000000
2025-03-23 03:38:59 - Starting epoch 1/1
2025-03-23 03:38:59 - Starting training epoch 1 with gradient accumulation steps: 2
2025-03-23 03:38:59 - Total batches: 2408, Batch size: 8
2025-03-23 03:38:59 - Initial CUDA memory allocated: 379.0 MB
2025-03-23 03:41:17 - Starting training with arguments: {'data_path': 'processed_data/got_char_data.pkl', 'batch_size': None, 'force_batch_size': False, 'epochs': 1, 'lr': 0.0003, 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'disable_amp': False, 'use_bfloat16': False, 'max_memory_usage': 0.85, 'gradient_accumulation_steps': 1, 'use_compile': False, 'num_workers': 2, 'use_onecycle': True, 'warmup_pct': 0.1, 'seed': 42, 'device': 'cuda', 'checkpoint_dir': 'checkpoints', 'sample_every_epoch': False}
2025-03-23 03:41:17 - Using device: cuda (NVIDIA GeForce GTX 1650 Ti), CUDA Version: 11.8, PyTorch: 2.6.0+cu118
2025-03-23 03:41:17 - Detected GTX 1650 Ti - Using enhanced optimizations for maximum speed
2025-03-23 03:41:17 - Setting optimized batch size for GTX 1650 Ti: 16
2025-03-23 03:41:17 - Setting max memory usage to 95.0% for maximum performance
2025-03-23 03:41:17 - Disabling Automatic Mixed Precision (AMP) for better speed on GTX 1650 Ti
2025-03-23 03:41:17 - Setting gradient_accumulation_steps to 4 for better throughput
2025-03-23 03:41:17 - Total CUDA memory: 4095.7MB
2025-03-23 03:41:17 - Usable CUDA memory (after context): 3295.7MB
2025-03-23 03:41:17 - Memory allocation target: 3130.9MB (95.0%)
2025-03-23 03:41:17 - Pre-allocating CUDA memory to force high utilization...
2025-03-23 03:41:17 - Successfully pre-allocated 2504.0MB of CUDA memory
2025-03-23 03:41:17 - Released pre-allocated memory, now available for training
2025-03-23 03:41:17 - Not using AMP - training with standard precision
2025-03-23 03:41:17 - Loading data from processed_data/got_char_data.pkl
2025-03-23 03:41:17 - Loading data from processed_data/got_char_data.pkl
2025-03-23 03:41:18 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 03:41:18 - Created data loaders: 1204 training batches, 34 validation batches
2025-03-23 03:41:18 - Using 2 workers for data loading with prefetch_factor=2
2025-03-23 03:41:20 - Model created with 85,910,784 parameters
2025-03-23 03:41:20 - Model size in memory: 327.72MB
2025-03-23 03:41:20 - Using pre-determined batch size of 16 for GTX 1650 Ti
2025-03-23 03:41:20 - This batch size with gradient_accumulation_steps=4 provides good performance
2025-03-23 03:41:22 - Using OneCycleLR scheduler with warmup_pct=0.1
2025-03-23 03:41:22 - OneCycleLR: Initial LR=0.000012, Peak LR=0.000900, Final LR=0.00000000
2025-03-23 03:41:22 - Starting epoch 1/1
2025-03-23 03:41:22 - Starting training epoch 1 with gradient accumulation steps: 4
2025-03-23 03:41:22 - Total batches: 1204, Batch size: 16
2025-03-23 03:41:22 - Initial CUDA memory allocated: 379.0 MB
2025-03-23 03:41:29 - Progress: 0.1% | Batch 1/1204 | Loss: 4.6084 | Rate: 4672.4 tokens/sec | ETA: N/A, CUDA Memory: 730.8MB/4095.7MB
2025-03-23 03:42:30 - Progress: 0.7% | Batch 8/1204 | Loss: 3.9699 | Rate: 2053.8 tokens/sec | ETA: N/A, CUDA Memory: 1397.3MB/4095.7MB
2025-03-23 03:42:50 - Progress: 0.8% | Batch 10/1204 | Loss: 3.7968 | Rate: 1938.3 tokens/sec | ETA: N/A, CUDA Memory: 1398.0MB/4095.7MB
2025-03-23 03:43:53 - Progress: 1.3% | Batch 16/1204 | Loss: 3.6154 | Rate: 1782.7 tokens/sec | ETA: 3.0h, CUDA Memory: 1398.0MB/4095.7MB
2025-03-23 03:44:55 - Progress: 1.8% | Batch 22/1204 | Loss: 3.4616 | Rate: 1725.8 tokens/sec | ETA: 3.1h, CUDA Memory: 1398.0MB/4095.7MB
2025-03-23 03:48:31 - Starting training with arguments: {'data_path': 'processed_data/got_char_data.pkl', 'batch_size': None, 'force_batch_size': False, 'epochs': 1, 'lr': 0.0003, 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'disable_amp': False, 'use_bfloat16': False, 'max_memory_usage': 0.85, 'gradient_accumulation_steps': 1, 'use_compile': False, 'num_workers': 2, 'use_onecycle': True, 'warmup_pct': 0.1, 'seed': 42, 'device': 'cuda', 'checkpoint_dir': 'checkpoints', 'sample_every_epoch': False}
2025-03-23 03:48:31 - Using device: cuda (NVIDIA GeForce GTX 1650 Ti), CUDA Version: 11.8, PyTorch: 2.6.0+cu118
2025-03-23 03:48:31 - Detected GTX 1650 Ti - Using enhanced optimizations for maximum VRAM utilization
2025-03-23 03:48:31 - Setting aggressive batch size for GTX 1650 Ti: 32
2025-03-23 03:48:31 - Setting max memory usage to 99.0% for maximum performance
2025-03-23 03:48:31 - Disabling Automatic Mixed Precision (AMP) for better speed on GTX 1650 Ti
2025-03-23 03:48:31 - Setting gradient_accumulation_steps to 2 for optimal throughput
2025-03-23 03:48:31 - Using higher weight decay (0.01) for better memory efficiency
2025-03-23 03:48:31 - Total CUDA memory: 4095.7MB
2025-03-23 03:48:31 - Usable CUDA memory (after context): 3295.7MB
2025-03-23 03:48:31 - Memory allocation target: 3262.7MB (99.0%)
2025-03-23 03:48:31 - Pre-allocating CUDA memory to force high utilization...
2025-03-23 03:48:31 - Successfully pre-allocated 2940.0MB of CUDA memory (2940.0MB total)
2025-03-23 03:48:31 - Released pre-allocated memory, now available for training
2025-03-23 03:48:31 - Memory after release: 0.0MB (reserved for PyTorch overhead)
2025-03-23 03:48:31 - Not using AMP - training with standard precision
2025-03-23 03:48:31 - Loading data from processed_data/got_char_data.pkl
2025-03-23 03:48:31 - Loading data from processed_data/got_char_data.pkl
2025-03-23 03:48:31 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 03:48:31 - Created data loaders: 602 training batches, 17 validation batches
2025-03-23 03:48:31 - Using 2 workers for data loading with prefetch_factor=2
2025-03-23 03:48:33 - Model created with 85,910,784 parameters
2025-03-23 03:48:33 - Model size in memory: 327.72MB
2025-03-23 03:48:33 - Gradient checkpointing not supported by this model
2025-03-23 03:48:33 - Using pre-determined batch size of 32 for GTX 1650 Ti
2025-03-23 03:48:33 - This batch size with gradient_accumulation_steps=2 provides good performance
2025-03-23 03:48:33 - Enabling optimizer state CPU offloading to save GPU memory
2025-03-23 03:48:35 - Using CPU offloading for optimizer states to save GPU memory
2025-03-23 03:48:35 - Using OneCycleLR scheduler with warmup_pct=0.1
2025-03-23 03:48:35 - OneCycleLR: Initial LR=0.000012, Peak LR=0.000900, Final LR=0.00000000
2025-03-23 03:48:35 - Starting epoch 1/1
2025-03-23 03:48:35 - Starting training epoch 1 with gradient accumulation steps: 2
2025-03-23 03:48:35 - Total batches: 602, Batch size: 32
2025-03-23 03:48:35 - Initial CUDA memory allocated: 379.0 MB
2025-03-23 03:48:35 - Total CUDA memory available: 4095.7 MB
2025-03-23 03:48:35 - CPU optimizer state offloading enabled - will transfer states as needed
2025-03-23 03:49:23 - Progress: 0.2% | Batch 1/602 | Loss: 4.6168 | Rate: 373.3 tokens/sec | ETA: N/A, CUDA Memory: 729.1MB/4095.7MB (17.8%)
2025-03-23 03:51:55 - Starting training with arguments: {'data_path': 'processed_data/got_char_data.pkl', 'batch_size': None, 'force_batch_size': False, 'epochs': 1, 'lr': 0.0003, 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'disable_amp': False, 'use_bfloat16': False, 'max_memory_usage': 0.85, 'gradient_accumulation_steps': 1, 'use_compile': False, 'num_workers': 2, 'use_onecycle': True, 'warmup_pct': 0.1, 'seed': 42, 'device': 'cuda', 'checkpoint_dir': 'checkpoints', 'sample_every_epoch': False}
2025-03-23 03:51:55 - Using device: cuda (NVIDIA GeForce GTX 1650 Ti), CUDA Version: 11.8, PyTorch: 2.6.0+cu118
2025-03-23 03:51:55 - Detected GTX 1650 Ti - Using enhanced optimizations for maximum VRAM utilization
2025-03-23 03:51:55 - Setting aggressive batch size for GTX 1650 Ti: 32
2025-03-23 03:51:55 - Setting max memory usage to 99.0% for maximum performance
2025-03-23 03:51:55 - Disabling Automatic Mixed Precision for optimal performance
2025-03-23 03:51:55 - Setting gradient_accumulation_steps to 2 for optimal throughput
2025-03-23 03:51:55 - Total CUDA memory: 4095.7MB
2025-03-23 03:51:55 - Usable CUDA memory (after context): 3295.7MB
2025-03-23 03:51:55 - Memory allocation target: 3262.7MB (99.0%)
2025-03-23 03:51:55 - Pre-allocating CUDA memory to force high utilization...
2025-03-23 03:51:55 - Successfully pre-allocated 3100.0MB of CUDA memory (3100.0MB total)
2025-03-23 03:51:55 - Allocated additional 154.6MB in second pass (total: 3254.6MB)
2025-03-23 03:51:55 - Released pre-allocated memory, now available for training
2025-03-23 03:51:55 - Memory after release: 154.6MB (reserved for PyTorch overhead)
2025-03-23 03:51:55 - Not using AMP - training with standard precision
2025-03-23 03:51:55 - Loading data from processed_data/got_char_data.pkl
2025-03-23 03:51:55 - Loading data from processed_data/got_char_data.pkl
2025-03-23 03:51:55 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 03:51:55 - Created data loaders: 602 training batches, 17 validation batches
2025-03-23 03:51:55 - Using 2 workers for data loading with prefetch_factor=2
2025-03-23 03:51:57 - Model created with 85,910,784 parameters
2025-03-23 03:51:57 - Model size in memory: 327.72MB
2025-03-23 03:51:57 - Gradient checkpointing not supported by this model
2025-03-23 03:51:57 - Using pre-determined batch size of 32 for GTX 1650 Ti
2025-03-23 03:51:57 - This batch size with gradient_accumulation_steps=2 provides good performance
2025-03-23 03:51:57 - Enabling optimizer state CPU offloading to save GPU memory
2025-03-23 03:51:59 - Using CPU offloading for optimizer states to save GPU memory
2025-03-23 03:51:59 - Using OneCycleLR scheduler with warmup_pct=0.1
2025-03-23 03:51:59 - OneCycleLR: Initial LR=0.000012, Peak LR=0.000900, Final LR=0.00000000
2025-03-23 03:51:59 - Starting epoch 1/1
2025-03-23 03:51:59 - Starting training epoch 1 with gradient accumulation steps: 2
2025-03-23 03:51:59 - Total batches: 602, Batch size: 32
2025-03-23 03:51:59 - Initial CUDA memory allocated: 531.7 MB
2025-03-23 03:51:59 - Total CUDA memory available: 4095.7 MB
2025-03-23 03:51:59 - CPU optimizer state offloading enabled - will transfer states as needed
2025-03-23 03:52:49 - Progress: 0.2% | Batch 1/602 | Loss: 4.6125 | Rate: 363.1 tokens/sec | ETA: N/A, CUDA Memory: 881.3MB/4095.7MB (21.5%)
2025-03-23 03:56:41 - Starting training with arguments: {'data_path': 'processed_data/got_char_data.pkl', 'batch_size': None, 'force_batch_size': False, 'epochs': 1, 'lr': 0.0003, 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'disable_amp': False, 'use_bfloat16': False, 'max_memory_usage': 0.85, 'gradient_accumulation_steps': 1, 'use_compile': False, 'num_workers': 2, 'use_onecycle': True, 'warmup_pct': 0.1, 'seed': 42, 'device': 'cuda', 'checkpoint_dir': 'checkpoints', 'sample_every_epoch': False}
2025-03-23 03:56:41 - Using device: cuda (NVIDIA GeForce GTX 1650 Ti), CUDA Version: 11.8, PyTorch: 2.6.0+cu118
2025-03-23 03:56:41 - Detected 4096MB total VRAM - optimizing for maximum utilization
2025-03-23 03:56:41 - Applied GTX 1650 Ti specific settings for MAXIMUM throughput and VRAM utilization
2025-03-23 03:56:41 - Detected GTX 1650 Ti - Using enhanced optimizations for maximum VRAM utilization
2025-03-23 03:56:41 - ⚡ Speed optimization: Using aggressive batch size of 64
2025-03-23 03:56:41 - ⚡ Speed optimization: Setting VRAM utilization target to 99.0%
2025-03-23 03:56:41 - ⚡ Speed optimization: Disabling AMP for optimal throughput
2025-03-23 03:56:41 - ⚡ Speed optimization: Setting gradient_accumulation_steps to 1
2025-03-23 03:56:41 - ⚡ Speed optimization: Maximizing parallelism for highest throughput
2025-03-23 03:56:41 - ⚡ Speed optimization: Enabled TensorCore operations (TF32) for faster computation
2025-03-23 03:56:41 - ⚡ Speed optimization: Disabled deterministic algorithms for faster training
2025-03-23 03:56:41 - ⚡ Speed optimization: Enabled cuDNN benchmarking for faster convolutions
2025-03-23 03:56:41 - Not using AMP - training with standard precision
2025-03-23 03:56:41 - Loading data from processed_data/got_char_data.pkl
2025-03-23 03:56:41 - Loading data from processed_data/got_char_data.pkl
2025-03-23 03:56:41 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 03:56:41 - Created data loaders: 301 training batches, 17 validation batches
2025-03-23 03:56:41 - Using 2 workers for data loading with prefetch_factor=2
2025-03-23 03:56:43 - Model created with 85,910,784 parameters
2025-03-23 03:56:43 - Model size in memory: 327.72MB
2025-03-23 03:56:43 - Gradient checkpointing not supported by this model
2025-03-23 03:56:43 - Enabling optimizer state CPU offloading to save GPU memory
2025-03-23 03:56:45 - Using CPU offloading for optimizer states to save GPU memory
2025-03-23 03:56:45 - Using OneCycleLR scheduler with warmup_pct=0.1
2025-03-23 03:56:45 - OneCycleLR: Initial LR=0.000012, Peak LR=0.000900, Final LR=0.00000000
2025-03-23 03:56:45 - Total CUDA memory: 4095.7MB
2025-03-23 03:56:45 - Usable CUDA memory (after context): 3295.7MB
2025-03-23 03:56:45 - Memory allocation target: 3262.7MB (99.0%)
2025-03-23 03:56:45 - ⚡ Speed optimization: Pre-allocating maximum CUDA memory to force high throughput...
2025-03-23 03:56:45 - ⚡ Memory allocation stage 1/4: Targeting 50% of allocation goal
2025-03-23 03:56:45 -   - Current VRAM: 779.0MB (19.0% of total VRAM)
2025-03-23 03:56:45 - ⚡ Memory allocation stage 2/4: Targeting 75% of allocation goal
2025-03-23 03:56:45 -   - Current VRAM: 1379.0MB (33.7% of total VRAM)
2025-03-23 03:56:45 - ⚡ Memory allocation stage 3/4: Targeting 90% of allocation goal
2025-03-23 03:56:45 -   - Current VRAM: 2099.0MB (51.2% of total VRAM)
2025-03-23 03:56:45 - ⚡ Memory allocation stage 4/4: Targeting 98% of allocation goal
2025-03-23 03:56:45 -   - Current VRAM: 2882.2MB (70.4% of total VRAM)
2025-03-23 03:56:45 - ⚡ Successfully pre-allocated 2503.3MB of CUDA memory (2882.2MB total, 70.4% of VRAM)
2025-03-23 03:56:45 - ⚡ Speed optimization: Attempting to allocate final 380.5MB of VRAM for maximum utilization
2025-03-23 03:56:45 - ⚡ Allocated additional 372.9MB in final pass (total: 3255.1MB, 79.5% of VRAM)
2025-03-23 03:56:45 - ⚡ Creating permanent VRAM buffer of 163.0MB to maintain high utilization
2025-03-23 03:56:45 - ⚡ Released pre-allocated memory, now available for training
2025-03-23 03:56:45 - ⚡ Memory after release: 617.5MB (15.1% of VRAM)
2025-03-23 03:56:45 - Starting epoch 1/1
2025-03-23 03:56:45 - Starting training epoch 1 with gradient accumulation steps: 1
2025-03-23 03:56:45 - Total batches: 301, Batch size: 64
2025-03-23 03:56:45 - Initial CUDA memory allocated: 617.5 MB
2025-03-23 03:56:45 - Total CUDA memory available: 4095.7 MB
2025-03-23 03:56:45 - CPU optimizer state offloading enabled - will transfer states as needed
2025-03-23 04:02:00 - Logging to file: C:\Users\nimbu\Desktop\Code\AI\ChatGoT\training_log.txt
2025-03-23 04:02:00 - === LOGGING STARTED AT 2025-03-23 04:02:00 ===
2025-03-23 04:02:00 - Logger initialized with console and file output
2025-03-23 04:02:00 - Starting training with arguments: {'data_path': 'processed_data/got_char_data.pkl', 'batch_size': None, 'force_batch_size': False, 'epochs': 1, 'lr': 0.0003, 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'disable_amp': False, 'use_bfloat16': False, 'max_memory_usage': 0.85, 'gradient_accumulation_steps': 1, 'use_compile': False, 'num_workers': 2, 'use_onecycle': True, 'warmup_pct': 0.1, 'seed': 42, 'device': 'cuda', 'checkpoint_dir': 'checkpoints', 'sample_every_epoch': False}
2025-03-23 04:02:00 - Using device: cuda (NVIDIA GeForce GTX 1650 Ti), CUDA Version: 11.8, PyTorch: 2.6.0+cu118
2025-03-23 04:02:00 - Detected 4096MB total VRAM - optimizing for maximum utilization
2025-03-23 04:02:00 - Applied GTX 1650 Ti specific settings for MAXIMUM throughput and VRAM utilization
2025-03-23 04:02:00 - Detected GTX 1650 Ti - Using enhanced optimizations for maximum VRAM utilization
2025-03-23 04:07:01 - Logging to file: C:\Users\nimbu\Desktop\Code\AI\ChatGoT\training_log.txt
2025-03-23 04:07:01 - === LOGGING STARTED AT 2025-03-23 04:07:01 ===
2025-03-23 04:07:01 - Logger initialized with console and file output
2025-03-23 04:07:01 - Starting training with arguments: {'data_path': 'processed_data/got_char_data.pkl', 'batch_size': None, 'force_batch_size': False, 'epochs': 1, 'lr': 0.0003, 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'disable_amp': False, 'use_bfloat16': False, 'max_memory_usage': 0.85, 'gradient_accumulation_steps': 1, 'use_compile': False, 'num_workers': 2, 'use_onecycle': True, 'warmup_pct': 0.1, 'seed': 42, 'device': 'cuda', 'checkpoint_dir': 'checkpoints', 'sample_every_epoch': False}
2025-03-23 04:07:01 - Using device: cuda (NVIDIA GeForce GTX 1650 Ti), CUDA Version: 11.8, PyTorch: 2.6.0+cu118
2025-03-23 04:07:01 - Detected 4096MB total VRAM - optimizing for maximum utilization
2025-03-23 04:07:01 - Applied GTX 1650 Ti specific settings for MAXIMUM throughput and VRAM utilization
2025-03-23 04:07:01 - Detected GTX 1650 Ti - Using enhanced optimizations for maximum VRAM utilization
2025-03-23 04:07:01 - Detected GTX 1650 Ti - Using enhanced optimizations for maximum VRAM utilization
2025-03-23 04:07:01 - [SPEED] Using aggressive batch size of 64
2025-03-23 04:07:01 - [SPEED] Setting VRAM utilization target to 99.0%
2025-03-23 04:07:01 - [SPEED] Disabling AMP for optimal throughput
2025-03-23 04:07:01 - [SPEED] Setting gradient_accumulation_steps to 1
2025-03-23 04:07:01 - [SPEED] Maximizing parallelism for highest throughput
2025-03-23 04:07:01 - [SPEED] Enabled TensorCore operations (TF32) for faster computation
2025-03-23 04:07:01 - [SPEED] Disabled deterministic algorithms for faster training
2025-03-23 04:07:01 - [SPEED] Enabled cuDNN benchmarking for faster convolutions
2025-03-23 04:07:01 - Not using AMP - training with standard precision
2025-03-23 04:07:01 - Loading data from processed_data/got_char_data.pkl
2025-03-23 04:07:01 - Loading data from processed_data/got_char_data.pkl
2025-03-23 04:07:01 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 04:07:01 - Created data loaders: 301 training batches, 17 validation batches
2025-03-23 04:07:01 - Using 2 workers for data loading with prefetch_factor=2
2025-03-23 04:07:03 - Model created with 85,910,784 parameters
2025-03-23 04:07:03 - Model size in memory: 327.72MB
2025-03-23 04:07:03 - Gradient checkpointing not supported by this model
2025-03-23 04:07:03 - Enabling optimizer state CPU offloading to save GPU memory
2025-03-23 04:07:05 - Using CPU offloading for optimizer states to save GPU memory
2025-03-23 04:07:05 - Using OneCycleLR scheduler with warmup_pct=0.1
2025-03-23 04:07:05 - OneCycleLR: Initial LR=0.000012, Peak LR=0.000900, Final LR=0.00000000
2025-03-23 04:07:05 - Total CUDA memory: 4095.7MB
2025-03-23 04:07:05 - Usable CUDA memory (after context): 3295.7MB
2025-03-23 04:07:05 - Memory allocation target: 3262.7MB (99.0%)
2025-03-23 04:07:05 - [SPEED] Pre-allocating maximum CUDA memory to force high throughput...
2025-03-23 04:07:05 - [MEMORY] Allocation stage 1/4: Targeting 50% of allocation goal
2025-03-23 04:07:06 -   - Current VRAM: 779.0MB (19.0% of total VRAM)
2025-03-23 04:07:06 - [MEMORY] Allocation stage 2/4: Targeting 75% of allocation goal
2025-03-23 04:07:06 -   - Current VRAM: 1379.0MB (33.7% of total VRAM)
2025-03-23 04:07:06 - [MEMORY] Allocation stage 3/4: Targeting 90% of allocation goal
2025-03-23 04:07:06 -   - Current VRAM: 2099.0MB (51.2% of total VRAM)
2025-03-23 04:07:06 - [MEMORY] Allocation stage 4/4: Targeting 98% of allocation goal
2025-03-23 04:07:06 -   - Current VRAM: 2882.2MB (70.4% of total VRAM)
2025-03-23 04:07:06 - [MEMORY] Successfully pre-allocated 2503.3MB of CUDA memory (2882.2MB total, 70.4% of VRAM)
2025-03-23 04:07:06 - [SPEED] Attempting to allocate final 380.5MB of VRAM for maximum utilization
2025-03-23 04:07:06 - [MEMORY] Allocated additional 372.9MB in final pass (total: 3255.1MB, 79.5% of VRAM)
2025-03-23 04:07:06 - [MEMORY] Creating permanent VRAM buffer of 163.0MB to maintain high utilization
2025-03-23 04:07:06 - [MEMORY] Released pre-allocated memory, now available for training
2025-03-23 04:07:06 - [MEMORY] Memory after release: 617.5MB (15.1% of VRAM)
2025-03-23 04:07:06 - Starting epoch 1/1
2025-03-23 04:07:06 - [04:07:06] Starting training epoch 1 with gradient accumulation steps: 1
2025-03-23 04:07:06 - [04:07:06] Total batches: 301, Batch size: 64
2025-03-23 04:07:06 - [04:07:06] Will log progress every 60 seconds (1 minute)
2025-03-23 04:10:21 - Logging to file: C:\Users\nimbu\Desktop\Code\AI\ChatGoT\training_log.txt
2025-03-23 04:10:21 - === LOGGING STARTED AT 2025-03-23 04:10:21 ===
2025-03-23 04:10:21 - Logger initialized with console and file output
2025-03-23 04:10:21 - Starting training with arguments: {'data_path': 'processed_data/got_char_data.pkl', 'batch_size': None, 'force_batch_size': False, 'epochs': 1, 'lr': 0.0003, 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'disable_amp': False, 'use_bfloat16': False, 'max_memory_usage': 0.85, 'gradient_accumulation_steps': 1, 'use_compile': False, 'num_workers': 2, 'use_onecycle': True, 'warmup_pct': 0.1, 'seed': 42, 'device': 'cuda', 'checkpoint_dir': 'checkpoints', 'sample_every_epoch': False}
2025-03-23 04:10:21 - Using device: cuda (NVIDIA GeForce GTX 1650 Ti), CUDA Version: 11.8, PyTorch: 2.6.0+cu118
2025-03-23 04:10:21 - Detected 4096MB total VRAM - optimizing for maximum utilization
2025-03-23 04:10:21 - Applied GTX 1650 Ti specific settings for MAXIMUM throughput and VRAM utilization
2025-03-23 04:10:21 - Detected GTX 1650 Ti - Using enhanced optimizations for maximum VRAM utilization
2025-03-23 04:10:21 - Detected GTX 1650 Ti - Using enhanced optimizations for maximum VRAM utilization
2025-03-23 04:10:21 - [SPEED] Using aggressive batch size of 64
2025-03-23 04:10:21 - [SPEED] Setting VRAM utilization target to 99.0%
2025-03-23 04:10:21 - [SPEED] Disabling AMP for optimal throughput
2025-03-23 04:10:21 - [SPEED] Setting gradient_accumulation_steps to 1
2025-03-23 04:10:21 - [SPEED] Maximizing parallelism for highest throughput
2025-03-23 04:10:21 - [SPEED] Enabled TensorCore operations (TF32) for faster computation
2025-03-23 04:10:21 - [SPEED] Disabled deterministic algorithms for faster training
2025-03-23 04:10:21 - [SPEED] Enabled cuDNN benchmarking for faster convolutions
2025-03-23 04:10:21 - Not using AMP - training with standard precision
2025-03-23 04:10:21 - Loading data from processed_data/got_char_data.pkl
2025-03-23 04:10:21 - Loading data from processed_data/got_char_data.pkl
2025-03-23 04:10:21 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 04:10:21 - Created data loaders: 301 training batches, 17 validation batches
2025-03-23 04:10:21 - Using 2 workers for data loading with prefetch_factor=2
2025-03-23 04:10:23 - Model created with 85,910,784 parameters
2025-03-23 04:10:23 - Model size in memory: 327.72MB
2025-03-23 04:10:23 - Gradient checkpointing not supported by this model
2025-03-23 04:10:23 - Enabling optimizer state CPU offloading to save GPU memory
2025-03-23 04:10:26 - Using CPU offloading for optimizer states to save GPU memory
2025-03-23 04:10:26 - Using OneCycleLR scheduler with warmup_pct=0.1
2025-03-23 04:10:26 - OneCycleLR: Initial LR=0.000012, Peak LR=0.000900, Final LR=0.00000000
2025-03-23 04:10:26 - Total CUDA memory: 4095.7MB
2025-03-23 04:10:26 - Usable CUDA memory (after context): 3295.7MB
2025-03-23 04:10:26 - Memory allocation target: 3262.7MB (99.0%)
2025-03-23 04:10:26 - [SPEED] Pre-allocating maximum CUDA memory to force high throughput...
2025-03-23 04:10:26 - [MEMORY] Allocation stage 1/4: Targeting 50% of allocation goal
2025-03-23 04:10:26 -   - Current VRAM: 779.0MB (19.0% of total VRAM)
2025-03-23 04:10:26 - [MEMORY] Allocation stage 2/4: Targeting 75% of allocation goal
2025-03-23 04:10:26 -   - Current VRAM: 1379.0MB (33.7% of total VRAM)
2025-03-23 04:10:26 - [MEMORY] Allocation stage 3/4: Targeting 90% of allocation goal
2025-03-23 04:10:26 -   - Current VRAM: 2099.0MB (51.2% of total VRAM)
2025-03-23 04:10:26 - [MEMORY] Allocation stage 4/4: Targeting 98% of allocation goal
2025-03-23 04:10:26 -   - Current VRAM: 2882.2MB (70.4% of total VRAM)
2025-03-23 04:10:26 - [MEMORY] Successfully pre-allocated 2503.3MB of CUDA memory (2882.2MB total, 70.4% of VRAM)
2025-03-23 04:10:26 - [SPEED] Attempting to allocate final 380.5MB of VRAM for maximum utilization
2025-03-23 04:10:26 - [MEMORY] Allocated additional 372.9MB in final pass (total: 3255.1MB, 79.5% of VRAM)
2025-03-23 04:10:26 - [MEMORY] Creating permanent VRAM buffer of 163.0MB to maintain high utilization
2025-03-23 04:10:26 - [MEMORY] Released pre-allocated memory, now available for training
2025-03-23 04:10:26 - [MEMORY] Memory after release: 617.5MB (15.1% of VRAM)
2025-03-23 04:10:26 - Starting epoch 1/1
2025-03-23 04:10:26 - [04:10:26] Starting training epoch 1 with gradient accumulation steps: 1
2025-03-23 04:10:26 - [04:10:26] Total batches: 301, Batch size: 64
2025-03-23 04:10:26 - [04:10:26] Will log progress every 60 seconds (1 minute)
2025-03-23 04:12:56 - [04:12:56] Progress: 0.3% | Batch 1/301 | Loss: 4.6140 | Rate: 109.4 tokens/sec | ETA: 12.5h | VRAM: 1308.5MB/4095.7MB (31.9%)
2025-03-23 04:12:56 - [04:12:56] Minute 2 update | Progress: 0.3% | Batch 1/301 | ETA: 6.2h | Memory: 1308.5MB (31.9%)
