2025-03-23 12:46:35 - Logging to file: C:\Users\nimbu\Desktop\Code\AI\ChatGoT\runs\run_20250323-124635\training.log
2025-03-23 12:46:35 - === LOGGING STARTED AT 2025-03-23 12:46:35 ===
2025-03-23 12:46:35 - Logger initialized with console and file output
2025-03-23 12:46:35 - Random seed set to 42 for reproducibility
2025-03-23 12:46:35 - Arguments saved to runs\run_20250323-124635\args.json
2025-03-23 12:46:36 - Using CPU (CUDA not available)
2025-03-23 12:46:36 - ================================================================================
2025-03-23 12:46:36 -                                   LOADING DATA                                  
2025-03-23 12:46:36 - ================================================================================
2025-03-23 12:46:36 - Loading data from processed_data/got_char_data.pkl
2025-03-23 12:46:36 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 12:46:36 - Vocabulary size: 89 characters
2025-03-23 12:46:36 - Created data loaders: 1204 training batches, 34 validation batches
2025-03-23 12:46:36 - Using 4 workers for data loading with prefetch_factor=2
2025-03-23 12:46:36 - ================================================================================
2025-03-23 12:46:36 -                                  CREATING MODEL                                 
2025-03-23 12:46:36 - ================================================================================
2025-03-23 12:46:37 - Model initialized with 768 dimensions, 85,192,793 parameters
2025-03-23 12:46:37 - Trainable parameters: 85,192,793
2025-03-23 12:46:37 - Estimated model size: 327.98MB
2025-03-23 12:46:37 - Created transformer model with 85,192,793 parameters
2025-03-23 12:46:37 - Model config: d_model=768, n_head=12, d_hid=3072, n_layers=12
2025-03-23 12:46:37 - Using standard GPT-2 Small architecture
2025-03-23 12:46:37 - Using memory-efficient attention implementation
2025-03-23 12:46:37 - Model moved to cpu
2025-03-23 12:46:39 - Using LambdaLR scheduler with warmup_steps=4000
2025-03-23 12:46:39 - Automatic Mixed Precision disabled (not supported on CPU)
2025-03-23 12:46:39 - ================================================================================
2025-03-23 12:46:39 -                              TRAINING CONFIGURATION                             
2025-03-23 12:46:39 - ================================================================================
2025-03-23 12:46:39 - Device: cpu
2025-03-23 12:46:39 - Model Architecture: Transformer (GPT-2 Style)
2025-03-23 12:46:39 - Total Parameters: 85,192,793
2025-03-23 12:46:39 - Trainable Parameters: 85,192,793
2025-03-23 12:46:39 - Estimated Model Size: 324.98 MB (FP32)
2025-03-23 12:46:39 - Estimated Training Memory: 2274.89 MB
2025-03-23 12:46:39 - Batch Size: 16
2025-03-23 12:46:39 - Gradient Accumulation Steps: 1
2025-03-23 12:46:39 - Effective Batch Size: 16
2025-03-23 12:46:39 - Learning Rate: 0.0005
2025-03-23 12:46:39 - Number of Epochs: 1
2025-03-23 12:46:39 - Using AMP: False
2025-03-23 12:46:39 - Max Gradient Norm: 1.0
2025-03-23 12:46:39 - Optimizer: AdamW
2025-03-23 12:46:39 - LR Scheduler: Linear warmup (4000 steps) then constant
2025-03-23 12:46:39 - Data Path: processed_data/got_char_data.pkl
2025-03-23 12:46:39 - Output Directory: runs\run_20250323-124635
2025-03-23 12:46:39 - Checkpoint Directory: checkpoints
2025-03-23 12:46:39 - Log File: runs\run_20250323-124635\training.log
2025-03-23 12:46:39 - Random Seed: 42
2025-03-23 12:46:39 - ================================================================================
2025-03-23 12:46:39 -                                STARTING TRAINING                                
2025-03-23 12:46:39 - ================================================================================
2025-03-23 12:46:39 - Starting training epoch 0 with gradient accumulation steps: 1
2025-03-23 12:46:39 - Total batches: 1204, Batch size: 16
2025-03-23 12:47:03 - Progress: 0.1% | Batch 1/1204 | Loss: 5.4167 | Rate: 168.6 tokens/sec | ETA: 8.1h | 
2025-03-23 12:47:46 - Minute 1 update | Progress: 0.2% | Batch 3/1204 | ETA: 5.6h | 
2025-03-23 12:48:06 - Progress: 0.3% | Batch 4/1204 | Loss: 5.3288 | Rate: 187.6 tokens/sec | ETA: 7.3h | 
2025-03-23 12:48:48 - Minute 2 update | Progress: 0.5% | Batch 6/1204 | ETA: 6.2h | 
2025-03-23 12:49:08 - Progress: 0.6% | Batch 7/1204 | Loss: 5.2633 | Rate: 192.2 tokens/sec | ETA: 7.1h | 
2025-03-23 12:50:09 - Progress: 0.8% | Batch 10/1204 | Loss: 5.0934 | Rate: 195.3 tokens/sec | ETA: 7.0h | 
2025-03-23 12:50:09 - Minute 3 update | Progress: 0.8% | Batch 10/1204 | ETA: 6.3h | 
2025-03-23 12:51:09 - Progress: 1.1% | Batch 13/1204 | Loss: 4.8809 | Rate: 197.4 tokens/sec | ETA: 6.9h | 
2025-03-23 12:51:09 - Minute 4 update | Progress: 1.1% | Batch 13/1204 | ETA: 6.4h | 
2025-03-23 12:52:10 - Progress: 1.3% | Batch 16/1204 | Loss: 4.6232 | Rate: 198.1 tokens/sec | ETA: 6.8h | 
2025-03-23 12:52:10 - Minute 5 update | Progress: 1.3% | Batch 16/1204 | ETA: 6.4h | 
2025-03-23 12:53:11 - Progress: 1.6% | Batch 19/1204 | Loss: 4.3900 | Rate: 198.6 tokens/sec | ETA: 6.8h | 
2025-03-23 12:53:11 - Minute 6 update | Progress: 1.6% | Batch 19/1204 | ETA: 6.4h | 
