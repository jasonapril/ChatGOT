2025-03-23 15:45:21 - Logging to file: C:\Users\nimbu\Desktop\Code\AI\ChatGoT\runs\run_20250323-154521\training.log
2025-03-23 15:45:21 - === LOGGING STARTED AT 2025-03-23 15:45:21 ===
2025-03-23 15:45:21 - Logger initialized with console and file output
2025-03-23 15:45:22 - Random seed set to 42 for reproducibility
2025-03-23 15:45:22 - Arguments saved to runs\run_20250323-154521\args.json
2025-03-23 15:45:22 - Using CUDA with 1 GPU(s)
2025-03-23 15:45:22 - PyTorch version: 2.6.0+cu118
2025-03-23 15:45:22 - GPU Memory Info:
GPU 0: NVIDIA GeForce GTX 1650 Ti, 4.00 GB total, 3.23 GB free
2025-03-23 15:45:22 - Enabled TF32 precision for faster training
2025-03-23 15:45:22 - Enabled cuDNN benchmark mode for faster training
2025-03-23 15:45:22 - Detected 4096MB total VRAM - optimizing for maximum utilization
2025-03-23 15:45:22 - Applied ULTRA-AGGRESSIVE GTX 1650 Ti settings for MAXIMUM throughput and VRAM utilization
2025-03-23 15:45:22 - [MEMORY] Setting VRAM utilization target to 99.0%
2025-03-23 15:45:22 - ================================================================================
2025-03-23 15:45:22 -                                   LOADING DATA                                  
2025-03-23 15:45:22 - ================================================================================
2025-03-23 15:45:22 - Loading data from processed_data/got_char_data.pkl
2025-03-23 15:45:22 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 15:45:22 - Vocabulary size: 89 characters
2025-03-23 15:45:22 - Created data loaders: 2408 training batches, 67 validation batches
2025-03-23 15:45:22 - Using 4 workers for data loading with prefetch_factor=2
2025-03-23 15:45:22 - ================================================================================
2025-03-23 15:45:22 -                                  CREATING MODEL                                 
2025-03-23 15:45:22 - ================================================================================
2025-03-23 15:45:22 - Model initialized with 768 dimensions, 85,192,793 parameters
2025-03-23 15:45:22 - Trainable parameters: 85,192,793
2025-03-23 15:45:22 - Estimated model size: 327.98MB
2025-03-23 15:45:23 - Created transformer model with 85,192,793 parameters
2025-03-23 15:45:23 - Model config: d_model=768, n_head=12, d_hid=3072, n_layers=12
2025-03-23 15:45:23 - Using standard GPT-2 Small architecture
2025-03-23 15:45:23 - Using memory-efficient attention implementation
2025-03-23 15:45:23 - Model moved to cuda
2025-03-23 15:45:24 - Using LambdaLR scheduler with warmup_steps=4000
2025-03-23 15:45:24 - ================================================================================
2025-03-23 15:45:24 -                              TRAINING CONFIGURATION                             
2025-03-23 15:45:24 - ================================================================================
2025-03-23 15:45:24 - Device: cuda
2025-03-23 15:45:24 - GPU: NVIDIA GeForce GTX 1650 Ti
2025-03-23 15:45:24 - CUDA: 11.8
2025-03-23 15:45:24 - PyTorch CUDA: 11.8
2025-03-23 15:45:24 - GPU Memory: 4.00 GB
2025-03-23 15:45:24 - [MEMORY] Target GPU memory usage: 3.96 GB (99.0%)
2025-03-23 15:45:24 - Model Architecture: Transformer (GPT-2 Style)
2025-03-23 15:45:24 - Total Parameters: 85,192,793
2025-03-23 15:45:24 - Trainable Parameters: 85,192,793
2025-03-23 15:45:24 - Estimated Model Size: 324.98 MB (FP32)
2025-03-23 15:45:24 - Estimated Training Memory: 2274.89 MB
2025-03-23 15:45:24 - Batch Size: 8
2025-03-23 15:45:24 - Gradient Accumulation Steps: 4
2025-03-23 15:45:24 - Effective Batch Size: 32
2025-03-23 15:45:24 - Learning Rate: 5e-05
2025-03-23 15:45:24 - Number of Epochs: 3
2025-03-23 15:45:24 - Using AMP: False
2025-03-23 15:45:24 - Max Gradient Norm: 1.0
2025-03-23 15:45:24 - Optimizer: AdamW
2025-03-23 15:45:24 - LR Scheduler: Linear warmup (4000 steps) then constant
2025-03-23 15:45:24 - Data Path: processed_data/got_char_data.pkl
2025-03-23 15:45:24 - Output Directory: runs\run_20250323-154521
2025-03-23 15:45:24 - Checkpoint Directory: checkpoints
2025-03-23 15:45:24 - Log File: runs\run_20250323-154521\training.log
2025-03-23 15:45:24 - Random Seed: 42
2025-03-23 15:45:24 - [SPEED] Enabled cuDNN benchmarking for faster training
2025-03-23 15:45:24 - ================================================================================
2025-03-23 15:45:24 -                                STARTING TRAINING                                
2025-03-23 15:45:24 - ================================================================================
2025-03-23 15:45:26 - Model successfully compiled with torch.compile (mode=reduce-overhead) in 1.20s
2025-03-23 15:45:26 - Starting training epoch 0 with gradient accumulation steps: 4
2025-03-23 15:45:26 - Total batches: 2408, Batch size: 8
2025-03-23 15:45:26 - [MEMORY] Current VRAM utilization is low (8.0%). Allocating cache tensors to increase GPU utilization.
2025-03-23 15:45:26 - [MEMORY] Allocating additional 2743.8MB to increase GPU utilization
2025-03-23 15:45:26 - [MEMORY] Increased VRAM utilization to 75.0%
2025-03-23 15:45:44 - Error occurred during training: backend='inductor' raised:
RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True
Traceback (most recent call last):
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\src\train_optimized.py", line 653, in <module>
    main()
    ~~~~^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\src\train_optimized.py", line 521, in main
    train_loss, tokens_per_sec = train_epoch(
                                 ~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<11 lines>...
        scaler=scaler
        ^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\src\trainer.py", line 222, in train_epoch
    outputs = model(inputs)
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_dynamo\eval_frame.py", line 574, in _fn
    return fn(*args, **kwargs)
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_dynamo\convert_frame.py", line 1380, in __call__
    return self._torchdynamo_orig_callable(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        frame, cache_entry, self.hooks, frame_state, skip=1
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_dynamo\convert_frame.py", line 1164, in __call__
    result = self._inner_convert(
        frame, cache_entry, hooks, frame_state, skip=skip + 1
    )
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_dynamo\convert_frame.py", line 547, in __call__
    return _compile(
        frame.f_code,
    ...<14 lines>...
        skip=skip + 1,
    )
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_dynamo\convert_frame.py", line 986, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_dynamo\convert_frame.py", line 715, in compile_inner
    return _compile_inner(code, one_graph, hooks, transform)
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_utils_internal.py", line 95, in wrapper_function
    return function(*args, **kwargs)
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_dynamo\convert_frame.py", line 750, in _compile_inner
    out_code = transform_code_object(code, transform)
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_dynamo\bytecode_transformation.py", line 1361, in transform_code_object
    transformations(instructions, code_options)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_dynamo\convert_frame.py", line 231, in _fn
    return fn(*args, **kwargs)
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_dynamo\convert_frame.py", line 662, in transform
    tracer.run()
    ~~~~~~~~~~^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_dynamo\symbolic_convert.py", line 2868, in run
    super().run()
    ~~~~~~~~~~~^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_dynamo\symbolic_convert.py", line 1052, in run
    while self.step():
          ~~~~~~~~~^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_dynamo\symbolic_convert.py", line 962, in step
    self.dispatch_table[inst.opcode](self, inst)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_dynamo\symbolic_convert.py", line 3048, in RETURN_VALUE
    self._return(inst)
    ~~~~~~~~~~~~^^^^^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_dynamo\symbolic_convert.py", line 3033, in _return
    self.output.compile_subgraph(
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        self,
        ^^^^^
    ...<2 lines>...
        ),
        ^^
    )
    ^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_dynamo\output_graph.py", line 1101, in compile_subgraph
    self.compile_and_call_fx_graph(
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        tx, list(reversed(stack_values)), root, output_replacements
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_dynamo\output_graph.py", line 1382, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_dynamo\output_graph.py", line 1432, in call_user_compiler
    return self._call_user_compiler(gm)
           ~~~~~~~~~~~~~~~~~~~~~~~~^^^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_dynamo\output_graph.py", line 1483, in _call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
        e.__traceback__
    ) from None
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_dynamo\output_graph.py", line 1462, in _call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_dynamo\repro\after_dynamo.py", line 130, in __call__
    compiled_gm = compiler_fn(gm, example_inputs)
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\__init__.py", line 2340, in __call__
    return compile_fx(model_, inputs_, config_patches=self.config)
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_inductor\compile_fx.py", line 1552, in compile_fx
    return compile_fx(
        model_,
    ...<3 lines>...
        decompositions=decompositions,
    )
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_inductor\compile_fx.py", line 1863, in compile_fx
    return aot_autograd(
           ~~~~~~~~~~~~~
    ...<6 lines>...
        cudagraphs=cudagraphs,
        ~~~~~~~~~~~~~~~~~~~~~~
    )(model_, example_inputs_)
    ~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_dynamo\backends\common.py", line 83, in __call__
    cg = aot_module_simplified(gm, example_inputs, **self.kwargs)
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_functorch\aot_autograd.py", line 1155, in aot_module_simplified
    compiled_fn = dispatch_and_compile()
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_functorch\aot_autograd.py", line 1131, in dispatch_and_compile
    compiled_fn, _ = create_aot_dispatcher_function(
                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        functional_call,
        ^^^^^^^^^^^^^^^^
    ...<3 lines>...
        shape_env,
        ^^^^^^^^^^
    )
    ^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_functorch\aot_autograd.py", line 580, in create_aot_dispatcher_function
    return _create_aot_dispatcher_function(
        flat_fn, fake_flat_args, aot_config, fake_mode, shape_env
    )
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_functorch\aot_autograd.py", line 830, in _create_aot_dispatcher_function
    compiled_fn, fw_metadata = compiler_fn(
                               ~~~~~~~~~~~^
        flat_fn,
        ^^^^^^^^
    ...<2 lines>...
        fw_metadata=fw_metadata,
        ^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_functorch\_aot_autograd\jit_compile_runtime_wrappers.py", line 678, in aot_dispatch_autograd
    compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_functorch\aot_autograd.py", line 489, in __call__
    return self.compiler_fn(gm, example_inputs)
           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_inductor\compile_fx.py", line 1741, in fw_compiler_base
    return inner_compile(
        gm,
    ...<5 lines>...
        boxed_forward_device_index=forward_device,
    )
  File "C:\Users\nimbu\AppData\Local\Programs\Python\Python313\Lib\contextlib.py", line 85, in inner
    return func(*args, **kwds)
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_inductor\compile_fx.py", line 569, in compile_fx_inner
    return wrap_compiler_debug(_compile_fx_inner, compiler_name="inductor")(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        gm,
        ^^^
        example_inputs,
        ^^^^^^^^^^^^^^^
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_dynamo\repro\after_aot.py", line 102, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_inductor\compile_fx.py", line 685, in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
        gm, example_inputs, inputs_to_check, **graph_kwargs
    )
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_inductor\compile_fx.py", line 1129, in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_inductor\compile_fx.py", line 1044, in codegen_and_compile
    compiled_fn = graph.compile_to_module().call
                  ~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_inductor\graph.py", line 2027, in compile_to_module
    return self._compile_to_module()
           ~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_inductor\graph.py", line 2033, in _compile_to_module
    self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
                                                             ~~~~~~~~~~~~^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_inductor\graph.py", line 1964, in codegen
    self.scheduler = Scheduler(self.operations)
                     ~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_inductor\scheduler.py", line 1798, in __init__
    self._init(nodes)
    ~~~~~~~~~~^^^^^^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_inductor\scheduler.py", line 1816, in _init
    self.nodes = [self.create_scheduler_node(n) for n in nodes]
                  ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_inductor\scheduler.py", line 1947, in create_scheduler_node
    return SchedulerNode(self, node)
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_inductor\scheduler.py", line 893, in __init__
    self._compute_attrs()
    ~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_inductor\scheduler.py", line 907, in _compute_attrs
    group_fn = self.scheduler.get_backend(device).group_fn
               ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_inductor\scheduler.py", line 3441, in get_backend
    self.backends[device] = self.create_backend(device)
                            ~~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "C:\Users\nimbu\Desktop\Code\AI\ChatGoT\.venv\Lib\site-packages\torch\_inductor\scheduler.py", line 3432, in create_backend
    raise RuntimeError(
        "Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton"  # noqa: B950
    )
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

