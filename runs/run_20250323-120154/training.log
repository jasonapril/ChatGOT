2025-03-23 12:01:54 - Logging to file: C:\Users\nimbu\Desktop\Code\AI\ChatGoT\runs\run_20250323-120154\training.log
2025-03-23 12:01:54 - === LOGGING STARTED AT 2025-03-23 12:01:54 ===
2025-03-23 12:01:54 - Logger initialized with console and file output
2025-03-23 12:01:54 - Random seed set to 42 for reproducibility
2025-03-23 12:01:54 - Arguments saved to runs\run_20250323-120154\args.json
2025-03-23 12:01:54 - Using CPU (CUDA not available)
2025-03-23 12:01:54 - ================================================================================
2025-03-23 12:01:54 -                                   LOADING DATA                                  
2025-03-23 12:01:54 - ================================================================================
2025-03-23 12:01:54 - Loading data from processed_data/got_char_data.pkl
2025-03-23 12:01:54 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 12:01:54 - Vocabulary size: 89 characters
2025-03-23 12:01:54 - Created data loaders: 301 training batches, 17 validation batches
2025-03-23 12:01:54 - Using 4 workers for data loading with prefetch_factor=2
2025-03-23 12:01:54 - ================================================================================
2025-03-23 12:01:54 -                                  CREATING MODEL                                 
2025-03-23 12:01:54 - ================================================================================
2025-03-23 12:01:54 - Creating transformer model with:
2025-03-23 12:01:54 - - vocab_size: 89
2025-03-23 12:01:54 - - d_model: 256
2025-03-23 12:01:54 - - n_head: 8
2025-03-23 12:01:54 - - n_layers: 6
2025-03-23 12:01:54 - - d_hid: 1024
2025-03-23 12:01:54 - - dropout: 0.1
2025-03-23 12:01:54 - - max_seq_length: 256
2025-03-23 12:01:54 - Model initialized with 256 dimensions, 4,784,729 parameters
2025-03-23 12:01:54 - Trainable parameters: 4,784,729
2025-03-23 12:01:54 - Estimated model size: 18.50MB
2025-03-23 12:01:54 - Model moved to device: cpu
2025-03-23 12:01:56 - Using LambdaLR scheduler with warmup_steps=4000
2025-03-23 12:01:56 - ================================================================================
2025-03-23 12:01:56 -                              TRAINING CONFIGURATION                             
2025-03-23 12:01:56 - ================================================================================
2025-03-23 12:01:57 - Device: cpu
2025-03-23 12:01:57 - Model Architecture: Transformer
2025-03-23 12:01:57 - Embedding Dimension: 256
2025-03-23 12:01:57 - Number of Attention Heads: 8
2025-03-23 12:01:57 - Number of Layers: 6
2025-03-23 12:01:57 - Hidden Dimension: 1024
2025-03-23 12:01:57 - Dropout: 0.1
2025-03-23 12:01:57 - Context Length: 256
2025-03-23 12:01:57 - Vocabulary Size: 89
2025-03-23 12:01:57 - Total Parameters: 4,784,729
2025-03-23 12:01:57 - Trainable Parameters: 4,784,729
2025-03-23 12:01:57 - Estimated Model Size: 18.25 MB (FP32)
2025-03-23 12:01:57 - Estimated Training Memory: 127.77 MB
2025-03-23 12:01:57 - Batch Size: 64
2025-03-23 12:01:57 - Gradient Accumulation Steps: 1
2025-03-23 12:01:57 - Effective Batch Size: 64
2025-03-23 12:01:57 - Learning Rate: 0.0005
2025-03-23 12:01:57 - Number of Epochs: 1
2025-03-23 12:01:57 - Using AMP: True
2025-03-23 12:01:57 - AMP Precision: float16
2025-03-23 12:01:57 - Max Gradient Norm: 1.0
2025-03-23 12:01:57 - Optimizer: AdamW
2025-03-23 12:01:57 - LR Scheduler: Linear warmup (4000 steps) then constant
2025-03-23 12:01:57 - Data Path: processed_data/got_char_data.pkl
2025-03-23 12:01:57 - Output Directory: runs\run_20250323-120154
2025-03-23 12:01:57 - Checkpoint Directory: checkpoints
2025-03-23 12:01:57 - Log File: runs\run_20250323-120154\training.log
2025-03-23 12:01:57 - Random Seed: 42
2025-03-23 12:01:57 - ================================================================================
2025-03-23 12:01:57 -                                STARTING TRAINING                                
2025-03-23 12:01:57 - ================================================================================
2025-03-23 12:01:57 - Starting training epoch 0 with gradient accumulation steps: 1
2025-03-23 12:01:57 - Total batches: 301, Batch size: 64
