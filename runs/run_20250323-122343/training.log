2025-03-23 12:23:43 - Logging to file: C:\Users\nimbu\Desktop\Code\AI\ChatGoT\runs\run_20250323-122343\training.log
2025-03-23 12:23:43 - === LOGGING STARTED AT 2025-03-23 12:23:43 ===
2025-03-23 12:23:43 - Logger initialized with console and file output
2025-03-23 12:23:43 - Random seed set to 42 for reproducibility
2025-03-23 12:23:43 - Arguments saved to runs\run_20250323-122343\args.json
2025-03-23 12:23:43 - Using CPU (CUDA not available)
2025-03-23 12:23:43 - ================================================================================
2025-03-23 12:23:43 -                                   LOADING DATA                                  
2025-03-23 12:23:43 - ================================================================================
2025-03-23 12:23:43 - Loading data from processed_data/got_char_data.pkl
2025-03-23 12:23:43 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 12:23:43 - Vocabulary size: 89 characters
2025-03-23 12:23:43 - Created data loaders: 2408 training batches, 67 validation batches
2025-03-23 12:23:43 - Using 4 workers for data loading with prefetch_factor=2
2025-03-23 12:23:43 - ================================================================================
2025-03-23 12:23:43 -                                  CREATING MODEL                                 
2025-03-23 12:23:43 - ================================================================================
2025-03-23 12:23:43 - Creating transformer model with:
2025-03-23 12:23:43 - - vocab_size: 89
2025-03-23 12:23:43 - - d_model: 256
2025-03-23 12:23:43 - - n_head: 8
2025-03-23 12:23:43 - - n_layers: 6
2025-03-23 12:23:43 - - d_hid: 1024
2025-03-23 12:23:43 - - dropout: 0.1
2025-03-23 12:23:43 - - max_seq_length: 256
2025-03-23 12:23:43 - Model initialized with 256 dimensions, 4,784,729 parameters
2025-03-23 12:23:43 - Trainable parameters: 4,784,729
2025-03-23 12:23:43 - Estimated model size: 18.50MB
2025-03-23 12:23:43 - Model moved to device: cpu
2025-03-23 12:23:45 - Using LambdaLR scheduler with warmup_steps=4000
2025-03-23 12:23:45 - ================================================================================
2025-03-23 12:23:45 -                              TRAINING CONFIGURATION                             
2025-03-23 12:23:45 - ================================================================================
2025-03-23 12:23:45 - Device: cpu
2025-03-23 12:23:45 - Model Architecture: Transformer
2025-03-23 12:23:45 - Embedding Dimension: 256
2025-03-23 12:23:45 - Number of Attention Heads: 8
2025-03-23 12:23:45 - Number of Layers: 6
2025-03-23 12:23:45 - Hidden Dimension: 1024
2025-03-23 12:23:45 - Dropout: 0.1
2025-03-23 12:23:45 - Context Length: 256
2025-03-23 12:23:45 - Vocabulary Size: 89
2025-03-23 12:23:45 - Total Parameters: 4,784,729
2025-03-23 12:23:45 - Trainable Parameters: 4,784,729
2025-03-23 12:23:45 - Estimated Model Size: 18.25 MB (FP32)
2025-03-23 12:23:45 - Estimated Training Memory: 127.77 MB
2025-03-23 12:23:45 - Batch Size: 8
2025-03-23 12:23:45 - Gradient Accumulation Steps: 1
2025-03-23 12:23:45 - Effective Batch Size: 8
2025-03-23 12:23:45 - Learning Rate: 0.0005
2025-03-23 12:23:45 - Number of Epochs: 1
2025-03-23 12:23:45 - Using AMP: False
2025-03-23 12:23:45 - Max Gradient Norm: 1.0
2025-03-23 12:23:45 - Optimizer: AdamW
2025-03-23 12:23:45 - LR Scheduler: Linear warmup (4000 steps) then constant
2025-03-23 12:23:45 - Data Path: processed_data/got_char_data.pkl
2025-03-23 12:23:45 - Output Directory: runs\run_20250323-122343
2025-03-23 12:23:45 - Checkpoint Directory: checkpoints
2025-03-23 12:23:45 - Log File: runs\run_20250323-122343\training.log
2025-03-23 12:23:45 - Random Seed: 42
2025-03-23 12:23:45 - ================================================================================
2025-03-23 12:23:45 -                                STARTING TRAINING                                
2025-03-23 12:23:45 - ================================================================================
2025-03-23 12:23:45 - Starting training epoch 0 with gradient accumulation steps: 1
2025-03-23 12:23:45 - Total batches: 2408, Batch size: 8
2025-03-23 12:23:50 - Progress: 0.0% | Batch 1/2408 | Loss: 4.7843 | Rate: 420.0 tokens/sec | ETA: 3.3h | 
2025-03-23 12:23:59 - Progress: 0.4% | Batch 10/2408 | Loss: 4.7994 | Rate: 1506.4 tokens/sec | ETA: 54.3m | 
