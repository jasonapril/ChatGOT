2025-03-23 15:46:06 - Logging to file: C:\Users\nimbu\Desktop\Code\AI\ChatGoT\runs\run_20250323-154606\training.log
2025-03-23 15:46:06 - === LOGGING STARTED AT 2025-03-23 15:46:06 ===
2025-03-23 15:46:06 - Logger initialized with console and file output
2025-03-23 15:46:07 - Random seed set to 42 for reproducibility
2025-03-23 15:46:07 - Arguments saved to runs\run_20250323-154606\args.json
2025-03-23 15:46:07 - Using CUDA with 1 GPU(s)
2025-03-23 15:46:07 - PyTorch version: 2.6.0+cu118
2025-03-23 15:46:07 - GPU Memory Info:
GPU 0: NVIDIA GeForce GTX 1650 Ti, 4.00 GB total, 3.23 GB free
2025-03-23 15:46:07 - Enabled TF32 precision for faster training
2025-03-23 15:46:07 - Enabled cuDNN benchmark mode for faster training
2025-03-23 15:46:07 - Detected 4096MB total VRAM - optimizing for maximum utilization
2025-03-23 15:46:07 - Applied ULTRA-AGGRESSIVE GTX 1650 Ti settings for MAXIMUM throughput and VRAM utilization
2025-03-23 15:46:07 - [MEMORY] Setting VRAM utilization target to 99.0%
2025-03-23 15:46:07 - ================================================================================
2025-03-23 15:46:07 -                                   LOADING DATA                                  
2025-03-23 15:46:07 - ================================================================================
2025-03-23 15:46:07 - Loading data from processed_data/got_char_data.pkl
2025-03-23 15:46:07 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 15:46:07 - Vocabulary size: 89 characters
2025-03-23 15:46:07 - Created data loaders: 2408 training batches, 67 validation batches
2025-03-23 15:46:07 - Using 4 workers for data loading with prefetch_factor=2
2025-03-23 15:46:07 - ================================================================================
2025-03-23 15:46:07 -                                  CREATING MODEL                                 
2025-03-23 15:46:07 - ================================================================================
2025-03-23 15:46:07 - Model initialized with 768 dimensions, 85,192,793 parameters
2025-03-23 15:46:07 - Trainable parameters: 85,192,793
2025-03-23 15:46:07 - Estimated model size: 327.98MB
2025-03-23 15:46:08 - Created transformer model with 85,192,793 parameters
2025-03-23 15:46:08 - Model config: d_model=768, n_head=12, d_hid=3072, n_layers=12
2025-03-23 15:46:08 - Using standard GPT-2 Small architecture
2025-03-23 15:46:08 - Using memory-efficient attention implementation
2025-03-23 15:46:08 - Model moved to cuda
2025-03-23 15:46:10 - Using LambdaLR scheduler with warmup_steps=4000
2025-03-23 15:46:10 - ================================================================================
2025-03-23 15:46:10 -                              TRAINING CONFIGURATION                             
2025-03-23 15:46:10 - ================================================================================
2025-03-23 15:46:10 - Device: cuda
2025-03-23 15:46:10 - GPU: NVIDIA GeForce GTX 1650 Ti
2025-03-23 15:46:10 - CUDA: 11.8
2025-03-23 15:46:10 - PyTorch CUDA: 11.8
2025-03-23 15:46:10 - GPU Memory: 4.00 GB
2025-03-23 15:46:10 - [MEMORY] Target GPU memory usage: 3.96 GB (99.0%)
2025-03-23 15:46:10 - Model Architecture: Transformer (GPT-2 Style)
2025-03-23 15:46:10 - Total Parameters: 85,192,793
2025-03-23 15:46:10 - Trainable Parameters: 85,192,793
2025-03-23 15:46:10 - Estimated Model Size: 324.98 MB (FP32)
2025-03-23 15:46:10 - Estimated Training Memory: 2274.89 MB
2025-03-23 15:46:10 - Batch Size: 8
2025-03-23 15:46:10 - Gradient Accumulation Steps: 4
2025-03-23 15:46:10 - Effective Batch Size: 32
2025-03-23 15:46:10 - Learning Rate: 5e-05
2025-03-23 15:46:10 - Number of Epochs: 1
2025-03-23 15:46:10 - Using AMP: False
2025-03-23 15:46:10 - Max Gradient Norm: 1.0
2025-03-23 15:46:10 - Optimizer: AdamW
2025-03-23 15:46:10 - LR Scheduler: Linear warmup (4000 steps) then constant
2025-03-23 15:46:10 - Data Path: processed_data/got_char_data.pkl
2025-03-23 15:46:10 - Output Directory: runs\run_20250323-154606
2025-03-23 15:46:10 - Checkpoint Directory: checkpoints
2025-03-23 15:46:10 - Log File: runs\run_20250323-154606\training.log
2025-03-23 15:46:10 - Random Seed: 42
2025-03-23 15:46:10 - [SPEED] Enabled cuDNN benchmarking for faster training
2025-03-23 15:46:10 - ================================================================================
2025-03-23 15:46:10 -                                STARTING TRAINING                                
2025-03-23 15:46:10 - ================================================================================
2025-03-23 15:46:10 - Starting training epoch 0 with gradient accumulation steps: 4
2025-03-23 15:46:10 - Total batches: 2408, Batch size: 8
2025-03-23 15:46:10 - [MEMORY] Current VRAM utilization is low (8.0%). Allocating cache tensors to increase GPU utilization.
2025-03-23 15:46:10 - [MEMORY] Allocating additional 2743.8MB to increase GPU utilization
2025-03-23 15:46:11 - [MEMORY] Increased VRAM utilization to 75.0%
2025-03-23 15:46:24 - Progress: 0.0% | Batch 1/2408 | Loss: 5.4032 | Rate: 146.1 tokens/sec | ETA: 9.4h | VRAM: 3423.7MB/4095.7MB (83.6%)
2025-03-23 15:47:12 - Minute 1 update | Progress: 0.2% | Batch 5/2408 | ETA: 6.9h | Memory: 4070.5MB (99.4%)
2025-03-23 15:47:29 - [WARNING] Extremely high memory pressure detected - performed emergency garbage collection
2025-03-23 15:47:29 - Progress: 0.2% | Batch 6/2408 | Loss: 5.3454 | Rate: 156.1 tokens/sec | ETA: 8.8h | VRAM: 4070.5MB/4095.7MB (99.4%)
2025-03-23 15:48:17 - Minute 2 update | Progress: 0.4% | Batch 9/2408 | ETA: 8.4h | Memory: 4069.8MB (99.4%)
2025-03-23 15:48:34 - [WARNING] Extremely high memory pressure detected - performed emergency garbage collection
2025-03-23 15:48:34 - Progress: 0.4% | Batch 10/2408 | Loss: 5.3925 | Rate: 142.5 tokens/sec | ETA: 9.6h | VRAM: 4069.8MB/4095.7MB (99.4%)
2025-03-23 15:49:25 - Minute 3 update | Progress: 0.5% | Batch 13/2408 | ETA: 9.3h | Memory: 4067.5MB (99.3%)
2025-03-23 15:49:43 - [WARNING] Extremely high memory pressure detected - performed emergency garbage collection
2025-03-23 15:49:43 - Progress: 0.6% | Batch 14/2408 | Loss: 5.3176 | Rate: 135.2 tokens/sec | ETA: 10.1h | VRAM: 4067.5MB/4095.7MB (99.3%)
2025-03-23 15:50:33 - Minute 4 update | Progress: 0.7% | Batch 17/2408 | ETA: 9.7h | Memory: 4068.3MB (99.3%)
2025-03-23 15:50:50 - [WARNING] Extremely high memory pressure detected - performed emergency garbage collection
2025-03-23 15:50:50 - Progress: 0.7% | Batch 18/2408 | Loss: 5.3327 | Rate: 131.8 tokens/sec | ETA: 10.3h | VRAM: 4068.3MB/4095.7MB (99.3%)
2025-03-23 15:51:42 - Minute 5 update | Progress: 0.9% | Batch 21/2408 | ETA: 10.0h | Memory: 4068.3MB (99.3%)
2025-03-23 15:52:00 - [WARNING] Extremely high memory pressure detected - performed emergency garbage collection
2025-03-23 15:52:00 - Progress: 0.9% | Batch 22/2408 | Loss: 5.3274 | Rate: 129.0 tokens/sec | ETA: 10.5h | VRAM: 4068.3MB/4095.7MB (99.3%)
2025-03-23 15:52:49 - Minute 6 update | Progress: 1.0% | Batch 25/2408 | ETA: 10.1h | Memory: 4068.3MB (99.3%)
2025-03-23 15:53:04 - [WARNING] Extremely high memory pressure detected - performed emergency garbage collection
2025-03-23 15:53:04 - Progress: 1.1% | Batch 26/2408 | Loss: 5.3771 | Rate: 128.7 tokens/sec | ETA: 10.5h | VRAM: 4068.3MB/4095.7MB (99.3%)
2025-03-23 15:53:54 - Minute 7 update | Progress: 1.2% | Batch 29/2408 | ETA: 10.2h | Memory: 4068.3MB (99.3%)
2025-03-23 15:54:11 - [WARNING] Extremely high memory pressure detected - performed emergency garbage collection
2025-03-23 15:54:11 - Progress: 1.2% | Batch 30/2408 | Loss: 5.2892 | Rate: 127.8 tokens/sec | ETA: 10.6h | VRAM: 4068.3MB/4095.7MB (99.3%)
2025-03-23 15:55:02 - Minute 8 update | Progress: 1.4% | Batch 33/2408 | ETA: 10.3h | Memory: 4068.3MB (99.3%)
2025-03-23 15:55:18 - [WARNING] Extremely high memory pressure detected - performed emergency garbage collection
2025-03-23 15:55:18 - Progress: 1.4% | Batch 34/2408 | Loss: 5.3075 | Rate: 127.1 tokens/sec | ETA: 10.6h | VRAM: 4068.3MB/4095.7MB (99.3%)
2025-03-23 15:56:10 - Minute 9 update | Progress: 1.5% | Batch 37/2408 | ETA: 10.4h | Memory: 4068.3MB (99.3%)
2025-03-23 15:56:27 - [WARNING] Extremely high memory pressure detected - performed emergency garbage collection
2025-03-23 15:56:27 - Progress: 1.6% | Batch 38/2408 | Loss: 5.3717 | Rate: 126.2 tokens/sec | ETA: 10.7h | VRAM: 4068.3MB/4095.7MB (99.3%)
2025-03-23 15:57:18 - Minute 11 update | Progress: 1.7% | Batch 41/2408 | ETA: 10.4h | Memory: 4068.3MB (99.3%)
2025-03-23 15:57:35 - [WARNING] Extremely high memory pressure detected - performed emergency garbage collection
2025-03-23 15:57:35 - Progress: 1.7% | Batch 42/2408 | Loss: 5.3637 | Rate: 125.6 tokens/sec | ETA: 10.7h | VRAM: 4068.3MB/4095.7MB (99.3%)
2025-03-23 15:58:25 - Minute 12 update | Progress: 1.9% | Batch 45/2408 | ETA: 10.5h | Memory: 4068.3MB (99.3%)
2025-03-23 15:58:42 - [WARNING] Extremely high memory pressure detected - performed emergency garbage collection
2025-03-23 15:58:43 - Progress: 1.9% | Batch 46/2408 | Loss: 5.2350 | Rate: 125.3 tokens/sec | ETA: 10.7h | VRAM: 4068.3MB/4095.7MB (99.3%)
2025-03-23 15:59:33 - Minute 13 update | Progress: 2.0% | Batch 49/2408 | ETA: 10.5h | Memory: 4068.3MB (99.3%)
2025-03-23 15:59:50 - [WARNING] Extremely high memory pressure detected - performed emergency garbage collection
2025-03-23 15:59:51 - Progress: 2.1% | Batch 50/2408 | Loss: 5.3197 | Rate: 124.9 tokens/sec | ETA: 10.7h | VRAM: 4068.3MB/4095.7MB (99.3%)
2025-03-23 16:00:40 - Minute 14 update | Progress: 2.2% | Batch 53/2408 | ETA: 10.5h | Memory: 4068.3MB (99.3%)
2025-03-23 16:00:58 - [WARNING] Extremely high memory pressure detected - performed emergency garbage collection
2025-03-23 16:00:58 - Progress: 2.2% | Batch 54/2408 | Loss: 5.2927 | Rate: 124.6 tokens/sec | ETA: 10.7h | VRAM: 4068.3MB/4095.7MB (99.3%)
2025-03-23 16:01:48 - Minute 15 update | Progress: 2.4% | Batch 57/2408 | ETA: 10.6h | Memory: 4068.3MB (99.3%)
2025-03-23 16:02:06 - [WARNING] Extremely high memory pressure detected - performed emergency garbage collection
2025-03-23 16:02:06 - Progress: 2.4% | Batch 58/2408 | Loss: 5.2931 | Rate: 124.3 tokens/sec | ETA: 10.8h | VRAM: 4068.3MB/4095.7MB (99.3%)
2025-03-23 16:02:57 - Minute 16 update | Progress: 2.5% | Batch 61/2408 | ETA: 10.6h | Memory: 4068.3MB (99.3%)
2025-03-23 16:03:15 - [WARNING] Extremely high memory pressure detected - performed emergency garbage collection
2025-03-23 16:03:15 - Progress: 2.6% | Batch 62/2408 | Loss: 5.2847 | Rate: 124.0 tokens/sec | ETA: 10.8h | VRAM: 4068.3MB/4095.7MB (99.3%)
