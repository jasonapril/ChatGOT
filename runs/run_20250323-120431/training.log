2025-03-23 12:04:31 - Logging to file: C:\Users\nimbu\Desktop\Code\AI\ChatGoT\runs\run_20250323-120431\training.log
2025-03-23 12:04:31 - === LOGGING STARTED AT 2025-03-23 12:04:31 ===
2025-03-23 12:04:31 - Logger initialized with console and file output
2025-03-23 12:04:31 - Random seed set to 42 for reproducibility
2025-03-23 12:04:31 - Arguments saved to runs\run_20250323-120431\args.json
2025-03-23 12:04:31 - Using CPU (CUDA not available)
2025-03-23 12:04:31 - ================================================================================
2025-03-23 12:04:31 -                                   LOADING DATA                                  
2025-03-23 12:04:31 - ================================================================================
2025-03-23 12:04:31 - Loading data from processed_data/got_char_data.pkl
2025-03-23 12:04:31 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 12:04:31 - Vocabulary size: 89 characters
2025-03-23 12:04:31 - Created data loaders: 301 training batches, 17 validation batches
2025-03-23 12:04:31 - Using 4 workers for data loading with prefetch_factor=2
2025-03-23 12:04:31 - ================================================================================
2025-03-23 12:04:31 -                                  CREATING MODEL                                 
2025-03-23 12:04:31 - ================================================================================
2025-03-23 12:04:31 - Creating transformer model with:
2025-03-23 12:04:31 - - vocab_size: 89
2025-03-23 12:04:31 - - d_model: 256
2025-03-23 12:04:31 - - n_head: 8
2025-03-23 12:04:31 - - n_layers: 6
2025-03-23 12:04:31 - - d_hid: 1024
2025-03-23 12:04:31 - - dropout: 0.1
2025-03-23 12:04:31 - - max_seq_length: 256
2025-03-23 12:04:31 - Model initialized with 256 dimensions, 4,784,729 parameters
2025-03-23 12:04:31 - Trainable parameters: 4,784,729
2025-03-23 12:04:31 - Estimated model size: 18.50MB
2025-03-23 12:04:31 - Model moved to device: cpu
2025-03-23 12:04:32 - Using LambdaLR scheduler with warmup_steps=4000
2025-03-23 12:04:32 - ================================================================================
2025-03-23 12:04:32 -                              TRAINING CONFIGURATION                             
2025-03-23 12:04:32 - ================================================================================
2025-03-23 12:04:32 - Device: cpu
2025-03-23 12:04:32 - Model Architecture: Transformer
2025-03-23 12:04:32 - Embedding Dimension: 256
2025-03-23 12:04:32 - Number of Attention Heads: 8
2025-03-23 12:04:32 - Number of Layers: 6
2025-03-23 12:04:32 - Hidden Dimension: 1024
2025-03-23 12:04:32 - Dropout: 0.1
2025-03-23 12:04:32 - Context Length: 256
2025-03-23 12:04:32 - Vocabulary Size: 89
2025-03-23 12:04:32 - Total Parameters: 4,784,729
2025-03-23 12:04:32 - Trainable Parameters: 4,784,729
2025-03-23 12:04:32 - Estimated Model Size: 18.25 MB (FP32)
2025-03-23 12:04:32 - Estimated Training Memory: 127.77 MB
2025-03-23 12:04:32 - Batch Size: 64
2025-03-23 12:04:32 - Gradient Accumulation Steps: 1
2025-03-23 12:04:32 - Effective Batch Size: 64
2025-03-23 12:04:32 - Learning Rate: 0.0005
2025-03-23 12:04:32 - Number of Epochs: 1
2025-03-23 12:04:32 - Using AMP: False
2025-03-23 12:04:32 - Max Gradient Norm: 1.0
2025-03-23 12:04:32 - Optimizer: AdamW
2025-03-23 12:04:32 - LR Scheduler: Linear warmup (4000 steps) then constant
2025-03-23 12:04:32 - Data Path: processed_data/got_char_data.pkl
2025-03-23 12:04:32 - Output Directory: runs\run_20250323-120431
2025-03-23 12:04:32 - Checkpoint Directory: checkpoints
2025-03-23 12:04:32 - Log File: runs\run_20250323-120431\training.log
2025-03-23 12:04:32 - Random Seed: 42
2025-03-23 12:04:32 - ================================================================================
2025-03-23 12:04:32 -                                STARTING TRAINING                                
2025-03-23 12:04:32 - ================================================================================
2025-03-23 12:04:32 - Starting training epoch 0 with gradient accumulation steps: 1
2025-03-23 12:04:32 - Total batches: 301, Batch size: 64
2025-03-23 12:04:46 - Progress: 0.3% | Batch 1/301 | Loss: 4.8353 | Rate: 1212.1 tokens/sec | ETA: 1.1h | 
2025-03-23 12:05:33 - Minute 1 update | Progress: 2.0% | Batch 6/301 | ETA: 42.2m | 
2025-03-23 12:05:51 - Progress: 2.7% | Batch 8/301 | Loss: 4.8151 | Rate: 1677.2 tokens/sec | ETA: 47.7m | 
2025-03-23 12:06:09 - Progress: 3.3% | Batch 10/301 | Loss: 4.8145 | Rate: 1689.0 tokens/sec | ETA: 47.0m | 
2025-03-23 12:06:37 - Minute 2 update | Progress: 4.3% | Batch 13/301 | ETA: 42.7m | 
2025-03-23 12:07:14 - Progress: 5.6% | Batch 17/301 | Loss: 4.7839 | Rate: 1723.5 tokens/sec | ETA: 45.0m | 
2025-03-23 12:07:43 - Minute 3 update | Progress: 6.6% | Batch 20/301 | ETA: 42.4m | 
2025-03-23 12:08:24 - Progress: 8.0% | Batch 24/301 | Loss: 4.7180 | Rate: 1694.1 tokens/sec | ETA: 44.6m | 
2025-03-23 12:08:43 - Minute 4 update | Progress: 8.6% | Batch 26/301 | ETA: 42.5m | 
2025-03-23 12:09:31 - Progress: 10.3% | Batch 31/301 | Loss: 4.6466 | Rate: 1703.1 tokens/sec | ETA: 43.3m | 
2025-03-23 12:09:49 - Minute 5 update | Progress: 11.0% | Batch 33/301 | ETA: 41.4m | 
2025-03-23 12:10:36 - Progress: 12.6% | Batch 38/301 | Loss: 4.5123 | Rate: 1713.5 tokens/sec | ETA: 41.9m | 
2025-03-23 12:10:54 - Minute 6 update | Progress: 13.3% | Batch 40/301 | ETA: 40.3m | 
