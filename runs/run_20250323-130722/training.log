2025-03-23 13:07:22 - Logging to file: C:\Users\nimbu\Desktop\Code\AI\ChatGoT\runs\run_20250323-130722\training.log
2025-03-23 13:07:22 - === LOGGING STARTED AT 2025-03-23 13:07:22 ===
2025-03-23 13:07:22 - Logger initialized with console and file output
2025-03-23 13:07:23 - Random seed set to 42 for reproducibility
2025-03-23 13:07:23 - Arguments saved to runs\run_20250323-130722\args.json
2025-03-23 13:07:23 - Using GPU: NVIDIA GeForce GTX 1650 Ti
2025-03-23 13:07:23 - Number of GPUs: 1
2025-03-23 13:07:23 - CUDA Version: 11.8
2025-03-23 13:07:23 - GPU Memory: 4.00 GB (Free: 3.23 GB)
2025-03-23 13:07:23 - Detected 4096MB total VRAM - optimizing for maximum utilization
2025-03-23 13:07:23 - Applied GTX 1650 Ti specific settings for MAXIMUM throughput and VRAM utilization
2025-03-23 13:07:23 - [MEMORY] Using optimized gradient accumulation steps: 1
2025-03-23 13:07:23 - [MEMORY] Setting VRAM utilization target to 99.0%
2025-03-23 13:07:23 - ================================================================================
2025-03-23 13:07:23 -                                   LOADING DATA                                  
2025-03-23 13:07:23 - ================================================================================
2025-03-23 13:07:23 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:07:23 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:07:23 - Vocabulary size: 89 characters
2025-03-23 13:07:23 - Created data loaders: 602 training batches, 17 validation batches
2025-03-23 13:07:23 - Using 4 workers for data loading with prefetch_factor=2
2025-03-23 13:07:23 - ================================================================================
2025-03-23 13:07:23 -                                  CREATING MODEL                                 
2025-03-23 13:07:23 - ================================================================================
2025-03-23 13:07:23 - Model initialized with 768 dimensions, 85,192,793 parameters
2025-03-23 13:07:23 - Trainable parameters: 85,192,793
2025-03-23 13:07:23 - Estimated model size: 327.98MB
2025-03-23 13:07:24 - Created transformer model with 85,192,793 parameters
2025-03-23 13:07:24 - Model config: d_model=768, n_head=12, d_hid=3072, n_layers=12
2025-03-23 13:07:24 - Using standard GPT-2 Small architecture
2025-03-23 13:07:24 - Using memory-efficient attention implementation
2025-03-23 13:07:24 - Model moved to cuda
2025-03-23 13:07:26 - Using LambdaLR scheduler with warmup_steps=4000
2025-03-23 13:07:26 - Automatic Mixed Precision enabled with torch.float16
2025-03-23 13:07:26 - ================================================================================
2025-03-23 13:07:26 -                              TRAINING CONFIGURATION                             
2025-03-23 13:07:26 - ================================================================================
2025-03-23 13:07:26 - Device: cuda
2025-03-23 13:07:26 - GPU: NVIDIA GeForce GTX 1650 Ti
2025-03-23 13:07:26 - CUDA: 11.8
2025-03-23 13:07:26 - PyTorch CUDA: 11.8
2025-03-23 13:07:26 - GPU Memory: 4.00 GB
2025-03-23 13:07:26 - [MEMORY] Target GPU memory usage: 3.96 GB (99.0%)
2025-03-23 13:07:26 - Model Architecture: Transformer (GPT-2 Style)
2025-03-23 13:07:26 - Total Parameters: 85,192,793
2025-03-23 13:07:26 - Trainable Parameters: 85,192,793
2025-03-23 13:07:26 - Estimated Model Size: 324.98 MB (FP32)
2025-03-23 13:07:26 - Estimated Training Memory: 2274.89 MB
2025-03-23 13:07:26 - Batch Size: 32
2025-03-23 13:07:26 - Gradient Accumulation Steps: 1
2025-03-23 13:07:26 - Effective Batch Size: 32
2025-03-23 13:07:26 - Learning Rate: 0.0005
2025-03-23 13:07:26 - Number of Epochs: 1
2025-03-23 13:07:26 - Using AMP: True
2025-03-23 13:07:26 - AMP Precision: float16
2025-03-23 13:07:26 - Max Gradient Norm: 1.0
2025-03-23 13:07:26 - Optimizer: AdamW
2025-03-23 13:07:26 - LR Scheduler: Linear warmup (4000 steps) then constant
2025-03-23 13:07:26 - Data Path: processed_data/got_char_data.pkl
2025-03-23 13:07:26 - Output Directory: runs\run_20250323-130722
2025-03-23 13:07:26 - Checkpoint Directory: checkpoints
2025-03-23 13:07:26 - Log File: runs\run_20250323-130722\training.log
2025-03-23 13:07:26 - Random Seed: 42
2025-03-23 13:07:26 - [SPEED] Enabled cuDNN benchmarking for faster training
2025-03-23 13:07:26 - ================================================================================
2025-03-23 13:07:26 -                                STARTING TRAINING                                
2025-03-23 13:07:26 - ================================================================================
2025-03-23 13:07:26 - Starting training epoch 0 with gradient accumulation steps: 1
2025-03-23 13:07:26 - Total batches: 602, Batch size: 32
2025-03-23 13:07:42 - Progress: 0.2% | Batch 1/602 | Loss: 5.3672 | Rate: 503.3 tokens/sec | ETA: 2.7h | VRAM: 999.2MB/4095.7MB (24.4%)
2025-03-23 13:08:32 - Minute 1 update | Progress: 0.8% | Batch 5/602 | ETA: 1.8h | Memory: 999.2MB (24.4%)
2025-03-23 13:08:44 - Progress: 1.0% | Batch 6/602 | Loss: 5.2891 | Rate: 628.0 tokens/sec | ETA: 2.2h | VRAM: 999.2MB/4095.7MB (24.4%)
2025-03-23 13:09:33 - Progress: 1.7% | Batch 10/602 | Loss: 5.1094 | Rate: 643.1 tokens/sec | ETA: 2.1h | VRAM: 999.2MB/4095.7MB (24.4%)
2025-03-23 13:09:33 - Minute 2 update | Progress: 1.7% | Batch 10/602 | ETA: 1.9h | Memory: 999.2MB (24.4%)
