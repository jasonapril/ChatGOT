2025-03-23 13:22:26 - Logging to file: C:\Users\nimbu\Desktop\Code\AI\ChatGoT\runs\run_20250323-132226\training.log
2025-03-23 13:22:26 - === LOGGING STARTED AT 2025-03-23 13:22:26 ===
2025-03-23 13:22:26 - Logger initialized with console and file output
2025-03-23 13:22:27 - Random seed set to 42 for reproducibility
2025-03-23 13:22:27 - Arguments saved to runs\run_20250323-132226\args.json
2025-03-23 13:22:27 - Using GPU: NVIDIA GeForce GTX 1650 Ti
2025-03-23 13:22:27 - Number of GPUs: 1
2025-03-23 13:22:27 - CUDA Version: 11.8
2025-03-23 13:22:28 - GPU Memory: 4.00 GB (Free: 3.23 GB)
2025-03-23 13:22:28 - Detected 4096MB total VRAM - optimizing for maximum utilization
2025-03-23 13:22:28 - Applied ULTRA-AGGRESSIVE GTX 1650 Ti settings for MAXIMUM throughput and VRAM utilization
2025-03-23 13:22:28 - Forced aggressive memory optimization (99% VRAM utilization target)
2025-03-23 13:22:28 - [MEMORY] Using optimized gradient accumulation steps: 1
2025-03-23 13:22:28 - [MEMORY] Setting VRAM utilization target to 99.0%
2025-03-23 13:22:28 - [MEMORY] Pre-allocating maximum CUDA memory to force high throughput...
2025-03-23 13:22:28 - [MEMORY] Pre-allocating maximum CUDA memory to force high throughput...
2025-03-23 13:22:28 - [MEMORY] Allocation stage 1/4: Targeting 50% of allocation goal
2025-03-23 13:22:28 -   - Current VRAM: 500.0MB (12.2% of total VRAM)
2025-03-23 13:22:28 - [MEMORY] Allocation stage 2/4: Targeting 75% of allocation goal
2025-03-23 13:22:28 -   - Current VRAM: 1244.9MB (30.4% of total VRAM)
2025-03-23 13:22:28 - [MEMORY] Allocation stage 3/4: Targeting 90% of allocation goal
2025-03-23 13:22:28 -   - Current VRAM: 2138.9MB (52.2% of total VRAM)
2025-03-23 13:22:28 - [MEMORY] Allocation stage 4/4: Targeting 98% of allocation goal
2025-03-23 13:22:28 -   - Current VRAM: 3112.3MB (76.0% of total VRAM)
2025-03-23 13:22:28 - [MEMORY] Successfully pre-allocated 3112.3MB of CUDA memory (3112.3MB total, 76.0% of VRAM)
2025-03-23 13:22:28 - [MEMORY] Attempting to allocate final 942.5MB of VRAM for maximum utilization
2025-03-23 13:22:28 - [MEMORY] Allocated additional 923.6MB in final pass (total: 4035.9MB, 98.5% of VRAM)
2025-03-23 13:22:28 - [MEMORY] Creating permanent VRAM buffer of 202.0MB to maintain high utilization
2025-03-23 13:22:28 - [MEMORY] Released pre-allocated memory, now available for training
2025-03-23 13:22:28 - [MEMORY] Memory after release: 386.7MB (9.4% of VRAM)
2025-03-23 13:22:28 - [MEMORY] Ready for training with 386.7MB reserved (9.4% of VRAM)
2025-03-23 13:22:28 - ================================================================================
2025-03-23 13:22:28 -                                   LOADING DATA                                  
2025-03-23 13:22:28 - ================================================================================
2025-03-23 13:22:28 - Loading data from processed_data/got_char_data.pkl
2025-03-23 13:22:28 - Data loaded: 19268 training sequences, 2141 validation sequences
2025-03-23 13:22:28 - Vocabulary size: 89 characters
2025-03-23 13:22:28 - Created data loaders: 1204 training batches, 34 validation batches
2025-03-23 13:22:28 - Using 4 workers for data loading with prefetch_factor=2
2025-03-23 13:22:28 - ================================================================================
2025-03-23 13:22:28 -                                  CREATING MODEL                                 
2025-03-23 13:22:28 - ================================================================================
2025-03-23 13:22:28 - Model initialized with 768 dimensions, 85,192,793 parameters
2025-03-23 13:22:28 - Trainable parameters: 85,192,793
2025-03-23 13:22:28 - Estimated model size: 327.98MB
2025-03-23 13:22:29 - Created transformer model with 85,192,793 parameters
2025-03-23 13:22:29 - Model config: d_model=768, n_head=12, d_hid=3072, n_layers=12
2025-03-23 13:22:29 - Using standard GPT-2 Small architecture
2025-03-23 13:22:29 - Using memory-efficient attention implementation
2025-03-23 13:22:29 - Model moved to cuda
2025-03-23 13:22:29 - [MEMORY] Enabling aggressive GPU memory strategies for data loading and processing
2025-03-23 13:22:29 - [MEMORY] Current VRAM utilization: 8.0%
2025-03-23 13:22:31 - Using LambdaLR scheduler with warmup_steps=4000
2025-03-23 13:22:31 - Automatic Mixed Precision enabled with torch.float16
2025-03-23 13:22:31 - ================================================================================
2025-03-23 13:22:31 -                              TRAINING CONFIGURATION                             
2025-03-23 13:22:31 - ================================================================================
2025-03-23 13:22:31 - Device: cuda
2025-03-23 13:22:31 - GPU: NVIDIA GeForce GTX 1650 Ti
2025-03-23 13:22:31 - CUDA: 11.8
2025-03-23 13:22:31 - PyTorch CUDA: 11.8
2025-03-23 13:22:31 - GPU Memory: 4.00 GB
2025-03-23 13:22:31 - [MEMORY] Target GPU memory usage: 3.96 GB (99.0%)
2025-03-23 13:22:31 - Model Architecture: Transformer (GPT-2 Style)
2025-03-23 13:22:31 - Total Parameters: 85,192,793
2025-03-23 13:22:31 - Trainable Parameters: 85,192,793
2025-03-23 13:22:31 - Estimated Model Size: 324.98 MB (FP32)
2025-03-23 13:22:31 - Estimated Training Memory: 2274.89 MB
2025-03-23 13:22:31 - Batch Size: 16
2025-03-23 13:22:31 - Gradient Accumulation Steps: 1
2025-03-23 13:22:31 - Effective Batch Size: 16
2025-03-23 13:22:31 - Learning Rate: 0.0005
2025-03-23 13:22:31 - Number of Epochs: 1
2025-03-23 13:22:31 - Using AMP: True
2025-03-23 13:22:31 - AMP Precision: float16
2025-03-23 13:22:31 - Max Gradient Norm: 1.0
2025-03-23 13:22:31 - Optimizer: AdamW
2025-03-23 13:22:31 - LR Scheduler: Linear warmup (4000 steps) then constant
2025-03-23 13:22:31 - Data Path: processed_data/got_char_data.pkl
2025-03-23 13:22:31 - Output Directory: runs\run_20250323-132226
2025-03-23 13:22:31 - Checkpoint Directory: checkpoints
2025-03-23 13:22:31 - Log File: runs\run_20250323-132226\training.log
2025-03-23 13:22:31 - [MEMORY] Aggressive memory optimization enabled
2025-03-23 13:22:31 - Random Seed: 42
2025-03-23 13:22:31 - [SPEED] Enabled cuDNN benchmarking for faster training
2025-03-23 13:22:31 - ================================================================================
2025-03-23 13:22:31 -                                STARTING TRAINING                                
2025-03-23 13:22:31 - ================================================================================
2025-03-23 13:22:31 - Starting training epoch 0 with gradient accumulation steps: 1
2025-03-23 13:22:31 - Total batches: 1204, Batch size: 16
2025-03-23 13:22:31 - [MEMORY] Current VRAM utilization is low (8.0%). Allocating cache tensors to increase GPU utilization.
2025-03-23 13:22:31 - [MEMORY] Allocating additional 2743.0MB to increase GPU utilization
2025-03-23 13:22:31 - [MEMORY] Increased VRAM utilization to 75.0%
2025-03-23 13:22:46 - Progress: 0.1% | Batch 1/1204 | Loss: 5.4062 | Rate: 269.8 tokens/sec | ETA: 5.1h | VRAM: 3745.3MB/4095.7MB (91.4%)
2025-03-23 13:23:35 - Minute 1 update | Progress: 0.5% | Batch 6/1204 | ETA: 3.1h | Memory: 3745.3MB (91.4%)
2025-03-23 13:23:56 - Progress: 0.7% | Batch 8/1204 | Loss: 5.2070 | Rate: 385.2 tokens/sec | ETA: 3.5h | VRAM: 3745.3MB/4095.7MB (91.4%)
2025-03-23 13:24:15 - Progress: 0.8% | Batch 10/1204 | Loss: 5.1094 | Rate: 391.8 tokens/sec | ETA: 3.5h | VRAM: 3745.3MB/4095.7MB (91.4%)
