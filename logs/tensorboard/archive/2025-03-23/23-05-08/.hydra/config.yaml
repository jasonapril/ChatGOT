models:
  architecture: gpt
  vocab_size: 50257
  d_model: 768
  n_layers: 12
  n_head: 12
  d_hid: 3072
  dropout: 0.1
  activation: gelu
  layer_norm_eps: 1.0e-05
  bias: true
  initialization:
    method: normal
    mean: 0.0
    std: 0.02
  positional_encoding:
    type: learned
    max_length: 1024
training:
  epochs: 5
  batch_size: 64
  sequence_length: 1024
  learning_rate: 5.0e-05
  weight_decay: 0.01
  clip_grad_norm: 1.0
  save_every: 1
  eval_every: 0.5
  optimizer:
    name: adamw
    beta1: 0.9
    beta2: 0.999
    eps: 1.0e-08
  scheduler:
    name: cosine
    warmup_ratio: 0.1
    min_lr_ratio: 0.1
  mixed_precision:
    enabled: true
    dtype: float16
  checkpoint:
    save_best: true
    metric: loss
    mode: min
    patience: 3
  gradient_accumulation:
    enabled: false
    steps: 1
  distributed:
    enabled: false
    backend: nccl
    world_size: -1
    find_unused_parameters: false
  profiling:
    enabled: false
    start_step: 10
    end_step: 20
data:
  dataset:
    name: got_char
    type: character
    split_ratio: 0.9
    shuffle: true
  processing:
    lowercase: false
    remove_special_chars: false
    min_frequency: 1
    max_vocab_size: 256
    padding_token: 0
    unknown_token: 1
  dataloader:
    num_workers: ${system.num_workers}
    pin_memory: ${system.pin_memory}
    drop_last: false
    prefetch_factor: 2
  augmentation:
    enabled: false
    techniques: []
  caching:
    enabled: true
    refresh: false
pipeline:
  pipeline:
    stages:
    - process
    - optimize
    - train
    - generate
    resume: false
    force_restart: false
    parallel: false
  process:
    enabled: true
    output_file: ${paths.processed_data}
  optimize:
    enabled: true
    strategy: bayesian
    metric: throughput
    trials: 10
    timeout: 600
    parameters:
      batch_size:
        min: 1
        max: 256
        step: 1
        distribution: log
  train:
    enabled: true
    use_optimized_params: true
  generate:
    enabled: true
    num_samples: 5
    max_length: 500
    temperature: 0.8
    top_k: 50
    top_p: 0.95
    seed_text: 'TYRION: '
    repetition_penalty: 1.1
    output_file: ${paths.output_dir}/samples.txt
project:
  name: ChatGoT
  version: 0.1.0
  description: Character-level GPT for Game of Thrones Text Generation
system:
  seed: 42
  log_level: INFO
  device: auto
  dtype: float32
  num_workers: 4
  pin_memory: true
paths:
  data_dir: data
  raw_data: ${paths.data_dir}/game_of_thrones_dataset.txt
  data_file: ${paths.raw_data}
  processed_data_dir: processed_data
  processed_data: ${paths.processed_data_dir}/got_char_data.pkl
  analysis_dir: ${paths.processed_data_dir}/analysis
  models_dir: models
  checkpoint_dir: checkpoints
  output_dir: runs/${now:%Y-%m-%d}/${experiment.name}
  log_dir: ${paths.output_dir}/logs
  artifacts_dir: ${paths.output_dir}/artifacts
experiment:
  name: default
  tags: []
  track: true
  tracking_uri: mlruns
